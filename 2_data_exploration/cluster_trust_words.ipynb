{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "127f5275",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "john10\n",
      "scr1\n",
      "/john10/scr1/yiweil\n"
     ]
    }
   ],
   "source": [
    "import os, glob, json\n",
    "import pickle\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "import re\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from collections import Counter, defaultdict\n",
    "import random, math\n",
    "from tqdm import tqdm\n",
    "import platform\n",
    "\n",
    "# print machine name\n",
    "machine_name = platform.node().split(\".\")[0]\n",
    "print(machine_name)\n",
    "\n",
    "# print available scratch directories\n",
    "print(\" \".join(os.listdir(f\"/{machine_name}\")))\n",
    "scr_dir = os.listdir(f\"/{machine_name}\")[0]\n",
    "\n",
    "DISK_IO_DIR = \"/{}/{}/yiweil\".format(machine_name,scr_dir)\n",
    "print(DISK_IO_DIR)\n",
    "if not os.path.exists(DISK_IO_DIR):\n",
    "    os.mkdir(DISK_IO_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52517c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_trf_bertbaseuncased_lg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "26070248",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1201 11:52:32.998061 139822730319616 configuration_utils.py:283] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at /sailhome/yiweil/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517\n",
      "I1201 11:52:33.000424 139822730319616 configuration_utils.py:319] Model config BertConfig {\n",
      "  \"_num_labels\": 2,\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bad_words_ids\": null,\n",
      "  \"bos_token_id\": null,\n",
      "  \"decoder_start_token_id\": null,\n",
      "  \"do_sample\": false,\n",
      "  \"early_stopping\": false,\n",
      "  \"eos_token_id\": null,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"is_decoder\": false,\n",
      "  \"is_encoder_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"length_penalty\": 1.0,\n",
      "  \"max_length\": 20,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"min_length\": 0,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"no_repeat_ngram_size\": 0,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_beams\": 1,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_return_sequences\": 1,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": true,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"prefix\": null,\n",
      "  \"pruned_heads\": {},\n",
      "  \"repetition_penalty\": 1.0,\n",
      "  \"task_specific_params\": null,\n",
      "  \"temperature\": 1.0,\n",
      "  \"top_k\": 50,\n",
      "  \"top_p\": 1.0,\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I1201 11:52:33.335651 139822730319616 modeling_utils.py:507] loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at /sailhome/yiweil/.cache/torch/transformers/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n",
      "I1201 11:52:43.434785 139822730319616 tokenization_utils.py:504] loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /sailhome/yiweil/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "# Loading the pre-trained BERT model\n",
    "###################################\n",
    "# Embeddings will be derived from\n",
    "# the outputs of this model\n",
    "model = BertModel.from_pretrained('bert-base-uncased',\n",
    "           output_hidden_states = True,)\n",
    "# Setting up the tokenizer\n",
    "###################################\n",
    "# This is the same tokenizer that\n",
    "# was used in the model to generate\n",
    "# embeddings to ensure consistency\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "cf815913",
   "metadata": {
    "code_folding": [
     0,
     33
    ]
   },
   "outputs": [],
   "source": [
    "def bert_text_preparation(text, tokenizer):\n",
    "    \"\"\"Preparing the input for BERT\n",
    "    \n",
    "    Takes a string argument and performs\n",
    "    pre-processing like adding special tokens,\n",
    "    tokenization, tokens to ids, and tokens to\n",
    "    segment ids. All tokens are mapped to seg-\n",
    "    ment id = 1.\n",
    "    \n",
    "    Args:\n",
    "        text (str): Text to be converted\n",
    "        tokenizer (obj): Tokenizer object\n",
    "            to convert text into BERT-re-\n",
    "            adable tokens and ids\n",
    "        \n",
    "    Returns:\n",
    "        list: List of BERT-readable tokens\n",
    "        obj: Torch tensor with token ids\n",
    "        obj: Torch tensor segment ids\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    marked_text = \"[CLS] \" + text + \" [SEP]\"\n",
    "    tokenized_text = tokenizer.tokenize(marked_text)\n",
    "    indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "    segments_ids = [1]*len(indexed_tokens)\n",
    "\n",
    "    # Convert inputs to PyTorch tensors\n",
    "    tokens_tensor = torch.tensor([indexed_tokens])\n",
    "    segments_tensors = torch.tensor([segments_ids])\n",
    "\n",
    "    return tokenized_text, tokens_tensor, segments_tensors\n",
    "\n",
    "def get_bert_embeddings(tokens_tensor, segments_tensors, model):\n",
    "    \"\"\"Get embeddings from an embedding model\n",
    "    \n",
    "    Args:\n",
    "        tokens_tensor (obj): Torch tensor size [n_tokens]\n",
    "            with token ids for each token in text\n",
    "        segments_tensors (obj): Torch tensor size [n_tokens]\n",
    "            with segment ids for each token in text\n",
    "        model (obj): Embedding model to generate embeddings\n",
    "            from token and segment ids\n",
    "    \n",
    "    Returns:\n",
    "        list: List of list of floats of size\n",
    "            [n_tokens, n_embedding_dimensions]\n",
    "            containing embeddings for each token\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # Gradient calculation id disabled\n",
    "    # Model is in inference mode\n",
    "    with torch.no_grad():\n",
    "        outputs = model(tokens_tensor, segments_tensors)\n",
    "        # Removing the first hidden state\n",
    "        # The first state is the input state\n",
    "        hidden_states = outputs[2][1:]\n",
    "\n",
    "    # Getting embeddings from the final BERT layer\n",
    "    token_embeddings = hidden_states[-1]\n",
    "    # Collapsing the tensor into 1-dimension\n",
    "    token_embeddings = torch.squeeze(token_embeddings, dim=0)\n",
    "    # Converting torchtensors to lists\n",
    "    list_token_embeddings = [token_embed.tolist() for token_embed in token_embeddings]\n",
    "\n",
    "    return list_token_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "4ea029c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [\n",
    "#     'They are a very trusting person.',\n",
    "#     'They are a very trustworthy person.'\n",
    "    'They are quick to trust people.',\n",
    "    'They inspire trust from people.',\n",
    "    'trusting',\n",
    "    'trustworthy',\n",
    "    'trusted'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "id": "54b69433",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Mary trusts and has faith in James.\"\n",
    "\n",
    "wois = ['James','Mary']\n",
    "\n",
    "contexts = [\n",
    "    \"They are highly trusting of others.\",\n",
    "    'They are highly trusted by others.'\n",
    "]\n",
    "\n",
    "words = [\n",
    "    'trusting',\n",
    "    'trusted'\n",
    "]\n",
    "\n",
    "benchmark_text = 'This is a benchmark to get rid of frequency effects.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "id": "cb6731bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#text = \"John cites Mary as a source of his inspiration.\"\n",
    "text = \"Mary comforts and soothes James.\"\n",
    "\n",
    "wois = ['James', 'Mary']\n",
    "\n",
    "contexts = [\n",
    "    \"The blanket is comforting.\",\n",
    "    \"The news is comforting.\",\n",
    "    \"Hot tea is comforting.\",\n",
    "    \"\"\n",
    "    'I feel comforted.'\n",
    "]\n",
    "\n",
    "words = ['comforting','comforted']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "id": "f1559c9c",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "#text = \"John cites Mary as a source of his inspiration.\"\n",
    "text = \"Mary inspires John.\"\n",
    "\n",
    "wois = ['John','Mary']\n",
    "\n",
    "contexts = [\n",
    "    \"He is inspiring to the next generation.\",\n",
    "    'I was inspired by his idea.'\n",
    "]\n",
    "\n",
    "words = ['inspiring','inspired']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "id": "29512806",
   "metadata": {},
   "outputs": [],
   "source": [
    "#text = \"John cites Mary as a source of his inspiration.\"\n",
    "#text = \"Mary bores John.\"\n",
    "text = 'John is bored by Mary.'\n",
    "\n",
    "wois = ['John','Mary']\n",
    "\n",
    "contexts = [\n",
    "    \"He is bored by the movie.\",\n",
    "    'The movie is boring.'\n",
    "]\n",
    "\n",
    "words = ['bored','boring']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "ed9d9d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'James finds John to be rather dull.'\n",
    "\n",
    "wois = ['James','John']\n",
    "\n",
    "words = [\n",
    "    'boring',\n",
    "    'bored'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1bd949b",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = ['The happy man ran home on hearing the news.',\n",
    "         'The happy news caused the woman to run home immediately.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "id": "ad485cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeds(contexts,wois):\n",
    "    target_word_embeddings = []\n",
    "    for i,context in enumerate(contexts):\n",
    "        tokenized_text, tokens_tensor, segments_tensors = bert_text_preparation(context, tokenizer)\n",
    "        tokenized_word, _, _ = bert_text_preparation(wois[i], tokenizer)\n",
    "        tokenized_word = tokenized_word[1]\n",
    "        list_token_embeddings = get_bert_embeddings(tokens_tensor, segments_tensors, model)\n",
    "        \n",
    "        word_index = tokenized_text.index(tokenized_word)\n",
    "        print(tokenized_text,tokenized_word,word_index)\n",
    "        word_embedding = list_token_embeddings[word_index]\n",
    "        target_word_embeddings.append(word_embedding)\n",
    "    return target_word_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "id": "a2a5cffa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'he', 'is', 'very', 'trusting', 'of', 'other', 'people', '.', '[SEP]'] trusting 4\n",
      "['[CLS]', 'she', 'is', 'trusting', 'and', 'quick', 'to', 'con', '##fide', 'her', 'secrets', '.', '[SEP]'] trusting 3\n"
     ]
    }
   ],
   "source": [
    "res = get_embeds(['He is very trusting of other people.','She is trusting and quick to confide her secrets.'],\n",
    "          ['trusting','trusting'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "id": "00c104df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8046946142354843"
      ]
     },
     "execution_count": 342,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1 - cosine(res[0],res[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "id": "df1cbbc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'he', 'is', 'very', 'trusted', '.', '[SEP]'] trusted 4\n",
      "['[CLS]', 'she', 'is', 'trusted', 'and', 'can', 'be', 'counted', 'on', 'to', 'keep', 'secrets', '.', '[SEP]'] trusted 3\n"
     ]
    }
   ],
   "source": [
    "res = get_embeds(['He is very trusted.','She is trusted and can be counted on to keep secrets.'],\n",
    "          'trusted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "id": "fce61cd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8453657171265186"
      ]
     },
     "execution_count": 340,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1 - cosine(res[0],res[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "id": "697e3929",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 John ['[CLS]', 'mary', 'inspire', '##s', 'john', '.', '[SEP]'] 4\n",
      "1 Mary ['[CLS]', 'mary', 'inspire', '##s', 'john', '.', '[SEP]'] 1\n",
      "0 ['[CLS]', 'he', 'is', 'inspiring', 'to', 'the', 'next', 'generation', '.', '[SEP]'] inspiring 3\n",
      "1 ['[CLS]', 'i', 'was', 'inspired', 'by', 'his', 'idea', '.', '[SEP]'] inspired 3\n"
     ]
    }
   ],
   "source": [
    "# Getting embeddings for the target\n",
    "# word in all given contexts\n",
    "target_woi_embeddings, target_word_embeddings, benchmark_embedding = [], [], None\n",
    "\n",
    "benchmark_tokenized, benchmark_tensor, benchmark_segments = bert_text_preparation(benchmark_text, tokenizer)\n",
    "benchmark_token_embeddings = get_bert_embeddings(benchmark_tensor, benchmark_segments, model)\n",
    "benchmark_embedding = benchmark_token_embeddings[benchmark_tokenized.index('frequency')]\n",
    "\n",
    "for i,woi in enumerate(wois):\n",
    "    tokenized_text, tokens_tensor, segments_tensors = bert_text_preparation(text, tokenizer)\n",
    "    list_token_embeddings = get_bert_embeddings(tokens_tensor, segments_tensors, model)\n",
    "    \n",
    "    word_index = tokenized_text.index(woi.lower())\n",
    "    print(i,woi,tokenized_text,word_index)\n",
    "    \n",
    "#     # Find the position 'bank' in list of tokens\n",
    "#     if 'interest' in tokenized_text:\n",
    "#         word_index = tokenized_text.index('editor')\n",
    "#     else:\n",
    "#         word_index = 0\n",
    "\n",
    "    # Get the embedding for bank\n",
    "    word_embedding = list_token_embeddings[word_index]\n",
    "    target_woi_embeddings.append(word_embedding)\n",
    "    \n",
    "for i,context in enumerate(contexts):\n",
    "    tokenized_text, tokens_tensor, segments_tensors = bert_text_preparation(context, tokenizer)\n",
    "    list_token_embeddings = get_bert_embeddings(tokens_tensor, segments_tensors, model)\n",
    "    \n",
    "    #word_index = 1\n",
    "    try:\n",
    "        word_index = tokenized_text.index(words[i])\n",
    "    except ValueError:\n",
    "        word_index = tokenized_text.index(words[i].split('ed')[0])\n",
    "    print(i,tokenized_text,words[i],word_index)\n",
    "    word_embedding = list_token_embeddings[word_index]\n",
    "    target_word_embeddings.append(word_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "id": "174b4358",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text1</th>\n",
       "      <th>text2</th>\n",
       "      <th>distance</th>\n",
       "      <th>dist(benchmark, text1)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Mary inspires John. | JOHN</td>\n",
       "      <td>inspiring</td>\n",
       "      <td>0.349679</td>\n",
       "      <td>0.217879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Mary inspires John. | JOHN</td>\n",
       "      <td>inspired</td>\n",
       "      <td>0.290506</td>\n",
       "      <td>0.217879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Mary inspires John. | MARY</td>\n",
       "      <td>inspiring</td>\n",
       "      <td>0.285497</td>\n",
       "      <td>0.262546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Mary inspires John. | MARY</td>\n",
       "      <td>inspired</td>\n",
       "      <td>0.201510</td>\n",
       "      <td>0.262546</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        text1      text2  distance  dist(benchmark, text1)\n",
       "0  Mary inspires John. | JOHN  inspiring  0.349679                0.217879\n",
       "1  Mary inspires John. | JOHN   inspired  0.290506                0.217879\n",
       "2  Mary inspires John. | MARY  inspiring  0.285497                0.262546\n",
       "3  Mary inspires John. | MARY   inspired  0.201510                0.262546"
      ]
     },
     "execution_count": 328,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "# Calculating the distance between the\n",
    "# embeddings of 'bank' in all the\n",
    "# given contexts of the word\n",
    "\n",
    "list_of_distances = []\n",
    "for woi, embed1 in zip(wois, target_woi_embeddings):\n",
    "    for text2, embed2 in zip(words, target_word_embeddings):\n",
    "        cos_dist = 1 - cosine(embed1, embed2)\n",
    "        benchmark_dist = 1 - cosine(benchmark_embedding, embed1)\n",
    "        list_of_distances.append([text + ' | ' + woi.upper(), text2, cos_dist, benchmark_dist])\n",
    "\n",
    "distances_df = pd.DataFrame(list_of_distances, columns=['text1', 'text2', 'distance', 'dist(benchmark, text1)'])\n",
    "distances_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "id": "69749b4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text1</th>\n",
       "      <th>text2</th>\n",
       "      <th>distance</th>\n",
       "      <th>dist(benchmark, text1)</th>\n",
       "      <th>adj_distance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Mary inspires John. | JOHN</td>\n",
       "      <td>inspiring</td>\n",
       "      <td>0.349679</td>\n",
       "      <td>0.217879</td>\n",
       "      <td>1.604920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Mary inspires John. | JOHN</td>\n",
       "      <td>inspired</td>\n",
       "      <td>0.290506</td>\n",
       "      <td>0.217879</td>\n",
       "      <td>1.333337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Mary inspires John. | MARY</td>\n",
       "      <td>inspiring</td>\n",
       "      <td>0.285497</td>\n",
       "      <td>0.262546</td>\n",
       "      <td>1.087419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Mary inspires John. | MARY</td>\n",
       "      <td>inspired</td>\n",
       "      <td>0.201510</td>\n",
       "      <td>0.262546</td>\n",
       "      <td>0.767522</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        text1      text2  distance  dist(benchmark, text1)  \\\n",
       "0  Mary inspires John. | JOHN  inspiring  0.349679                0.217879   \n",
       "1  Mary inspires John. | JOHN   inspired  0.290506                0.217879   \n",
       "2  Mary inspires John. | MARY  inspiring  0.285497                0.262546   \n",
       "3  Mary inspires John. | MARY   inspired  0.201510                0.262546   \n",
       "\n",
       "   adj_distance  \n",
       "0      1.604920  \n",
       "1      1.333337  \n",
       "2      1.087419  \n",
       "3      0.767522  "
      ]
     },
     "execution_count": 329,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distances_df['adj_distance'] = distances_df['distance']/distances_df['dist(benchmark, text1)']\n",
    "distances_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "id": "f8a8d2d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0213567023693606\n",
      "0.9036939665213485\n",
      "1.1588246775007274\n",
      "1.0253253020067223\n"
     ]
    }
   ],
   "source": [
    "d_J_ing = 0.314680\n",
    "d_J_ed = 0.308100\n",
    "d_M_ing = 0.271551\n",
    "d_M_ed = 0.300490\n",
    "d_ing_bench = 0.280248\n",
    "d_ed_bench = 0.292386\n",
    "print(d_J_ing / d_J_ed) # J closer to trusting than trusted\n",
    "print(d_M_ing / d_M_ed) # M closer to trusted than trusting\n",
    "print(d_J_ing / d_M_ing) # trusting closer to John than Mary\n",
    "print(d_J_ed / d_M_ed) # trusted closer to John than Mary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "id": "1e8606d9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text1</th>\n",
       "      <th>text2</th>\n",
       "      <th>distance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Mary trusts and has faith in James. | JAMES</td>\n",
       "      <td>trusting</td>\n",
       "      <td>0.314680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Mary trusts and has faith in James. | MARY</td>\n",
       "      <td>trusting</td>\n",
       "      <td>0.271551</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         text1     text2  distance\n",
       "0  Mary trusts and has faith in James. | JAMES  trusting  0.314680\n",
       "2   Mary trusts and has faith in James. | MARY  trusting  0.271551"
      ]
     },
     "execution_count": 310,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distances_df.loc[distances_df['text2']=='trusting'].sort_values('distance',ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "id": "0a4328c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text1</th>\n",
       "      <th>text2</th>\n",
       "      <th>distance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Mary trusts and has faith in James. | JAMES</td>\n",
       "      <td>trusted</td>\n",
       "      <td>0.30810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Mary trusts and has faith in James. | MARY</td>\n",
       "      <td>trusted</td>\n",
       "      <td>0.30049</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         text1    text2  distance\n",
       "1  Mary trusts and has faith in James. | JAMES  trusted   0.30810\n",
       "3   Mary trusts and has faith in James. | MARY  trusted   0.30049"
      ]
     },
     "execution_count": 311,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distances_df.loc[distances_df['text2']=='trusted'].sort_values('distance',ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "id": "b635a7c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text1</th>\n",
       "      <th>text2</th>\n",
       "      <th>distance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Mary bores John. | JOHN</td>\n",
       "      <td>bored</td>\n",
       "      <td>0.247160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Mary bores John. | MARY</td>\n",
       "      <td>bored</td>\n",
       "      <td>0.245556</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     text1  text2  distance\n",
       "0  Mary bores John. | JOHN  bored  0.247160\n",
       "2  Mary bores John. | MARY  bored  0.245556"
      ]
     },
     "execution_count": 298,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distances_df.loc[distances_df['text2']=='bored'].sort_values('distance',ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "id": "be0b6a7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text1</th>\n",
       "      <th>text2</th>\n",
       "      <th>distance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Mary bores John. | JOHN</td>\n",
       "      <td>boring</td>\n",
       "      <td>0.239369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Mary bores John. | MARY</td>\n",
       "      <td>boring</td>\n",
       "      <td>0.197656</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     text1   text2  distance\n",
       "1  Mary bores John. | JOHN  boring  0.239369\n",
       "3  Mary bores John. | MARY  boring  0.197656"
      ]
     },
     "execution_count": 299,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distances_df.loc[distances_df['text2']=='boring'].sort_values('distance',ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "id": "3b34de25",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text1</th>\n",
       "      <th>text2</th>\n",
       "      <th>distance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Mary inspires John. | JOHN</td>\n",
       "      <td>inspired</td>\n",
       "      <td>0.290506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Mary inspires John. | MARY</td>\n",
       "      <td>inspired</td>\n",
       "      <td>0.201510</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        text1     text2  distance\n",
       "1  Mary inspires John. | JOHN  inspired  0.290506\n",
       "3  Mary inspires John. | MARY  inspired  0.201510"
      ]
     },
     "execution_count": 293,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distances_df.loc[distances_df['text2']=='inspired'].sort_values('distance',ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "id": "ae783753",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text1</th>\n",
       "      <th>text2</th>\n",
       "      <th>distance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Mary inspires John. | JOHN</td>\n",
       "      <td>inspiring</td>\n",
       "      <td>0.349679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Mary inspires John. | MARY</td>\n",
       "      <td>inspiring</td>\n",
       "      <td>0.285497</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        text1      text2  distance\n",
       "0  Mary inspires John. | JOHN  inspiring  0.349679\n",
       "2  Mary inspires John. | MARY  inspiring  0.285497"
      ]
     },
     "execution_count": 294,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distances_df.loc[distances_df['text2']=='inspiring'].sort_values('distance',ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5d53dca3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        word emotion  emotion-intensity-score\n",
      "0   outraged   anger                    0.964\n",
      "1  brutality   anger                    0.959\n",
      "2     hatred   anger                    0.953\n",
      "3    hateful   anger                    0.940\n",
      "4  terrorize   anger                    0.939\n",
      "fear            1754\n",
      "trust           1562\n",
      "anger           1475\n",
      "sadness         1292\n",
      "joy             1265\n",
      "disgust         1085\n",
      "anticipation     862\n",
      "surprise         578\n",
      "Name: emotion, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "BLACKLIST_WORDS = {'shit','shitty','fuck','crap','crappy','gore','trump','tree','sun','soil','shanghai','john',\n",
    "                  'desert','turn','earthquake','lightning','hurricane','cyclone','tornado','storm','wildfire',\n",
    "                  'fire','cool','warm','hot','human','earth','planet','global','climate','change','cold'}\n",
    "PATH_TO_EMOLEX = \"/u/scr/yiweil/datasets/NRC-Emotion-Intensity-Lexicon-v1.txt\"\n",
    "EMOLEX = pd.read_csv(PATH_TO_EMOLEX,sep='\\t')\n",
    "EMOLEX = EMOLEX.loc[~EMOLEX['word'].isin(BLACKLIST_WORDS)]\n",
    "EMOLEX_EMOS = EMOLEX['emotion'].value_counts().index\n",
    "print(EMOLEX.head())\n",
    "\n",
    "print(EMOLEX[\n",
    "    'emotion'\n",
    "].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "id": "d26f5e2f",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "high_words_per_emotion = {\n",
    "    emo: EMOLEX.loc[(EMOLEX['emotion'] == emo) &\n",
    "                    (EMOLEX['emotion-intensity-score']>=0.5)]['word'].values\n",
    "    for emo in EMOLEX_EMOS}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "id": "5d762064",
   "metadata": {
    "code_folding": [
     1
    ]
   },
   "outputs": [],
   "source": [
    "nlps_per_emotion, vectors_per_emotion = defaultdict(dict), defaultdict(dict)\n",
    "for emo in EMOLEX_EMOS:\n",
    "    for w in high_words_per_emotion[emo]:\n",
    "        nlps_per_emotion[emo][w] = nlp(w)\n",
    "        vectors_per_emotion[emo][w] = nlps_per_emotion[emo][w].vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "id": "46d44991",
   "metadata": {
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'he', 'is', 'very', 'trusting', 'of', 'other', 'people', '.', '[SEP]'] trusting 4\n",
      "['[CLS]', 'she', 'is', 'trusting', 'and', 'quick', 'to', 'con', '##fide', 'her', 'secrets', '.', '[SEP]'] trusting 3\n",
      "['[CLS]', 'sometimes', 'i', 'am', 'too', 'trusting', 'of', 'strangers', '.', '[SEP]'] trusting 5\n",
      "['[CLS]', 'they', 'are', 'not', 'very', 'trusting', 'and', 'somewhat', 'skeptical', 'of', 'the', 'news', '.', '[SEP]'] trusting 5\n",
      "['[CLS]', 'he', 'is', 'very', 'trusted', '.', '[SEP]'] trusted 4\n",
      "['[CLS]', 'she', 'is', 'trusted', 'and', 'can', 'be', 'counted', 'on', 'to', 'keep', 'secrets', '.', '[SEP]'] trusted 3\n",
      "['[CLS]', 'the', 'information', 'comes', 'from', 'a', 'trusted', 'source', '.', '[SEP]'] trusted 6\n",
      "['[CLS]', 'the', 'conclusions', 'are', 'robust', 'and', 'can', 'be', 'trusted', '.', '[SEP]'] trusted 8\n",
      "['[CLS]', 'he', 'is', 'very', 'trust', '##worthy', '.', '[SEP]'] trust 4\n",
      "['[CLS]', 'she', 'is', 'trust', '##worthy', 'and', 'can', 'be', 'depended', 'on', 'to', 'keep', 'secrets', '.', '[SEP]'] trust 3\n",
      "['[CLS]', 'the', 'information', 'comes', 'from', 'a', 'trust', '##worthy', 'and', 'reliable', 'source', '.', '[SEP]'] trust 6\n",
      "['[CLS]', 'the', 'conclusions', 'are', 'robust', 'and', 'trust', '##worthy', '.', '[SEP]'] trust 6\n",
      "['[CLS]', 'he', 'is', 'very', 'fearful', 'of', 'other', 'people', '.', '[SEP]'] fearful 4\n",
      "['[CLS]', 'she', 'is', 'fearful', 'and', 'easily', 'frightened', '.', '[SEP]'] frightened 6\n",
      "['[CLS]', 'i', 'am', 'really', 'scared', 'of', 'spiders', '.', '[SEP]'] scared 4\n",
      "['[CLS]', 'they', 'are', 'easily', 'scared', 'by', 'horror', 'movies', '.', '[SEP]'] scared 4\n",
      "['[CLS]', 'he', 'is', 'highly', 'feared', 'as', 'a', 'ma', '##fi', '##oso', '.', '[SEP]'] feared 4\n",
      "['[CLS]', 'she', 'is', 'feared', 'and', 'can', 'make', 'you', 'tremble', 'in', 'fright', '.', '[SEP]'] feared 3\n",
      "['[CLS]', 'the', 'professor', 'was', 'feared', 'by', 'all', 'his', 'students', '.', '[SEP]'] feared 4\n",
      "['[CLS]', 'they', 'are', 'formidable', 'and', 'fears', '##ome', 'opponents', '.', '[SEP]'] fears 5\n",
      "['[CLS]', 'the', 'child', 'is', 'madden', '##ing', 'and', 'makes', 'me', 'so', 'frustrated', '.', '[SEP]'] madden 4\n",
      "['[CLS]', 'this', 'customer', 'is', 'so', 'annoying', '.', '[SEP]'] annoying 5\n",
      "['[CLS]', 'the', 'news', 'is', 'really', 'in', '##fur', '##iating', 'and', 'has', 'me', 'extremely', 'provoked', '.', '[SEP]'] in 5\n",
      "['[CLS]', 'this', 'child', 'makes', 'me', 'so', 'mad', 'and', 'frustrated', '[SEP]'] mad 6\n",
      "['[CLS]', 'this', 'customer', 'is', 'really', 'angry', 'at', 'the', 'clerk', '.', '[SEP]'] angry 5\n",
      "['[CLS]', 'i', 'am', 'furious', 'at', 'the', 'anger', '-', 'pro', '##voking', 'news', '.', '[SEP]'] furious 3\n",
      "['[CLS]', 'the', 'story', 'was', 'truly', 'tragic', 'and', 'moved', 'the', 'audience', 'to', 'tears', '.', '[SEP]'] tragic 5\n",
      "['[CLS]', 'it', 'is', 'so', 'sad', '##den', '##ing', 'to', 'see', 'children', 'crying', '.', '[SEP]'] sad 4\n",
      "['[CLS]', 'the', 'sad', 'movie', 'left', 'the', 'audience', 'weeping', '.', '[SEP]'] sad 2\n",
      "['[CLS]', 'the', 'sad', 'news', 'completely', 'erased', 'her', 'good', 'mood', '.', '[SEP]'] sad 2\n",
      "['[CLS]', 'the', 'audience', 'was', 'sad', 'and', 'tear', '##ful', 'by', 'the', 'end', 'of', 'the', 'tragic', 'story', '.', '[SEP]'] sad 4\n",
      "['[CLS]', 'the', 'children', 'were', 'extremely', 'sad', 'and', 'sobbing', 'loudly', '.', '[SEP]'] sad 5\n",
      "['[CLS]', 'many', 'audience', 'members', 'were', 'sad', 'to', 'the', 'point', 'of', 'weeping', 'after', 'watching', 'the', 'tragedy', '.', '[SEP]'] sad 5\n",
      "['[CLS]', 'her', 'good', 'mood', 'turned', 'sad', 'after', 'she', 'heard', 'about', 'the', 'accident', '.', '[SEP]'] sad 5\n",
      "['[CLS]', 'he', 'was', 'extremely', 'happy', 'after', 'hearing', 'the', 'joy', '##ous', 'news', '.', '[SEP]'] joy 8\n",
      "['[CLS]', 'the', 'happy', 'encounter', 'left', 'her', 'el', '##ated', 'and', 'jumping', 'for', 'joy', '.', '[SEP]'] happy 2\n",
      "['[CLS]', 'the', 'heart', '##ening', 'story', 'cheered', 'her', 'up', 'significantly', '.', '[SEP]'] heart 2\n",
      "['[CLS]', 'he', 'was', 'extremely', 'happy', 'after', 'hearing', 'the', 'joy', '##ous', 'news', '.', '[SEP]'] happy 4\n",
      "['[CLS]', 'she', 'was', 'el', '##ated', 'and', 'jumping', 'for', 'joy', 'to', 'see', 'her', 'boyfriend', 'after', 'a', 'long', 'time', '.', '[SEP]'] el 3\n",
      "['[CLS]', 'the', 'birthday', 'surprise', 'left', 'her', 'glad', '##dened', 'and', 'filled', 'her', 'with', 'happiness', '[SEP]'] glad 6\n",
      "['[CLS]', 'the', 'rotten', 'and', 'mold', '##y', 'food', 'is', 'absolutely', 'disgusting', '.', '[SEP]'] disgusting 9\n",
      "['[CLS]', 'this', 'kind', 'of', 'gross', 'behavior', 'is', 'rightful', '##ly', 'considered', 'disgusting', '.', '[SEP]'] disgusting 10\n",
      "['[CLS]', 'the', 'smell', 'is', 'disgusting', 'and', 'makes', 'me', 'want', 'to', 'pu', '##ke', '.', '[SEP]'] disgusting 4\n",
      "['[CLS]', 'he', 'was', 'disgusted', 'by', 'the', 'mold', 'on', 'his', 'food', '.', '[SEP]'] disgusted 3\n",
      "['[CLS]', 'she', 'felt', 'disgusted', 'by', 'the', 'man', \"'\", 's', 'gross', 'behavior', '[SEP]'] disgusted 3\n",
      "['[CLS]', 'i', 'am', 'so', 'disgusted', 'that', 'i', 'want', 'to', 'pu', '##ke', '[SEP]'] disgusted 4\n",
      "['[CLS]', 'the', 'eager', 'fans', 'are', 'nervously', 'anticipating', 'the', 'game', '.', '[SEP]'] anticipating 6\n",
      "['[CLS]', 'readers', 'are', 'eagerly', 'anticipating', 'the', 'new', 'book', ',', 'as', 'evidenced', 'by', 'record', '-', 'shattering', 'pre', '-', 'orders', '.', '[SEP]'] anticipating 4\n",
      "['[CLS]', 'fans', 'of', 'the', 'franchise', 'have', 'been', 'anticipating', 'the', 'sequel', 'for', 'years', '.', '[SEP]'] anticipating 7\n",
      "['[CLS]', 'we', 'are', 'struggling', 'to', 'be', 'patient', 'in', 'awaiting', 'the', 'outcome', '.', '[SEP]'] awaiting 8\n",
      "['[CLS]', 'the', 'game', 'tomorrow', 'night', 'will', 'be', 'highly', 'anticipated', 'by', 'eager', 'fans', '.', '[SEP]'] anticipated 8\n",
      "['[CLS]', 'the', 'novelist', \"'\", 's', 'next', 'book', 'is', 'highly', 'anticipated', ',', 'with', 'pre', '-', 'orders', 'setting', 'new', 'records', '.', '[SEP]'] anticipated 9\n",
      "['[CLS]', 'the', 'eagerly', 'awaited', 'sequel', 'finally', 'comes', 'to', 'theaters', 'this', 'month', '.', '[SEP]'] awaited 3\n",
      "['[CLS]', 'the', 'outcome', 'had', 'been', 'long', 'anticipated', '.', '[SEP]'] anticipated 6\n",
      "['[CLS]', 'the', 'surprising', 'turn', 'of', 'events', 'left', 'the', 'crowd', 'speechless', '.', '[SEP]'] surprising 2\n",
      "['[CLS]', 'the', 'unexpected', 'ending', 'took', 'the', 'audience', 'by', 'surprise', '.', '[SEP]'] unexpected 2\n",
      "['[CLS]', 'the', 'proposal', 'was', 'surprising', ',', 'since', 'they', 'had', 'been', 'dating', 'for', 'only', 'two', 'months', '.', '[SEP]'] surprising 4\n",
      "['[CLS]', 'the', 'experience', 'can', 'be', 'surprising', ',', 'going', 'against', 'your', 'initial', 'expectations', '.', '[SEP]'] surprising 5\n",
      "['[CLS]', 'the', 'crowd', 'was', 'left', 'speechless', 'and', 'utterly', 'surprised', '.', '[SEP]'] surprised 8\n",
      "['[CLS]', 'the', 'audience', 'was', 'completely', 'surprised', 'by', 'the', 'unexpected', 'ending', '.', '[SEP]'] surprised 5\n",
      "['[CLS]', 'she', 'was', 'surprised', 'by', 'his', 'proposal', ',', 'since', 'they', 'had', 'been', 'dating', 'for', 'only', 'two', 'months', '.', '[SEP]'] surprised 3\n",
      "['[CLS]', 'we', 'were', 'surprised', 'by', 'the', 'unfamiliar', 'experience', ',', 'which', 'went', 'against', 'our', 'initial', 'expectations', '.', '[SEP]'] surprised 3\n"
     ]
    }
   ],
   "source": [
    "seed_embeds_per_emotion = {\n",
    "    'trust': {'exp': get_embeds(['He is very trusting of other people.',\n",
    "                                       'She is trusting and quick to confide her secrets.',\n",
    "                                       'Sometimes I am too trusting of strangers.',\n",
    "                                      'They are not very trusting and somewhat skeptical of the news.'],\n",
    "          ['trusting']*4),\n",
    "              'stim':get_embeds(['He is very trusted.',\n",
    "                                        'She is trusted and can be counted on to keep secrets.',\n",
    "                                       'The information comes from a trusted source.',\n",
    "                                       'The conclusions are robust and can be trusted.',\n",
    "                                       'He is very trustworthy.',\n",
    "                                       'She is trustworthy and can be depended on to keep secrets.',\n",
    "                                       'The information comes from a trustworthy and reliable source.',\n",
    "                                       'The conclusions are robust and trustworthy.'],\n",
    "          ['trusted']*4+['trustworthy']*4)},\n",
    "    'fear': {'exp': get_embeds(['He is very fearful of other people.',\n",
    "                                       'She is fearful and easily frightened.',\n",
    "                                       'I am really scared of spiders.',\n",
    "                                      'They are easily scared by horror movies.'],\n",
    "          ['fearful','frightened','scared','scared']),\n",
    "             'stim': get_embeds(['He is highly feared as a mafioso.',\n",
    "                                        'She is feared and can make you tremble in fright.',\n",
    "                                       'The professor was feared by all his students.',\n",
    "                                       'They are formidable and fearsome opponents.'],\n",
    "          ['feared','feared','feared','fearsome'])\n",
    "            },\n",
    "    'anger': {'stim': get_embeds(['The child is maddening and makes me so frustrated.',\n",
    "                                'This customer is so annoying.',\n",
    "                                'The news is really infuriating and has me extremely provoked.'],\n",
    "                                ['maddening','annoying','infuriating']),\n",
    "              'exp': get_embeds(['This child makes me so mad and frustrated',\n",
    "                               'This customer is really angry at the clerk.',\n",
    "                               'I am furious at the anger-provoking news.'],\n",
    "                               ['mad','angry','furious'])\n",
    "             },\n",
    "    'sadness': {'stim': get_embeds(['The story was truly tragic and moved the audience to tears.',\n",
    "                                  'It is so saddening to see children crying.',\n",
    "                                  'The sad movie left the audience weeping.',\n",
    "                                  'The sad news completely erased her good mood.'],\n",
    "                                  ['tragic','saddening','sad','sad']),\n",
    "              'exp': get_embeds(['The audience was sad and tearful by the end of the tragic story.',\n",
    "                               'The children were extremely sad and sobbing loudly.',\n",
    "                               'Many audience members were sad to the point of weeping after watching the tragedy.',\n",
    "                               'Her good mood turned sad after she heard about the accident.'],\n",
    "                               ['sad','sad','sad','sad'])\n",
    "             },\n",
    "    'joy': {'stim': get_embeds(['He was extremely happy after hearing the joyous news.',\n",
    "                               'The happy encounter left her elated and jumping for joy.',\n",
    "                              'The heartening story cheered her up significantly.'],\n",
    "                              ['joyous','happy','heartening']),\n",
    "              'exp': get_embeds(['He was extremely happy after hearing the joyous news.',\n",
    "                               'She was elated and jumping for joy to see her boyfriend after a long time.',\n",
    "                               'The birthday surprise left her gladdened and filled her with happiness'],\n",
    "                               ['happy','elated','gladdened'])\n",
    "             },\n",
    "    'disgust': {'stim': get_embeds(['The rotten and moldy food is absolutely disgusting.',\n",
    "                                  'This kind of gross behavior is rightfully considered disgusting.',\n",
    "                                  'The smell is disgusting and makes me want to puke.'],\n",
    "                                  ['disgusting','disgusting','disgusting']),\n",
    "              'exp': get_embeds(['He was disgusted by the mold on his food.',\n",
    "                               \"She felt disgusted by the man's gross behavior\",\n",
    "                               'I am so disgusted that I want to puke'],\n",
    "                               ['disgusted','disgusted','disgusted'])\n",
    "             },\n",
    "    'anticipation': {'exp': get_embeds(['The eager fans are nervously anticipating the game.',\n",
    "                                       \"Readers are eagerly anticipating the new book, as evidenced by record-shattering pre-orders.\",\n",
    "                                       'Fans of the franchise have been anticipating the sequel for years.',\n",
    "                                       'We are struggling to be patient in awaiting the outcome.'],\n",
    "                                       ['anticipating','anticipating','anticipating','awaiting']),\n",
    "              'stim': get_embeds(['The game tomorrow night will be highly anticipated by eager fans.',\n",
    "                               \"The novelist's next book is highly anticipated, with pre-orders setting new records.\",\n",
    "                               'The eagerly awaited sequel finally comes to theaters this month.',\n",
    "                               'The outcome had been long anticipated.'],\n",
    "                               ['anticipated','anticipated','awaited','anticipated'])\n",
    "             },\n",
    "    'surprise': {'stim': get_embeds(['The surprising turn of events left the crowd speechless.',\n",
    "                                   'The unexpected ending took the audience by surprise.',\n",
    "                                   'The proposal was surprising, since they had been dating for only two months.',\n",
    "                                   'The experience can be surprising, going against your initial expectations.'],\n",
    "                                   ['surprising','unexpected','surprising','surprising']),\n",
    "              'exp': get_embeds(['The crowd was left speechless and utterly surprised.',\n",
    "                               'The audience was completely surprised by the unexpected ending.',\n",
    "                               'She was surprised by his proposal, since they had been dating for only two months.',\n",
    "                               'We were surprised by the unfamiliar experience, which went against our initial expectations.'],\n",
    "                               ['surprised','surprised','surprised','surprised'])\n",
    "             },}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "id": "1f93a981",
   "metadata": {
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "seed_sims_per_emo = defaultdict(lambda: defaultdict(dict))\n",
    "for emo in EMOLEX_EMOS:\n",
    "    for seed_cat in ['exp','stim']:\n",
    "        for w in high_words_per_emotion[emo]:\n",
    "            seed_sims_per_emo[emo][seed_cat][w] = sum([(1-cosine(seed_embed, vectors_per_emotion[emo][w]))\n",
    "                                                  for seed_embed in seed_embeds_per_emotion[emo][seed_cat]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "id": "c61d71db",
   "metadata": {},
   "outputs": [],
   "source": [
    "ratios_exp_stim_per_emo = {emo: {w: (seed_sims_per_emo[emo]['exp'][w]/len(seed_embeds_per_emotion[emo]['exp']))/\\\n",
    "                               (seed_sims_per_emo[emo]['stim'][w]/len(seed_embeds_per_emotion[emo]['stim']))\n",
    "                               for w in high_words_per_emotion[emo]}\n",
    "                         for emo in EMOLEX_EMOS}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "id": "a0f39a45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'befriend, pleasant, apologetic, generosity, timing, admiration, ally, agreeing, proud, amour, bless, kind, complimentary, intelligent, guide, eager, bride, protective, forgive, admitting, bloom, gospel, considerate, lovable, nerds, forgiving, inviting, generous, kindred, ourselves, humble, compliment, instructions, cuddle, praise, trusting, homage, innocent, temperate, romantic'"
      ]
     },
     "execution_count": 461,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "', '.join([x[0] for x in sorted(ratios_ed_ing_per_emo['trust'].items(), key=lambda x: x[1], reverse=True)[-40:]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "id": "b9fddacf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'perish, combatant, cemetery, crippling, appalling, insurmountable, hellish, bloodshed, assail, grisly, perpetrator, soscared, harrowing, incrimination, wrath, irreparable, horrid, reprisal, perdition, annihilation, murderer, plunder, upheaval, nefarious, angina'"
      ]
     },
     "execution_count": 458,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "', '.join([x[0] for x in sorted(ratios_ed_ing_per_emo['fear'].items(), key=lambda x: x[1], reverse=True)[:25]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "id": "3fa49343",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'prejudicial, atrocious, overbearing, blasphemous, manslaughter, unforgiving, intolerant, aggressor, coercion, depraved, mutilation, barbaric, intolerable, morbidity, inexcusable, vexed, monstrosity, intrusive, condescension, persecute, irreconcilable, disrespectful, depravity, extermination, crucifixion'"
      ]
     },
     "execution_count": 456,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "', '.join([x[0] for x in sorted(ratios_ed_ing_per_emo['anger'].items(), key=lambda x: x[1], reverse=True)[-25:]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "id": "c85063a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'homeless, unwell, teary, comatose, defenseless, departed, upset, disillusionment, urn, disgruntled, defeated, injured, demoralized, displeased, unhappy, grieving, battered, disfigured, disabled, decayed, bereaved, tearful, destitute, dumps, homesick'"
      ]
     },
     "execution_count": 454,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "', '.join([x[0] for x in sorted(ratios_ed_ing_per_emo['sadness'].items(), key=lambda x: x[1], reverse=True)[:25]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "id": "d21ddfb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'diamond, passionate, humor, intimately, fortunes, mastery, treat, aspiring, sweets, darling, special, cruising, leisure, dayoff, achieved, wellness, princely, bloom, therapeutic, entertainment, achievement, holidays, awards, winner, award'"
      ]
     },
     "execution_count": 452,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "', '.join([x[0] for x in sorted(ratios_ed_ing_per_emo['joy'].items(), key=lambda x: x[1], reverse=True)[-25:]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "id": "053c1fa6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sump, barf, retard, cringe, enslaved, flea, loo, disgust, wench, bitterly, repellant, aghast, cess, repulsion, hag, revulsion, resentment, brawl, gob, canker, shame, remains, outhouse, dank, vulture'"
      ]
     },
     "execution_count": 450,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "', '.join([x[0] for x in sorted(ratios_ed_ing_per_emo['disgust'].items(), key=lambda x: x[1], reverse=True)[:25]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "id": "626a3920",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'strive, enthusiasm, exigent, thriving, grow, develop, warn, hankering, owing, contingent, sneaking, cue, tease, grasping, vacation, thrill, readiness, accelerate, puppy, worrying, enjoying, feeling, forming, young, compensate'"
      ]
     },
     "execution_count": 448,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "', '.join([x[0] for x in sorted(ratios_ed_ing_per_emo['anticipation'].items(), key=lambda x: x[1], reverse=True)[-25:]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "id": "fe5bae10",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'awestruck, outcry, blitz, bewildered, rejoice, bewilderment, flinch, paralyzed, aghast, amazement, frightened, jubilant, strangle, astonishment, murderous, surprised, revolt, amazedness, breathless, excited, gasp, dismay, bomb, awe, thunderstruck'"
      ]
     },
     "execution_count": 445,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "', '.join([x[0] for x in sorted(ratios_ed_ing_per_emo['surprise'].items(), key=lambda x: x[1], reverse=True)[:25]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "id": "3232456e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(ratios_exp_stim_per_emo,open('ratios_exp_stim_per_emo.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "id": "00f8dd57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(450, 379)"
      ]
     },
     "execution_count": 398,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trustworthy_words = [w for w in ratio_trustworthy_trusting if ratio_trustworthy_trusting[w] >= 1]\n",
    "trusting_words = [w for w in ratio_trustworthy_trusting if ratio_trustworthy_trusting[w] < 1]\n",
    "len(trustworthy_words),len(trusting_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "id": "53e21466",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0039665078711244"
      ]
     },
     "execution_count": 395,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratio_trustworthy_trusting['scientist']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "id": "67df59de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9812272847585687"
      ]
     },
     "execution_count": 397,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratio_trustworthy_trusting['trustworthy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ad88c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# unquestionably, unquestionable, truthful, truthfulness, soundness, proficiency, competent, correctness, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "id": "79078954",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.33895293176166386, 0.3990370391627408)"
      ]
     },
     "execution_count": 355,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed_sims['trustworthy']['faith'],seed_sims['trusting']['faith']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "id": "8983b1f7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.2868058417195176, 0.23842786730440946)"
      ]
     },
     "execution_count": 346,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed_sims['trustworthy']['convincing'],seed_sims['trusting']['convincing']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "id": "b09fd4ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.27545600616534915, 0.23470223817721514)"
      ]
     },
     "execution_count": 347,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed_sims['trustworthy']['expert'],seed_sims['trusting']['expert']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "id": "da45495e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.2221806177175083, 0.18040295915752946)"
      ]
     },
     "execution_count": 348,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed_sims['trustworthy']['scientific'],seed_sims['trusting']['scientific']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "id": "5625e676",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.23006445235912099, 0.20640873045769803)"
      ]
     },
     "execution_count": 349,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed_sims['trustworthy']['team'],seed_sims['trusting']['team']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "id": "c5495817",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.2812008071995461, 0.23671180939025604)"
      ]
     },
     "execution_count": 350,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed_sims['trustworthy']['credibility'],seed_sims['trusting']['credibility']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "id": "7c0c962d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.4615678234249321, 0.49477126198217425)"
      ]
     },
     "execution_count": 351,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed_sims['trustworthy']['loyalty'],seed_sims['trusting']['loyalty']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "id": "703d68d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.3428814107521706, 0.3340188495347566)"
      ]
     },
     "execution_count": 352,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed_sims['trustworthy']['friend'],seed_sims['trusting']['friend']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "id": "1f8654bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.5043866783085269, 0.5573110484678789)"
      ]
     },
     "execution_count": 354,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed_sims['trustworthy']['confide'],seed_sims['trusting']['confide']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "c22bdcea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1562, 768)\n",
      "Cluster id labels for inputted data\n",
      "[0 1 1 ... 1 1 0]\n",
      "Centroids data\n",
      "[[-0.17448919 -1.272423   -0.62499315 ... -0.13562134 -0.43684334\n",
      "  -0.2900922 ]\n",
      " [ 0.32006335 -0.14674556 -0.555922   ...  0.01643606 -0.49558103\n",
      "  -0.12312236]]\n",
      "Score (Opposite of the value of X on the K-means objective which is Sum of distances of samples to their closest cluster center):\n",
      "-534307.7\n",
      "Silhouette_score: \n",
      "0.39945617\n"
     ]
    }
   ],
   "source": [
    "from sklearn import cluster, metrics\n",
    "NUM_CLUSTERS = 2\n",
    "X = np.asarray(list(trust_vectors.values()))\n",
    "print(X.shape)\n",
    "kmeans = cluster.KMeans(n_clusters=NUM_CLUSTERS)\n",
    "kmeans.fit(X)\n",
    " \n",
    "labels = kmeans.labels_\n",
    "centroids = kmeans.cluster_centers_\n",
    " \n",
    "print (\"Cluster id labels for inputted data\")\n",
    "print (labels)\n",
    "print (\"Centroids data\")\n",
    "print (centroids)\n",
    " \n",
    "print (\"Score (Opposite of the value of X on the K-means objective which is Sum of distances of samples to their closest cluster center):\")\n",
    "print (kmeans.score(X))\n",
    " \n",
    "silhouette_score = metrics.silhouette_score(X, labels, metric='euclidean')\n",
    " \n",
    "print (\"Silhouette_score: \")\n",
    "print (silhouette_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "fb322fa1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    1295\n",
       "0     267\n",
       "Name: cluster, dtype: int64"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2cluster = {trust_words[i]: labels[i] for i in range(len(trust_words))}\n",
    "df = pd.DataFrame({\n",
    "    'word': trust_words,\n",
    "    'cluster': labels\n",
    "})\n",
    "df['cluster'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "87952825",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['truthfulness', 'truthful', 'entrust', 'safekeeping', 'admirable',\n",
       "       'reassurance', 'heartfelt', 'credential', 'virtuous', 'motherhood',\n",
       "       'friendliness', 'skillful', 'approving', 'agreeable',\n",
       "       'indestructible', 'justifiable', 'monogamy', 'confide',\n",
       "       'commendable', 'indivisible', 'diligence', 'affirm', 'sanctify',\n",
       "       'cohesive', 'unwavering', 'benevolence', 'attestation',\n",
       "       'impeccable', 'steadfast', 'constancy', 'matrimony', 'lovable',\n",
       "       'advisable', 'competency', 'unconditionally', 'conscientious',\n",
       "       'durability', 'coexist', 'credence', 'devotional',\n",
       "       'sanctification', 'authorize', 'fireman', 'positivity', 'certify',\n",
       "       'purify', 'commend', 'unimpeachable', 'attentive', 'corroborate',\n",
       "       'absolution', 'considerate', 'magnificence', 'babysitter',\n",
       "       'humility', 'serenity', 'unconditional', 'unquestionable',\n",
       "       'mainstay', 'transcendence', 'befriend', 'enlighten', 'eminence',\n",
       "       'vigilant', 'tranquility', 'substantiate', 'vigilance',\n",
       "       'unfailing', 'husbandry', 'joyful', 'angelic', 'impartiality',\n",
       "       'coexisting', 'unquestioned', 'irrefutable', 'cheerfulness',\n",
       "       'dutiful', 'unequivocal', 'counsellor', 'insure', 'matron',\n",
       "       'punctual', 'everlasting', 'corroboration', 'unquestionably',\n",
       "       'pedigree', 'frankness', 'vitality', 'applaud', 'uplift',\n",
       "       'lovemaking', 'pertinent', 'betrothed', 'restorative', 'foresight',\n",
       "       'exalt', 'coalesce', 'impartial', 'midwife', 'cultivate',\n",
       "       'congruence', 'greatness', 'meditate', 'rejoice', 'impenetrable',\n",
       "       'eyewitness', 'prudent', 'conciliation', 'tantamount',\n",
       "       'inimitable', 'uncensured', 'obliging', 'immovable', 'inoculation',\n",
       "       'watchman', 'exalted', 'chastity', 'forefathers', 'inseparable',\n",
       "       'omniscient', 'immunization', 'expedient', 'fraternal',\n",
       "       'firstborn', 'approbation', 'adoration', 'paragon', 'salutary',\n",
       "       'liberate', 'connoisseur', 'countenance', 'unimpeached',\n",
       "       'obstetrician', 'perpetuity', 'formative', 'amen', 'confessional',\n",
       "       'housewife', 'custodian', 'accolade', 'unsurpassed', 'instruct',\n",
       "       'beautification', 'infallibility', 'synchronize', 'optimist',\n",
       "       'nerds', 'indelible', 'watchful', 'laudatory', 'radiance',\n",
       "       'hosannah', 'jubilant', 'proverbs', 'arbiter', 'approbate',\n",
       "       'voucher', 'countryman', 'calculator', 'preponderance', 'veracity',\n",
       "       'judicious', 'immerse', 'cashier', 'tutelage', 'eagerness',\n",
       "       'uncritical', 'bridesmaid', 'oblige', 'encomium', 'strategist',\n",
       "       'synergistic', 'impart', 'exaltation', 'tradesmen', 'indemnity',\n",
       "       'enliven', 'bridegroom', 'magnify', 'exhaustive', 'conveyancing',\n",
       "       'arbitrator', 'comptroller', 'sentry', 'elucidate', 'estimable',\n",
       "       'bequest', 'lender', 'toughness', 'simplify', 'lesbians',\n",
       "       'occupant', 'reconsideration', 'eulogize', 'antiseptic',\n",
       "       'shopkeeper', 'probity', 'supplication', 'respite', 'wonderstruck',\n",
       "       'preservative', 'amortization', 'glorify', 'dermatologist',\n",
       "       'reimbursement', 'abacus', 'executor', 'parietal', 'barter',\n",
       "       'eulogy', 'eulogistic', 'intercession', 'plaudit', 'encomiastic',\n",
       "       'curfew', 'reparation', 'berminat', 'bursary', 'dogma', 'bailiff',\n",
       "       'knickers', 'disclaim', 'boomerang', 'phalanx', 'eulogium',\n",
       "       'axiomatic', 'crescendo', 'proviso', 'gauging', 'pulsa', 'girder',\n",
       "       'respon', 'spaniel', 'imperfections', 'prophylactic',\n",
       "       'thermometer', 'aquapix', 'presumption', 'flatter', 'tiket',\n",
       "       'hairclip', 'tickle', 'contagiously', 'chicane', 'insolvent',\n",
       "       'gullible', 'ejaculation', 'sappy', 'indent', 'antifungal',\n",
       "       'sepatu', 'wadsworth', 'acrobat', 'zealous', 'pesawat',\n",
       "       'beliebers', 'sceptical', 'thrift', 'whstupp', 'disappointments',\n",
       "       'deceiving', 'insecurities', 'mislead', 'unaccountable',\n",
       "       'falsehood', 'bieber', 'scoundrel'], dtype=object)"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[df['cluster']==0]['word'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "b3e4fd6c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['trusted', 'trustworthy', 'truth', ..., 'divorce', 'mistakes',\n",
       "       'bait'], dtype=object)"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[df['cluster']==1]['word'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "644731d0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['confidentially', 'authenticate', 'kindred', 'cooperating',\n",
       "       'reverence', 'affirmatively', 'guidebook', 'warranty', 'mamma',\n",
       "       'endorse', 'fortitude', 'aspiration', 'saintly', 'brotherly',\n",
       "       'approvement', 'cherish', 'praiseworthy', 'mediator', 'notary',\n",
       "       'vetted', 'brighten', 'complimentary', 'sobriety', 'helper',\n",
       "       'revere', 'seniority', 'godly', 'bylaw', 'checklist', 'attest',\n",
       "       'adore', 'legalized', 'watchdog', 'connective', 'cuddle',\n",
       "       'adhering', 'curable', 'mediate', 'invocation', 'doubtless',\n",
       "       'answerable', 'verily', 'candid', 'edification', 'captivate',\n",
       "       'benedictory', 'reverie', 'admissible', 'savor', 'vouch',\n",
       "       'forgiving', 'undying', 'pontiff', 'backer', 'aspire', 'hosannah',\n",
       "       'endow', 'crowning', 'laudation', 'votive', 'putative', 'partake',\n",
       "       'chairwoman', 'laud', 'journeyman', 'foresee', 'dictum', 'enliven',\n",
       "       'mlk', 'villager', 'conformity', 'merci', 'governess', 'atone',\n",
       "       'sentry', 'bridal', 'utopian', 'impressionable', 'assignee',\n",
       "       'primer', 'specialize', 'cogent', 'fellas', 'rapt', 'kudos',\n",
       "       'relancer', 'sanction', 'favs', 'fain', 'quaint', 'paean', 'homie',\n",
       "       'surrogate', 'downright', 'recline', 'assessor', 'jual',\n",
       "       'berminat', 'roadster', 'aga', 'lettered', 'mortgagee',\n",
       "       'pocketbac', 'aneka', 'purr', 'shoutout', 'hairchalk', 'youll',\n",
       "       'karunia', 'handgel', 'sist', 'behel', 'aksesoris', 'heyday',\n",
       "       'sib', 'tema', 'veal', 'bruh', 'mau', 'holla', 'pake', 'sundial',\n",
       "       'harga', 'dha', 'ganna', 'divan', 'levee', 'tiket', 'pith', 'ori',\n",
       "       'kita', 'hemat', 'pivot', 'talaga', 'zest', 'shorty', 'ribet',\n",
       "       'gon', 'terbang', 'tentang', 'fisheye', 'unik', 'jellylens',\n",
       "       'nigga', 'bisa', 'chubby', 'bbw', 'jodie', 'sepatu', 'minat',\n",
       "       'utk', 'urs', 'rota', 'flange', 'aku', 'juga', 'gusset', 'tas',\n",
       "       'wot', 'alb', 'naman', 'murah', 'pesawat', 'dgn', 'shyt', 'peepul',\n",
       "       'koo', 'idc', 'censor', 'garansi', 'whstupp', 'coax', 'untuk',\n",
       "       'ehh', 'yuk', 'wack', 'thang'], dtype=object)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[df['cluster']==2]['word'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "26ff3e56",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['truthfulness', 'trustworthy', 'truthful', 'entrust', 'sisterhood',\n",
       "       'reputable', 'wholesome', 'reassurance', 'heartfelt', 'credential',\n",
       "       'creditable', 'motherhood', 'guardianship', 'friendliness',\n",
       "       'nurture', 'skillful', 'approving', 'agreeable', 'justifiable',\n",
       "       'solidity', 'confide', 'affirm', 'faultless', 'cohesive',\n",
       "       'harmoniously', 'lovable', 'advisable', 'competency', 'coexist',\n",
       "       'soundness', 'devotional', 'authorize', 'positivity', 'surety',\n",
       "       'considerate', 'humility', 'serenity', 'mainstay', 'transcendence',\n",
       "       'eminence', 'joyful', 'firmness', 'angelic', 'coexisting',\n",
       "       'cheerfulness', 'insure', 'punctual', 'determinate', 'frankness',\n",
       "       'vitality', 'cohesion', 'uplift', 'lovemaking', 'spotless',\n",
       "       'pertinent', 'concordance', 'restorative', 'foresight', 'coalesce',\n",
       "       'congruence', 'greatness', 'meditate', 'tantamount', 'inimitable',\n",
       "       'immovable', 'antidote', 'regularity', 'stamina', 'chastity',\n",
       "       'forefathers', 'inseparable', 'deliverance', 'expedient',\n",
       "       'firstborn', 'paragon', 'ratify', 'correctness', 'passwords',\n",
       "       'formative', 'confessional', 'instruct', 'vindication',\n",
       "       'infallibility', 'synchronize', 'optimist', 'nerds', 'watchful',\n",
       "       'formality', 'arbiter', 'voucher', 'veracity', 'underwrite',\n",
       "       'immerse', 'eagerness', 'moorings', 'patriarchal', 'encomium',\n",
       "       'synergistic', 'impart', 'magnify', 'exhaustive', 'estimable',\n",
       "       'lender', 'toughness', 'simplify', 'lesbians', 'axiom',\n",
       "       'enablement', 'probity', 'respite', 'wonderstruck', 'theocratic',\n",
       "       'deference', 'extol', 'abacus', 'barter', 'intercession',\n",
       "       'encomiastic', 'periodicity', 'dogma', 'phalanx', 'axiomatic',\n",
       "       'proviso', 'repute', 'zeal', 'gauging', 'pulsa', 'girder',\n",
       "       'respon', 'reseller', 'imperfections', 'aquapix', 'flatter',\n",
       "       'hairclip', 'tickle', 'contagiously', 'chicane', 'sappy', 'indent',\n",
       "       'xxx', 'beliebers', 'thrift', 'disappointments', 'deceiving',\n",
       "       'insecurities', 'mislead', 'falsehood'], dtype=object)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[df['cluster']==3]['word'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a4b1f112",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['safekeeping', 'fireman', 'purify', 'babysitter', 'befriend',\n",
       "       'husbandry', 'counsellor', 'matron', 'betrothed', 'midwife',\n",
       "       'cultivate', 'eyewitness', 'inoculation', 'watchman',\n",
       "       'immunization', 'connoisseur', 'obstetrician', 'housewife',\n",
       "       'custodian', 'beautification', 'radiance', 'proverbs',\n",
       "       'countryman', 'calculator', 'cashier', 'bridesmaid', 'strategist',\n",
       "       'tradesmen', 'conveyancing', 'arbitrator', 'comptroller',\n",
       "       'elucidate', 'antiseptic', 'shopkeeper', 'preservative',\n",
       "       'amortization', 'dermatologist', 'curfew', 'bursary', 'bailiff',\n",
       "       'knickers', 'boomerang', 'spaniel', 'prophylactic', 'thermometer',\n",
       "       'antifungal', 'wadsworth', 'acrobat', 'sceptical', 'bieber',\n",
       "       'scoundrel'], dtype=object)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[df['cluster']==4]['word'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "eb173434",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['monogamy', 'indivisible', 'sanctify', 'attestation', 'constancy',\n",
       "       'matrimony', 'credence', 'sanctification', 'certify', 'commend',\n",
       "       'corroborate', 'absolution', 'enlighten', 'substantiate',\n",
       "       'unquestioned', 'everlasting', 'corroboration', 'pedigree',\n",
       "       'applaud', 'exalt', 'rejoice', 'conciliation', 'uncensured',\n",
       "       'exalted', 'fraternal', 'approbation', 'adoration', 'liberate',\n",
       "       'perpetuity', 'amen', 'accolade', 'indelible', 'laudatory',\n",
       "       'approbate', 'preponderance', 'oblige', 'exaltation', 'indemnity',\n",
       "       'bridegroom', 'bequest', 'occupant', 'reconsideration', 'eulogize',\n",
       "       'supplication', 'glorify', 'reimbursement', 'executor', 'parietal',\n",
       "       'eulogy', 'eulogistic', 'plaudit', 'reparation', 'disclaim',\n",
       "       'eulogium', 'crescendo', 'presumption', 'insolvent', 'ejaculation'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[df['cluster']==5]['word'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "819ccc6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['trusted', 'truth', 'honor', 'honest', 'trusting', 'brotherhood',\n",
       "       'credibility', 'integrity', 'honorable', 'committed', 'trust',\n",
       "       'faithful', 'sincere', 'true', 'partnership', 'responsible',\n",
       "       'wisdom', 'cooperative', 'vow', 'oath', 'credible', 'respectable',\n",
       "       'promise', 'verified', 'hero', 'respected', 'compassion',\n",
       "       'approval', 'loyal', 'competence', 'ally', 'respect', 'advised',\n",
       "       'safeguard', 'authentic', 'genuine', 'friend', 'stable',\n",
       "       'friendships', 'respects', 'faith', 'proven', 'accepting',\n",
       "       'unification', 'confirmation', 'protecting', 'supporter',\n",
       "       'respectful', 'fact', 'believing', 'confident', 'partners',\n",
       "       'agreement', 'excellence', 'equality', 'virtue', 'honored',\n",
       "       'closeness', 'allegiance', 'determination', 'straightforward',\n",
       "       'guarantee', 'morality', 'admire', 'admiration', 'assurance',\n",
       "       'mentor', 'innocent', 'reliable', 'prepared', 'sanctuary',\n",
       "       'mother', 'fulfilled', 'compassionate', 'certified', 'morals',\n",
       "       'pact', 'accountable', 'esteem', 'competent', 'valor', 'secure',\n",
       "       'humanity', 'perfection', 'believed', 'pledge', 'understanding',\n",
       "       'goodness', 'expertise', 'confirmed', 'hope', 'strongest',\n",
       "       'grandfather', 'approve', 'familiarity', 'humble', 'agreeing',\n",
       "       'elders', 'protector', 'inspire', 'acceptance', 'mastery',\n",
       "       'accepted', 'healing', 'dignity', 'safe', 'intelligent',\n",
       "       'believes', 'covenant', 'authority', 'fulfillment', 'supporting',\n",
       "       'proficient', 'disciple', 'reliance', 'communicate', 'expert',\n",
       "       'heroic', 'accept', 'specialist', 'strengthening', 'favorable',\n",
       "       'proficiency', 'holiness', 'scientific', 'structure', 'familiar',\n",
       "       'elder', 'achieve', 'excellent', 'gentleman', 'supports', 'agreed',\n",
       "       'inspiration', 'noble', 'comrade', 'counselor', 'assuredly',\n",
       "       'rely', 'accepts', 'peaceful', 'brother', 'protective', 'saint',\n",
       "       'uphold', 'worthy', 'assure', 'lawful', 'doctor', 'leading',\n",
       "       'kind', 'establish', 'loving', 'benefactor', 'real', 'intimate',\n",
       "       'fidelity', 'therapeutic', 'homage', 'important', 'backbone',\n",
       "       'practiced', 'legitimacy', 'bless', 'courageous', 'hopeful',\n",
       "       'deepest', 'approved', 'convincing', 'blessing', 'angel',\n",
       "       'beliefs', 'accurate', 'assured', 'profound', 'authorization',\n",
       "       'majestic', 'civilized', 'engaged', 'regard', 'wonderful',\n",
       "       'encourage', 'evident', 'reassure', 'heroine', 'triumphant',\n",
       "       'rational', 'haven', 'truce', 'constantly', 'gospel', 'passionate',\n",
       "       'believer', 'good', 'testament', 'qualities', 'teach', 'god',\n",
       "       'accord', 'fundamental', 'thoughtful', 'brilliant', 'comfort',\n",
       "       'civilization', 'praise', 'progression', 'awareness',\n",
       "       'purification', 'heal', 'scripture', 'steward', 'recommend',\n",
       "       'definitive', 'liking', 'excel', 'happy', 'witness', 'experienced',\n",
       "       'guarded', 'nun', 'helpful', 'embrace', 'identify', 'brave',\n",
       "       'peace', 'symmetry', 'steady', 'inspired', 'sympathetic',\n",
       "       'fulfill', 'generosity', 'strengthen', 'marry', 'spouse', 'grant',\n",
       "       'efficient', 'visionary', 'praised', 'fairly', 'openly',\n",
       "       'philanthropic', 'regent', 'sage', 'generous', 'gratitude',\n",
       "       'depend', 'sweetheart', 'father', 'magnificent', 'mate', 'rooted',\n",
       "       'convince', 'share', 'ambition', 'enjoying', 'lovely',\n",
       "       'collectively', 'serene', 'designation', 'revival', 'upright',\n",
       "       'shelter', 'authoritative', 'communion', 'convinced', 'inviting',\n",
       "       'obey', 'structural', 'careful', 'abundance', 'compass',\n",
       "       'measured', 'modest', 'complement', 'heavenly', 'immaculate',\n",
       "       'plausible', 'advise', 'relative', 'earned', 'pleasant', 'devout',\n",
       "       'vigorous', 'congregation', 'remarkable', 'save', 'earn',\n",
       "       'compensate', 'strong', 'succeeding', 'soothing', 'admit',\n",
       "       'miracle', 'insight', 'intuition', 'influential', 'illumination',\n",
       "       'constant', 'destined', 'worship', 'patience', 'reunion',\n",
       "       'providing', 'succeed', 'vote', 'reconciliation', 'buddy',\n",
       "       'privacy', 'inclusion', 'readiness', 'forgive', 'durable',\n",
       "       'statement', 'promises', 'reinforcement', 'commanding',\n",
       "       'prevalent', 'salvation', 'engaging', 'provide', 'temperate',\n",
       "       'liberty', 'proud', 'govern', 'registry', 'ancestral', 'memorable',\n",
       "       'obedience', 'responsive', 'proctor', 'define', 'obvious',\n",
       "       'instructions', 'eager', 'perfect', 'admitting', 'recovery',\n",
       "       'delightful', 'choices', 'aspiring', 'cradle', 'princely',\n",
       "       'dependent', 'seal', 'bride', 'deed', 'bloom', 'strive', 'terms',\n",
       "       'rescue', 'heavens', 'evergreen', 'accompaniment', 'honeymoon',\n",
       "       'coronation', 'hug', 'celebration', 'ourselves', 'account',\n",
       "       'unbroken', 'powerful', 'neighbor', 'feeling', 'sentinel',\n",
       "       'minded', 'framework', 'deserve', 'compliment', 'freely', 'fully',\n",
       "       'tender', 'confession', 'remedy', 'predominant', 'pray', 'advent',\n",
       "       'measure', 'inform', 'enchanted', 'nobleman', 'reward', 'shepherd',\n",
       "       'enable', 'tolerance', 'intact', 'virgin', 'amour', 'sovereign',\n",
       "       'pious', 'gateway', 'timing', 'guard', 'doctrine', 'darling',\n",
       "       'warden', 'majesty', 'guru', 'related', 'treasure', 'infinity',\n",
       "       'apologetic', 'harbor', 'efforts', 'grow', 'synonymous', 'salute',\n",
       "       'popularity', 'constitute', 'inevitable', 'privy', 'psalm',\n",
       "       'spokesman', 'fellow', 'concord', 'sweet', 'armor', 'forecast',\n",
       "       'hire', 'favorite', 'enjoy', 'clean', 'found', 'path', 'affection',\n",
       "       'diagnosis', 'like', 'ranger', 'sacrifice', 'signature', 'frank',\n",
       "       'word', 'purely', 'follow', 'lamb', 'apologize', 'glory',\n",
       "       'landmark', 'consul', 'sermon', 'matter', 'sterling', 'counted',\n",
       "       'expect', 'capacity', 'prefer', 'gain', 'instructor', 'marshal',\n",
       "       'intended', 'dove', 'regal', 'flagship', 'effective', 'mighty',\n",
       "       'judgment', 'cautious', 'fortress', 'admitted', 'manage', 'aunt',\n",
       "       'lifestyle', 'lesson', 'crucial', 'socially', 'enroll', 'kitten',\n",
       "       'courier', 'contains', 'consecration', 'theological',\n",
       "       'necessarily', 'custody', 'finally', 'excited', 'perceive', 'dawn',\n",
       "       'exchange', 'commonplace', 'level', 'puppy', 'eagle', 'employ',\n",
       "       'asserting', 'usual', 'sir', 'matters', 'operation', 'orthodoxy',\n",
       "       'remains', 'sensual', 'lord', 'chocolate', 'abbot', 'oracle',\n",
       "       'tandem', 'imagination', 'fate', 'elect', 'emphasize', 'digit',\n",
       "       'butler', 'count', 'invite', 'blanket', 'circle', 'persuade',\n",
       "       'pay', 'grit', 'rule', 'playful', 'compact', 'repay', 'jeremiah',\n",
       "       'hail', 'cop', 'easiest', 'continue', 'dominion', 'treat',\n",
       "       'apprentice', 'trading', 'truss', 'dealings', 'supremacy', 'sing',\n",
       "       'intend', 'hymn', 'fixed', 'forum', 'cash', 'routine', 'consort',\n",
       "       'leap', 'akin', 'gate', 'offering', 'synod', 'privilege', 'serve',\n",
       "       'legislator', 'patrol', 'fitting', 'judged', 'leisure', 'clan',\n",
       "       'intense', 'paths', 'popular', 'iron', 'glow', 'yearning',\n",
       "       'pursue', 'neutral', 'explain', 'nest', 'clearance', 'gentry',\n",
       "       'deluxe', 'cube', 'grin', 'deal', 'baggage', 'accounts', 'collins',\n",
       "       'messenger', 'regardless', 'follower', 'escort', 'leaning', 'lean',\n",
       "       'possess', 'fortune', 'someday', 'bounty', 'crisp', 'picnic',\n",
       "       'adjust', 'sex', 'bouquet', 'cover', 'clapping', 'desires',\n",
       "       'garrison', 'swell', 'marrow', 'superstar', 'magnet', 'throne',\n",
       "       'collateral', 'homosexual', 'pretty', 'bargain', 'stark', 'money',\n",
       "       'circumstances', 'calls', 'footing', 'privileged', 'fade', 'maxim',\n",
       "       'fuse', 'suggest', 'rave', 'lace', 'dame', 'bounce', 'clap',\n",
       "       'dealt', 'white', 'weigh', 'monde', 'holder', 'endless', 'petit',\n",
       "       'cement', 'food', 'carol', 'lens', 'execution', 'canons', 'wear',\n",
       "       'struggles', 'swear', 'rod', 'fill', 'darkest', 'sur', 'proxy',\n",
       "       'disposed', 'cap', 'pill', 'hermit', 'secret', 'exception', 'warn',\n",
       "       'carl', 'congressman', 'shopping', 'pin', 'calf', 'machine',\n",
       "       'owing', 'weight', 'flirt', 'fiesta', 'mule', 'denying', 'don',\n",
       "       'rejection', 'conquer', 'autism', 'weakness', 'fashion', 'truck',\n",
       "       'crumpled', 'puff', 'stephanie', 'dare', 'hai', 'ella', 'sadness',\n",
       "       'secrecy', 'ting', 'denial', 'secrets', 'mag', 'ada', 'bitterness',\n",
       "       'burden', 'reject', 'devastating', 'flaws', 'envy', 'chuckle',\n",
       "       'erotic', 'libel', 'pawn', 'denied', 'sneaking', 'undone', 'doubt',\n",
       "       'hopeless', 'moat', 'dubious', 'erased', 'pry', 'jealousy', 'deny',\n",
       "       'weaknesses', 'doubts', 'shatter', 'unreliable', 'addict',\n",
       "       'fugitive', 'mistakes', 'bait'], dtype=object)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[df['cluster']==6]['word'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fb1dbe8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
