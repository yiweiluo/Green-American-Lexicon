{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from pprint import pprint\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import praw\n",
    "from praw.models import Submission\n",
    "from prawcore.exceptions import Forbidden,NotFound\n",
    "#from psaw import PushshiftAPI\n",
    "from pmaw import PushshiftAPI\n",
    "import csv\n",
    "import os\n",
    "from urllib.error import HTTPError\n",
    "import glob\n",
    "import requests\n",
    "import json\n",
    "from json import JSONDecodeError\n",
    "import datetime\n",
    "import pickle\n",
    "from collections import defaultdict\n",
    "import datetime as dt\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "john15\n",
      "scr1\n"
     ]
    }
   ],
   "source": [
    "import platform\n",
    "\n",
    "# print machine name\n",
    "machine_name = platform.node().split(\".\")[0]\n",
    "print(machine_name)\n",
    "\n",
    "# print available scratch directories\n",
    "print(\" \".join(os.listdir(f\"/{machine_name}\")))\n",
    "\n",
    "DISK_IO_DIR = \"/{}/scr1/yiweil\".format(machine_name)\n",
    "print(DISK_IO_DIR)\n",
    "if not os.path.exists(DISK_IO_DIR):\n",
    "    os.mkdir(DISK_IO_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "_RE_COMBINE_WHITESPACE = re.compile(r\"\\s+\")\n",
    "\n",
    "def strip_whitespace(text):\n",
    "    return _RE_COMBINE_WHITESPACE.sub(\" \", text).strip()\n",
    "\n",
    "with open('../keywords_high_precision.txt','r') as f:\n",
    "    KEYWORDS_HI_PREC = f.read().splitlines()\n",
    "\n",
    "KEYWORDS_SHORT = set([\"climate change\",\"global warming\",\"carbon\",\"co2\",\"methane\",\n",
    "                  \"green\",\"environment\",\"fossil fuel\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two ways of getting Reddit data illustrated in this notebook:\n",
    "* 1. [PRAW (Python Reddit API Wrapper)](#https://praw.readthedocs.io/en/latest/)\n",
    "* 2. [Pushshift API](#https://pushshift.io/api-parameters/)\n",
    "    \n",
    "The main advantage of Pushshift is that it accesses data from an archive, so even posts from currently banned subreddits (e.g., r/The_Donald) are accessible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using PRAW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You need to first create a reddit instance to use the PRAW API. Fill in the 3 fields (left blank) according to the instructions [here](#https://praw.readthedocs.io/en/latest/getting_started/authentication.html). Note: you will need to have a Reddit account and to register a developer app [here](#https://www.reddit.com/prefs/apps/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Western-Wishbone573\n"
     ]
    }
   ],
   "source": [
    "reddit = praw.Reddit(client_id='ACEORGSlEeJyQhPyNRAUiA',\n",
    "                     client_secret='xGN33fztXu_4jiOek_RHUtMGhHXAcQ',\n",
    "                     user_agent='me',\n",
    "                    password='redditC0bintr@sena',\n",
    "                    username='Western-Wishbone573')\n",
    "print(reddit.user.me())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Replace with the subreddits you're interested in\n",
    "non_niche_subs = set(['AskReddit','environment','politics','worldnews','climateskeptics',\n",
    "                     'Showerthoughts','climate','askscience','The_Donald','science','EcoInternet',\n",
    "                     'collapse','explainlikeimfive','conspiracy','NoStupidQuestions','australia',\n",
    "                     'unpopularopinion','climatechange','news','energy','canada','Conservative',\n",
    "                     'skeptic','todayilearned','shittyaskscience','ChapoTrapHouse','CanadaPolitics',\n",
    "                     'EverythingScience','worldpolitics','europe','AskScienceDiscussion',\n",
    "                     'ClimateOffensive','changemyview','ClimateActionPlan','AskTrumpSupporters',\n",
    "                     'GlobalWarming','GlobalClimateChange','esist','Green'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "code_folding": [
     0,
     53
    ]
   },
   "outputs": [],
   "source": [
    "def get_praw_submissions(reddit_instance,subreddit_str):\n",
    "    \n",
    "    if not os.path.exists('praw_output'):\n",
    "        os.mkdir('praw_output')\n",
    "    \n",
    "    subreddit = reddit_instance.subreddit(subreddit_str)\n",
    "    print('Getting submissions and comments from: {} ...'.format(subreddit.display_name))  \n",
    "    try:\n",
    "        title = subreddit.title\n",
    "        desc = subreddit.description\n",
    "\n",
    "        if not os.path.exists('subreddits.tsv'):\n",
    "            with open('subreddits.tsv','w') as f:\n",
    "                csvwriter = csv.writer(f, delimiter='\\t')\n",
    "                csvwriter.writerow([subreddit.display_name,title,desc])\n",
    "        else:\n",
    "            with open('subreddits.tsv','a') as f:\n",
    "                csvwriter = csv.writer(f, delimiter='\\t')\n",
    "                csvwriter.writerow([subreddit.display_name,title,desc])\n",
    "\n",
    "        # Write header\n",
    "        with open(os.path.join('praw_output','{}.tsv'.format(subreddit.display_name)), 'w', newline='\\n') as csvfile:\n",
    "            csvwriter = csv.writer(csvfile, delimiter='\\t',\n",
    "                                    quotechar='|', quoting=csv.QUOTE_MINIMAL)\n",
    "            csvwriter.writerow(['title','author','date','is_video','id','num_downs','num_ups','upvote_ratio',\n",
    "                               'num_comments','score','text','subreddit'])\n",
    "\n",
    "        # Write body\n",
    "        for submission in subreddit.new(limit=None):\n",
    "            sub_title = submission.title\n",
    "            sub_author = submission.author.name if submission.author is not None else -1\n",
    "            sub_date = submission.created\n",
    "            sub_is_vid = submission.is_video\n",
    "            sub_id = submission.id\n",
    "            sub_downvotes = submission.downs\n",
    "            sub_upvotes = submission.ups\n",
    "            sub_upvote_ratio = submission.upvote_ratio\n",
    "            sub_num_comments = submission.num_comments\n",
    "            sub_score = submission.score\n",
    "            sub_text = submission.selftext.strip().replace('\\t','').replace('\\n','')\n",
    "            sub_subreddit = submission.subreddit.display_name\n",
    "            with open(os.path.join('praw_output','{}.tsv'.format(subreddit.display_name)), 'a', newline='\\n') as csvfile:\n",
    "                csvwriter = csv.writer(csvfile, delimiter='\\t',\n",
    "                                    quotechar='|', quoting=csv.QUOTE_MINIMAL)\n",
    "                csvwriter.writerow([sub_title,sub_author,sub_date,sub_is_vid,sub_id,sub_downvotes,\n",
    "                                   sub_upvotes,sub_upvote_ratio,sub_num_comments,sub_score,sub_text,sub_subreddit])\n",
    "        \n",
    "        print('Wrote output to:', os.path.join('praw_output','{}.tsv'.format(subreddit.display_name)))\n",
    "        \n",
    "    except HTTPError as e:\n",
    "        if e.code == 403:\n",
    "            print('Forbidden: private subreddit.')\n",
    "            \n",
    "def get_praw_submission_comments(reddit_instance,subreddit,submission_id):\n",
    "    \n",
    "    submission = Submission(reddit_instance,id=submission_id)\n",
    "    \n",
    "    try:\n",
    "        submission.comments.replace_more(limit=0)\n",
    "        all_comments = submission.comments.list()\n",
    "\n",
    "        # Write header\n",
    "        with open(os.path.join('praw_output','post_comments','{}_COMMENTS.tsv'.format(subreddit)), 'w', newline='\\n') as csvfile:\n",
    "            csvwriter = csv.writer(csvfile, delimiter='\\t',\n",
    "                                    quotechar='|', quoting=csv.QUOTE_MINIMAL)\n",
    "            csvwriter.writerow(['submission_id','author','text','date','id','controversiality','num_downs','num_ups',\n",
    "                               'num_likes','score','subreddit'])\n",
    "\n",
    "        # Write body\n",
    "        for comment in all_comments:\n",
    "            sub_id = comment._submission.id\n",
    "            assert sub_id == submission_id\n",
    "            author_name = comment.author.name if comment.author is not None else -1\n",
    "            comment_body = comment.body.strip().replace('\\t','').replace('\\n','')\n",
    "            date_created = comment.created\n",
    "            comment_id = comment.id\n",
    "            controversiality = comment.controversiality\n",
    "            num_downs = comment.downs\n",
    "            num_ups = comment.ups\n",
    "            num_likes = comment.likes\n",
    "            score = comment.score\n",
    "            subreddit_name = comment.subreddit.display_name\n",
    "            #print(subreddit_name,subreddit)\n",
    "            assert subreddit_name == subreddit\n",
    "            \n",
    "            with open(os.path.join('praw_output','post_comments','{}_COMMENTS.tsv'.format(subreddit_name)), 'a', newline='\\n') as csvfile:\n",
    "                csvwriter = csv.writer(csvfile, delimiter='\\t',\n",
    "                                    quotechar='|', quoting=csv.QUOTE_MINIMAL)\n",
    "                csvwriter.writerow([sub_id,author_name,comment_body,date_created,comment_id,controversiality,\n",
    "                                   num_downs,num_ups,num_likes,score,subreddit_name])\n",
    "    except HTTPError as e:\n",
    "        if e.code == 403:\n",
    "            print('Forbidden: private subreddit.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/u/scr/yiweil/Green-American-Lexicon/1_data_collection/\\\n",
    "reddit/post_fields.txt','r') as f:\n",
    "    POST_FIELDS = f.read().splitlines()\n",
    "with open('/u/scr/yiweil/Green-American-Lexicon/1_data_collection/\\\n",
    "reddit/reddit_comment_fields.txt','r') as f:\n",
    "    COMMENT_FIELDS = f.read().splitlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scratch (re-running PRAW with \"non-lazy\" objects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = os.path.join(DISK_IO_DIR,'pmaw_output')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dir = os.path.join(DISK_IO_DIR,'praw_output')\n",
    "if not os.path.exists(out_dir):\n",
    "    os.mkdir(out_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/u/nlp/anaconda/main/anaconda3/envs/yiwei-climate/lib/python3.6/site-packages/IPython/core/interactiveshell.py:3072: DtypeWarning: Columns (28,32,41,42,43,44,45,46,47,48,49,50,53,57,58,60,61,62,63,64,65,66,67,68,69,70,71,72,73,75,77,78,79,80,83,84,85,86,87,88,90,92,93,94,95,96,97,98,101,103,104) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n",
      "/u/nlp/anaconda/main/anaconda3/envs/yiwei-climate/lib/python3.6/site-packages/IPython/core/interactiveshell.py:3072: DtypeWarning: Columns (7,8,10,13,15,16,17,18,22,23,24,26,27,28,29,30,31,33,34,36,37,39,40,41,42,43,44,45,46,49,51,52,53,56,57,58) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "master_posts_df = pd.read_csv(data_dir+'/submissions/changemyview_all/\\\n",
    "1-1-2010_to_09-27-2021.csv',index_col=0)\n",
    "master_comments_df = pd.read_csv(data_dir+'/post_comments/changemyview_all/\\\n",
    "1-1-2010_to_09-24-2021.csv',index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique comments/posts in current dataset: 169926\n",
      "Missing 38692 comment parents.\n"
     ]
    }
   ],
   "source": [
    "# First get all missing comments\n",
    "current_ids = set(list(master_comments_df['id'].values)) | \\\n",
    "                set(list(master_posts_df['id'].values))\n",
    "print('Number of unique comments/posts in current dataset:',\n",
    "      len(current_ids))\n",
    "\n",
    "parent_ids = set(list(master_comments_df['parent_id'].values))\n",
    "missing_parent_ids = [x for x in parent_ids if type(x)==str \n",
    "                      and x.split('_')[-1] not in current_ids]\n",
    "print('Missing {} comment parents.'.format(len(missing_parent_ids)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_post_ids = [x for x in missing_parent_ids if type(x) == str\n",
    "                    and x.split('_')[0] == 't3']\n",
    "missing_comment_ids = [x for x in missing_parent_ids if type(x) == str\n",
    "                    and x.split('_')[0] == 't1']\n",
    "missing_other_ids = [x for x in missing_parent_ids if type(x) == str\n",
    "                    and x.split('_')[0] == 't5']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3243830, 79925)"
      ]
     },
     "execution_count": 361,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed = set([x.split('/')[-1].split('.json')[0]\n",
    "                 for x in glob.glob(os.path.join(out_dir,'*.json'))])\n",
    "len(processed),len(processed.intersection(missing_post_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/john15/scr1/yiweil/praw_output'"
      ]
     },
     "execution_count": 389,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13059"
      ]
     },
     "execution_count": 362,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(processed.intersection(missing_comment_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "11272\n"
     ]
    }
   ],
   "source": [
    "missing_post_ids = [p for p in missing_post_ids if \n",
    "                      p not in processed]\n",
    "print(len(missing_post_ids)) # 34358-10027\n",
    "\n",
    "missing_comment_ids = [c for c in missing_comment_ids if \n",
    "                      c not in processed]\n",
    "print(len(missing_comment_ids)) # 34358-10027"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 11272)"
      ]
     },
     "execution_count": 364,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(missing_post_ids),len(missing_comment_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['t1_fmwzbvp', 't1_dws6crr', 't1_dbhl9pz']"
      ]
     },
     "execution_count": 380,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "missing_comment_ids[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "changemyview    11716\n",
       "Name: subreddit, dtype: int64"
      ]
     },
     "execution_count": 385,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "master_comments_df.loc[master_comments_df['parent_id'].isin(\n",
    "set(missing_comment_ids))]['subreddit'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 365,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "missing_post_ids = list(set(['t3_{}'.format(p) \n",
    "    for p in master_posts_df['id'].values]).difference(processed))\n",
    "len(missing_post_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 84258/84258 [01:47<00:00, 785.72it/s] \n"
     ]
    }
   ],
   "source": [
    "# Check that all comments (at whatever depth) of an OP \n",
    "# have also been processsed\n",
    "op_ids = [x for x in processed if x.startswith('t3_')]\n",
    "print(\"Found {} original posts processed.\".format(len(op_ids)))\n",
    "\n",
    "all_comment_ids = []\n",
    "for op_id in tqdm(op_ids):\n",
    "    with open(os.path.join(\n",
    "        out_dir,'{}.json'.format(op_id)),'r') as f:\n",
    "        json_obj = json.load(f)\n",
    "        all_comment_ids.extend(json_obj['all_comment_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3159572 comments attached to OPs, with 0 missing from processed set.\n"
     ]
    }
   ],
   "source": [
    "# Excellent!\n",
    "\n",
    "print(\"Found {} comments attached to OPs, \\\n",
    "with {} missing from processed set.\".format(\n",
    "    len(all_comment_ids),\n",
    "    len(set(all_comment_ids).difference(processed))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [],
   "source": [
    "post = reddit.submission(id=missing_comment_ids[0].split('_')[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NotFound",
     "evalue": "received 404 HTTP response",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotFound\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-388-cc363c8cbfb1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpost\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauthor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/u/nlp/anaconda/main/anaconda3/envs/yiwei-climate/lib/python3.6/site-packages/praw/models/reddit/base.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, attribute)\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0;34m\"\"\"Return the value of `attribute`.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mattribute\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"_\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetched\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattribute\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         raise AttributeError(\n",
      "\u001b[0;32m/u/nlp/anaconda/main/anaconda3/envs/yiwei-climate/lib/python3.6/site-packages/praw/models/reddit/submission.py\u001b[0m in \u001b[0;36m_fetch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    615\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    616\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 617\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetch_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    618\u001b[0m         \u001b[0msubmission_listing\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcomment_listing\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m         \u001b[0mcomment_listing\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mListing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reddit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcomment_listing\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"data\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/u/nlp/anaconda/main/anaconda3/envs/yiwei-climate/lib/python3.6/site-packages/praw/models/reddit/submission.py\u001b[0m in \u001b[0;36m_fetch_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    612\u001b[0m         \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfields\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetch_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    613\u001b[0m         \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAPI_PATH\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mfields\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 614\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reddit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"GET\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    615\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    616\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/u/nlp/anaconda/main/anaconda3/envs/yiwei-climate/lib/python3.6/site-packages/praw/reddit.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, path, params, data, files, json)\u001b[0m\n\u001b[1;32m    861\u001b[0m                 \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    862\u001b[0m                 \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 863\u001b[0;31m                 \u001b[0mjson\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    864\u001b[0m             )\n\u001b[1;32m    865\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mBadRequest\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/u/nlp/anaconda/main/anaconda3/envs/yiwei-climate/lib/python3.6/site-packages/prawcore/sessions.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, path, data, files, json, params, timeout)\u001b[0m\n\u001b[1;32m    335\u001b[0m             \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m             \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 337\u001b[0;31m             \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    338\u001b[0m         )\n\u001b[1;32m    339\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/u/nlp/anaconda/main/anaconda3/envs/yiwei-climate/lib/python3.6/site-packages/prawcore/sessions.py\u001b[0m in \u001b[0;36m_request_with_retries\u001b[0;34m(self, data, files, json, method, params, timeout, url, retry_strategy_state)\u001b[0m\n\u001b[1;32m    264\u001b[0m             )\n\u001b[1;32m    265\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSTATUS_EXCEPTIONS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 266\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSTATUS_EXCEPTIONS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    267\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mcodes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"no_content\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotFound\u001b[0m: received 404 HTTP response"
     ]
    }
   ],
   "source": [
    "post.author"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {
    "code_folding": [
     8
    ],
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "41337it [11:23:52,  2.07s/it]IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "49641it [13:42:08,  1.23it/s]IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "58302it [16:06:23,  1.06it/s]IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "66806it [18:28:06,  1.15it/s]IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "75541it [20:53:43,  1.01s/it]IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "79925it [22:06:48,  1.00it/s]\n"
     ]
    }
   ],
   "source": [
    "for i,missing_id in tqdm(enumerate(missing_post_ids[0:])):\n",
    "    post_json_name = os.path.join(out_dir,'{}.json'.format(missing_id))\n",
    "    reply_ids = []\n",
    "    deeper_comment_ids = []\n",
    "    \n",
    "    post = reddit.submission(id=missing_id.split('_')[-1])\n",
    "    title = post.title # make non-lazy\n",
    "    \n",
    "    if post._fetched:\n",
    "        #print(post.__dict__)\n",
    "        #print(len(post.__dict__['_comments_by_id']))\n",
    "        p_dict = post.__dict__\n",
    "        p_dict['root'] = missing_id\n",
    "        p_dict['parent_id'] = missing_id\n",
    "        p_dict['top_level_comments'] = []\n",
    "        p_dict['all_comment_ids'] = list(post._comments_by_id.keys())\n",
    "        p_dict['subreddit'] = post.subreddit.display_name\n",
    "        if post.author is not None:\n",
    "            p_dict['author'] = post.author.name\n",
    "        for comment_id in post._comments_by_id:\n",
    "            comm_json_name = os.path.join(out_dir,'{}.json'.format(\n",
    "                comment_id))\n",
    "            c_dict = post._comments_by_id[comment_id].\\\n",
    "                                            __dict__\n",
    "            c_dict['root'] = missing_id\n",
    "            assert 'parent_id' in c_dict\n",
    "            if c_dict['parent_id'] != missing_id: # deeper cmnt (AKA reply)\n",
    "                deeper_comment_ids.append(comment_id)\n",
    "            else: # is a top-level comment\n",
    "                p_dict['top_level_comments'].append(comment_id)\n",
    "            reply_ids_ = ['t1_{}'.format(x.id) for x in \n",
    "                             c_dict['_replies']]\n",
    "            reply_ids.extend(reply_ids_)\n",
    "            c_dict['reply_ids'] = reply_ids_\n",
    "            c_dict['subreddit'] = c_dict['subreddit'].display_name\n",
    "            if c_dict['author'] is not None:\n",
    "                c_dict['author'] = c_dict['author'].name\n",
    "            c_dict = {key: c_dict[key] for key \n",
    "                                        in c_dict if key in \n",
    "                                        COMMENT_FIELDS}\n",
    "            with open(comm_json_name, 'w') as outfile:\n",
    "                json.dump(c_dict, outfile)\n",
    "            \n",
    "        # by the time all comments have been visited, union of replies\n",
    "        # should be the same as union of non-level-one comments\n",
    "        assert set(deeper_comment_ids).difference(\n",
    "            set(reply_ids)) == set()\n",
    "            \n",
    "        p_dict = {key: p_dict[key] for key in p_dict \n",
    "                                  if key in POST_FIELDS}\n",
    "        with open(post_json_name, 'w') as outfile:\n",
    "            json.dump(p_dict, outfile)\n",
    "    \n",
    "# Then go back and visit all original post ids--Done!\n",
    "\n",
    "# Then go back and visit all reply_ids and comment_ids\n",
    "# that don't have a .json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Get a post by its post id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "p_id = '1dc1d2'\n",
    "post = reddit.submission(id=p_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "c_id = 'hcgnm97'\n",
    "comment = reddit.comment(id=c_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['comment_limit',\n",
       " 'comment_sort',\n",
       " 'id',\n",
       " '_reddit',\n",
       " '_fetched',\n",
       " '_comments_by_id',\n",
       " 'approved_at_utc',\n",
       " 'subreddit',\n",
       " 'selftext',\n",
       " 'user_reports',\n",
       " 'saved',\n",
       " 'mod_reason_title',\n",
       " 'gilded',\n",
       " 'clicked',\n",
       " 'title',\n",
       " 'link_flair_richtext',\n",
       " 'subreddit_name_prefixed',\n",
       " 'hidden',\n",
       " 'pwls',\n",
       " 'link_flair_css_class',\n",
       " 'downs',\n",
       " 'thumbnail_height',\n",
       " 'top_awarded_type',\n",
       " 'parent_whitelist_status',\n",
       " 'hide_score',\n",
       " 'name',\n",
       " 'quarantine',\n",
       " 'link_flair_text_color',\n",
       " 'upvote_ratio',\n",
       " 'author_flair_background_color',\n",
       " 'subreddit_type',\n",
       " 'ups',\n",
       " 'total_awards_received',\n",
       " 'media_embed',\n",
       " 'thumbnail_width',\n",
       " 'author_flair_template_id',\n",
       " 'is_original_content',\n",
       " 'author_fullname',\n",
       " 'secure_media',\n",
       " 'is_reddit_media_domain',\n",
       " 'is_meta',\n",
       " 'category',\n",
       " 'secure_media_embed',\n",
       " 'link_flair_text',\n",
       " 'can_mod_post',\n",
       " 'score',\n",
       " 'approved_by',\n",
       " 'is_created_from_ads_ui',\n",
       " 'author_premium',\n",
       " 'thumbnail',\n",
       " 'edited',\n",
       " 'author_flair_css_class',\n",
       " 'author_flair_richtext',\n",
       " 'gildings',\n",
       " 'content_categories',\n",
       " 'is_self',\n",
       " 'mod_note',\n",
       " 'created',\n",
       " 'link_flair_type',\n",
       " 'wls',\n",
       " 'removed_by_category',\n",
       " 'banned_by',\n",
       " 'author_flair_type',\n",
       " 'domain',\n",
       " 'allow_live_comments',\n",
       " 'selftext_html',\n",
       " 'likes',\n",
       " 'suggested_sort',\n",
       " 'banned_at_utc',\n",
       " 'view_count',\n",
       " 'archived',\n",
       " 'no_follow',\n",
       " 'is_crosspostable',\n",
       " 'pinned',\n",
       " 'over_18',\n",
       " 'all_awardings',\n",
       " 'awarders',\n",
       " 'media_only',\n",
       " 'can_gild',\n",
       " 'spoiler',\n",
       " 'locked',\n",
       " 'author_flair_text',\n",
       " 'treatment_tags',\n",
       " 'visited',\n",
       " 'removed_by',\n",
       " 'num_reports',\n",
       " 'distinguished',\n",
       " 'subreddit_id',\n",
       " 'author_is_blocked',\n",
       " 'mod_reason_by',\n",
       " 'removal_reason',\n",
       " 'link_flair_background_color',\n",
       " 'is_robot_indexable',\n",
       " 'num_duplicates',\n",
       " 'report_reasons',\n",
       " 'author',\n",
       " 'discussion_type',\n",
       " 'num_comments',\n",
       " 'send_replies',\n",
       " 'media',\n",
       " 'contest_mode',\n",
       " 'author_patreon_flair',\n",
       " 'author_flair_text_color',\n",
       " 'permalink',\n",
       " 'whitelist_status',\n",
       " 'stickied',\n",
       " 'url',\n",
       " 'subreddit_subscribers',\n",
       " 'created_utc',\n",
       " 'num_crossposts',\n",
       " 'mod_reports',\n",
       " 'is_video',\n",
       " '_comments']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(post.__dict__.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_replies': [],\n",
       " '_submission': None,\n",
       " 'id': 'hcgnm97',\n",
       " '_reddit': <praw.reddit.Reddit at 0x7fae5d134cf8>,\n",
       " '_fetched': True,\n",
       " 'subreddit_id': 't5_2w2s8',\n",
       " 'approved_at_utc': None,\n",
       " 'author_is_blocked': False,\n",
       " 'comment_type': None,\n",
       " 'edited': False,\n",
       " 'mod_reason_by': None,\n",
       " 'banned_by': None,\n",
       " 'ups': 5,\n",
       " 'num_reports': None,\n",
       " 'author_flair_type': 'text',\n",
       " 'total_awards_received': 0,\n",
       " 'subreddit': Subreddit(display_name='changemyview'),\n",
       " 'author_flair_template_id': None,\n",
       " 'likes': None,\n",
       " 'user_reports': [],\n",
       " 'saved': False,\n",
       " 'banned_at_utc': None,\n",
       " 'mod_reason_title': None,\n",
       " 'gilded': 0,\n",
       " 'archived': False,\n",
       " 'collapsed_reason_code': None,\n",
       " 'no_follow': False,\n",
       " 'author': Redditor(name='MrJive01'),\n",
       " 'can_mod_post': False,\n",
       " 'send_replies': True,\n",
       " 'parent_id': 't3_pm9gph',\n",
       " 'score': 5,\n",
       " 'author_fullname': 't2_1nay0pca',\n",
       " 'report_reasons': None,\n",
       " 'removal_reason': None,\n",
       " 'approved_by': None,\n",
       " 'all_awardings': [],\n",
       " 'body': \"We can't do it as long as the medical insurance industry is still going.  They'll push every terminal patient to it so long as its profitable.\",\n",
       " 'awarders': [],\n",
       " 'top_awarded_type': None,\n",
       " 'downs': 0,\n",
       " 'author_flair_css_class': None,\n",
       " 'author_patreon_flair': False,\n",
       " 'collapsed': False,\n",
       " 'author_flair_richtext': [],\n",
       " 'is_submitter': False,\n",
       " 'body_html': '<div class=\"md\"><p>We can&#39;t do it as long as the medical insurance industry is still going.  They&#39;ll push every terminal patient to it so long as its profitable.</p>\\n</div>',\n",
       " 'gildings': {},\n",
       " 'collapsed_reason': None,\n",
       " 'associated_award': None,\n",
       " 'stickied': False,\n",
       " 'author_premium': False,\n",
       " 'can_gild': True,\n",
       " 'link_id': 't3_pm9gph',\n",
       " 'unrepliable_reason': None,\n",
       " 'author_flair_text_color': None,\n",
       " 'score_hidden': False,\n",
       " 'permalink': '/r/changemyview/comments/pm9gph/cmv_humane_euthanisia_should_be_legal/hcgnm97/',\n",
       " 'subreddit_type': 'public',\n",
       " 'locked': False,\n",
       " 'name': 't1_hcgnm97',\n",
       " 'created': 1631384177.0,\n",
       " 'author_flair_text': None,\n",
       " 'treatment_tags': [],\n",
       " 'created_utc': 1631384177.0,\n",
       " 'subreddit_name_prefixed': 'r/changemyview',\n",
       " 'controversiality': 0,\n",
       " 'author_flair_background_color': None,\n",
       " 'collapsed_because_crowd_control': None,\n",
       " 'mod_reports': [],\n",
       " 'mod_note': None,\n",
       " 'distinguished': None}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comment.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#[x.split('_')[-1] for x in post.__dict__['_comments_by_id'].keys()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://www.reddit.com/r/changemyview/comments/2yrvob/cmvthe_general_public_should_be_left_in_the_dark/'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "post.url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "post.is_self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The general public should not have any say in science.\\n\\nAt my university we had a discussion about the tranparency of science for the general public with students and staff. One of the students brought forward this statement, but aside from the comment that it was ridicules the statement was ignored by the scientific staff. As I would like it better for someone to (try) change my view then it just being ignored I decide to try it here. \\n\\nNow why do I believe this: \\n- As science advances, science will be increasingly difficult to explain to the general public. \\nTo illustrate: (in general) classical fysics is quite intuitive and visible. Drop something from a height and you can see gravity at work. However quantum fysics has a lot of counterintuitive aspects. Something can be 2 things at the same time (wave vs particle).\\nThis on itself is not really a big problem. However the human society has a fear for the unknown. These two combined lead to people who disstrust science because they just don't understand it. \\n\\nExample: The (rather small, but still present) demonstrations against CERN, because they believe it will destroy the world.\\n\\n- People fear the bad more than the appreciate the good. \\nThis is a problem as the general public does not have the expertise to know what is true and what is false and therefor are dependent on what they are told. When 1 person says A and 2 say B the it is logical to say B will be more likely be true. However this doesn't work in real life. \\nWithout taking in consideration what is correct let's say we have one scientist say something is bad and one say it's good, the general public will think it's bad. If we have 1 scientist for bad and 2 saying good, still the general public will not trust the 'something'. With these low numbers this is only logic, however we also see this with realistic numbers of scientists/professionals. Just look at the climate change 'debate' or the vaccine/anti-vaccine 'debate'. \\n\\nExample: The anti-vaccine movement is (almost) only based on the 'Vaccines causes autism' paper. Not considering that the paper is retracted, there are a lot of papers saying this is not true, almost all docters say vaccines do not cause autism. Still there are people who believe the old retracted vaccines causes autism paper. \\n\\n- The general public do not have the expertise to know what is important and what is not. \\nTo do science money is needed. How much a project is getting is (not all) dependent on how it is viewed in the eyes of the general public. However the general public can not tell which is imporatant and which is not. So they should not play any part in dividing this money. Also besides lacking the knowlegde about science, there is also religion and other ideologies that can put certain aspects of science in a bad light and thus stop the money flow to these science projects. (cloning vs religion) \\n\\n- The average media want senation\\nScience can be very boring. The general public do not like boring and media that only do boring are ignored. So media will not do boring and are looking for senation. To report on a subject where there is almost a concensus is boring so media will create debate. This is creates the diffences in how science view things and how the general public view things. \\n\\nExample: Climate change debate. I can't explain it better than [John Oliver](https://www.youtube.com/watch?v=cjuGCJJUGsg)\\n\\n- Science is actually already doing stuff without telling the general public.\\nFor example how many of you know that we use nanotechnology in regular sunscreens nowadays.\\n\\nI realise that a lot of this can be solved by better and more information for the general public. However the gap between  of the general public and the scientific community is so big I feel it is better for science to ignore the general public untill they get smarter.\\n\\n**tl;dr the general public is to stupid, science should just ignore them**\\n\\nEDIT1: During the discussion I noticed that my statement is actually not complete. It should be: The general public should be kept in the dark about science developments until they are capable to understand these developments.\\nEDIT2: CMV succesful. As /u/Namemedickles pointed out most of the issues I was referring to are not caused by poor understanding but rather due to beliefs or agendas. These will not be changed in the maner describe at EDIT1\\n\\n/u/Hq3473 makes a very good point, which comes down to (if I understand correctly) what stops the world to come up with a new 'superstition'. And why do they have to believe science rather than the shamans. \\n\\nAnd /u/Znyper came up with problems that might arise when scientists have to make value jugdments. I think he explains this better so for more in the comments. \\n\\nIf I paraphrased you guys wrong please let me know\\n_____\\n\\n> *Hello, users of CMV! This is a footnote from your moderators. We'd just like to remind you of a couple of things. Firstly, please remember to* ***[read through our rules](http://www.reddit.com/r/changemyview/wiki/rules)***. *If you see a comment that has broken one, it is more effective to report it than downvote it. Speaking of which,* ***[downvotes don't change views](http://www.reddit.com/r/changemyview/wiki/guidelines#wiki_upvoting.2Fdownvoting)****! If you are thinking about submitting a CMV yourself, please have a look through our* ***[popular topics wiki](http://www.reddit.com/r/changemyview/wiki/populartopics)*** *first. Any questions or concerns? Feel free to* ***[message us](http://www.reddit.com/message/compose?to=/r/changemyview)***. *Happy CMVing!*\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "post.selftext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Get posts from a subreddit (e.g. r/spambotwatch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting submissions and comments from: spambotwatch ...\n",
      "Wrote output to: praw_output/spambotwatch.tsv\n"
     ]
    }
   ],
   "source": [
    "get_praw_submissions(reddit,'spambotwatch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>date</th>\n",
       "      <th>is_video</th>\n",
       "      <th>id</th>\n",
       "      <th>num_downs</th>\n",
       "      <th>num_ups</th>\n",
       "      <th>upvote_ratio</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>score</th>\n",
       "      <th>text</th>\n",
       "      <th>subreddit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>JEEVAN BOBY (u/jeevanbobyvallickad) - Reddit</td>\n",
       "      <td>TheGeorge</td>\n",
       "      <td>1.566348e+09</td>\n",
       "      <td>False</td>\n",
       "      <td>ct1svp</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>spambotwatch</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>overview for funnynova</td>\n",
       "      <td>BuckRowdy</td>\n",
       "      <td>1.555999e+09</td>\n",
       "      <td>False</td>\n",
       "      <td>bg7w8w</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>spambotwatch</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Spams Dating Tips Websites all with Stolen Con...</td>\n",
       "      <td>TheGeorge</td>\n",
       "      <td>1.554842e+09</td>\n",
       "      <td>False</td>\n",
       "      <td>bb7e57</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>spambotwatch</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>overview for poopcake5</td>\n",
       "      <td>ActionScripter9109</td>\n",
       "      <td>1.501372e+09</td>\n",
       "      <td>False</td>\n",
       "      <td>6qbmgl</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>spambotwatch</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>overview for sutei_m</td>\n",
       "      <td>ActionScripter9109</td>\n",
       "      <td>1.501372e+09</td>\n",
       "      <td>False</td>\n",
       "      <td>6qbmar</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>spambotwatch</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title              author  \\\n",
       "0       JEEVAN BOBY (u/jeevanbobyvallickad) - Reddit           TheGeorge   \n",
       "1                             overview for funnynova           BuckRowdy   \n",
       "2  Spams Dating Tips Websites all with Stolen Con...           TheGeorge   \n",
       "3                             overview for poopcake5  ActionScripter9109   \n",
       "4                               overview for sutei_m  ActionScripter9109   \n",
       "\n",
       "           date  is_video      id  num_downs  num_ups  upvote_ratio  \\\n",
       "0  1.566348e+09     False  ct1svp          0        1           1.0   \n",
       "1  1.555999e+09     False  bg7w8w          0        1           1.0   \n",
       "2  1.554842e+09     False  bb7e57          0        1           1.0   \n",
       "3  1.501372e+09     False  6qbmgl          0        1           1.0   \n",
       "4  1.501372e+09     False  6qbmar          0        1           1.0   \n",
       "\n",
       "   num_comments  score  text     subreddit  \n",
       "0             1      1   NaN  spambotwatch  \n",
       "1             0      1   NaN  spambotwatch  \n",
       "2             0      1   NaN  spambotwatch  \n",
       "3             1      1   NaN  spambotwatch  \n",
       "4             1      1   NaN  spambotwatch  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spambotwatch_df = pd.read_csv('praw_output/spambotwatch.tsv',sep='\\t',header=0)\n",
    "spambotwatch_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Get posts from list of subreddits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for sub in non_niche_subs:\n",
    "    get_submissions(reddit,sub)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Inspect output: tsv of subreddits and meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>climateskeptics</td>\n",
       "      <td>Climate Skeptics: Trying to see through the al...</td>\n",
       "      <td>Seeing past hyperbole, alarmism and environmen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>skeptic</td>\n",
       "      <td>skeptic</td>\n",
       "      <td>## [Click this link to Read the Rules](http://...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>climatechange</td>\n",
       "      <td>A place for a rational discussion on a divisiv...</td>\n",
       "      <td>This is a place for the rational discussion of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>climate</td>\n",
       "      <td>Information about the world's climate</td>\n",
       "      <td>Real and accurate data about the Earth's clima...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>science</td>\n",
       "      <td>Reddit Science</td>\n",
       "      <td># [Submission Rules](https://www.reddit.com/r/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>republicans</td>\n",
       "      <td>Republicans - RNC - GOP: Grand Old Party</td>\n",
       "      <td>Republican, RNC and GOP news, issues, gossip, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>askaconservative</td>\n",
       "      <td>Ask A Conservative: Ask Conservatives And Repu...</td>\n",
       "      <td>#[Ask a Conservative](/r/askaconservative)\\n\\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>Conservative</td>\n",
       "      <td>Conservative</td>\n",
       "      <td>#####\\n**[Join us on discord.](https://discord...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>conservatives</td>\n",
       "      <td>conservatives</td>\n",
       "      <td>Conservatism (from, conservare, \"to preserve\")...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>AskTrumpSupporters</td>\n",
       "      <td>AskTrumpSupporters</td>\n",
       "      <td>#We are not a typical subreddit. Read the [ful...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>68 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     0                                                  1  \\\n",
       "0      climateskeptics  Climate Skeptics: Trying to see through the al...   \n",
       "1              skeptic                                            skeptic   \n",
       "2        climatechange  A place for a rational discussion on a divisiv...   \n",
       "3              climate              Information about the world's climate   \n",
       "4              science                                     Reddit Science   \n",
       "..                 ...                                                ...   \n",
       "63         republicans           Republicans - RNC - GOP: Grand Old Party   \n",
       "64    askaconservative  Ask A Conservative: Ask Conservatives And Repu...   \n",
       "65        Conservative                                       Conservative   \n",
       "66       conservatives                                      conservatives   \n",
       "67  AskTrumpSupporters                                 AskTrumpSupporters   \n",
       "\n",
       "                                                    2  \n",
       "0   Seeing past hyperbole, alarmism and environmen...  \n",
       "1   ## [Click this link to Read the Rules](http://...  \n",
       "2   This is a place for the rational discussion of...  \n",
       "3   Real and accurate data about the Earth's clima...  \n",
       "4   # [Submission Rules](https://www.reddit.com/r/...  \n",
       "..                                                ...  \n",
       "63  Republican, RNC and GOP news, issues, gossip, ...  \n",
       "64  #[Ask a Conservative](/r/askaconservative)\\n\\n...  \n",
       "65  #####\\n**[Join us on discord.](https://discord...  \n",
       "66  Conservatism (from, conservare, \"to preserve\")...  \n",
       "67  #We are not a typical subreddit. Read the [ful...  \n",
       "\n",
       "[68 rows x 3 columns]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv('subreddits.tsv',sep='\\t',header=None).drop_duplicates(0,keep='first')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Inspect output: tsv of one subreddit's posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('praw_output/350.tsv',sep='\\t',header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['title', 'author', 'date', 'is_video', 'id', 'num_downs', 'num_ups',\n",
       "       'upvote_ratio', 'num_comments', 'score', 'text', 'subreddit'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    127\n",
       "Name: is_video, dtype: int64"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.is_video.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "350    127\n",
       "Name: subreddit, dtype: int64"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.subreddit.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#df.head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Get comments for all posts with non-zero num comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "code_folding": [
     1
    ],
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already got comments for subreddit praw_output/posts/350.tsv\n",
      "Already got comments for subreddit praw_output/posts/350ppm.tsv\n",
      "Already got comments for subreddit praw_output/posts/askaconservative.tsv\n",
      "Already got comments for subreddit praw_output/posts/AskTrumpSupporters.tsv\n",
      "Already got comments for subreddit praw_output/posts/carboncapture.tsv\n",
      "Already got comments for subreddit praw_output/posts/carbontax.tsv\n",
      "Already got comments for subreddit praw_output/posts/ccfunding.tsv\n",
      "Already got comments for subreddit praw_output/posts/climate.tsv\n",
      "Already got comments for subreddit praw_output/posts/climate_activism.tsv\n",
      "Already got comments for subreddit praw_output/posts/climate_discussion.tsv\n",
      "Already got comments for subreddit praw_output/posts/climate_science.tsv\n",
      "Already got comments for subreddit praw_output/posts/ClimateActionPlan.tsv\n",
      "Already got comments for subreddit praw_output/posts/climatechange.tsv\n",
      "Already got comments for subreddit praw_output/posts/ClimateChangeCancer.tsv\n",
      "Already got comments for subreddit praw_output/posts/climatecmv.tsv\n",
      "Already got comments for subreddit praw_output/posts/ClimateCrisis.tsv\n",
      "Already got comments for subreddit praw_output/posts/climatedebate.tsv\n",
      "Already got comments for subreddit praw_output/posts/climatedebate2.tsv\n",
      "Already got comments for subreddit praw_output/posts/climategate.tsv\n",
      "0 comments among all posts in subreddit: ClimateMobilization\n",
      "Already got comments for subreddit praw_output/posts/ClimateOffensive.tsv\n",
      "Already got comments for subreddit praw_output/posts/climatepredicts.tsv\n",
      "Already got comments for subreddit praw_output/posts/ClimatePreparation.tsv\n",
      "Already got comments for subreddit praw_output/posts/climatesecurity.tsv\n",
      "Already got comments for subreddit praw_output/posts/climateskeptics.tsv\n",
      "Already got comments for subreddit praw_output/posts/ClimateSkepticScience.tsv\n",
      "Already got comments for subreddit praw_output/posts/ClimateSplattergate.tsv\n",
      "0 comments among all posts in subreddit: climatestasis\n",
      "Already got comments for subreddit praw_output/posts/Climatology.tsv\n",
      "Already got comments for subreddit praw_output/posts/climatskeptics.tsv\n",
      "Already got comments for subreddit praw_output/posts/CollapseSurvival.tsv\n",
      "Already got comments for subreddit praw_output/posts/conservation.tsv\n",
      "Already got comments for subreddit praw_output/posts/Conservative.tsv\n",
      "Already got comments for subreddit praw_output/posts/conservatives.tsv\n",
      "Subreddit praw_output/posts/DebateAClimateSkeptic.tsv has no posts\n",
      "Getting comments from posts in subreddit: deep_ecology\n",
      "Getting comments from posts in subreddit: drought\n",
      "Getting comments from posts in subreddit: EarthDisaster\n",
      "Getting comments from posts in subreddit: EarthScience\n",
      "Getting comments from posts in subreddit: EcoInternet\n",
      "Getting comments from posts in subreddit: ecology\n",
      "Subreddit praw_output/posts/ecologycenter.tsv has no posts\n",
      "Getting comments from posts in subreddit: EcoRestoration\n",
      "Getting comments from posts in subreddit: energy\n",
      "Getting comments from posts in subreddit: enviroaction\n",
      "Getting comments from posts in subreddit: Enviroment\n",
      "Getting comments from posts in subreddit: environment\n",
      "Getting comments from posts in subreddit: environmental_science\n",
      "Subreddit praw_output/posts/Global_Warming.tsv has no posts\n",
      "Getting comments from posts in subreddit: GlobalClimateChange\n",
      "Getting comments from posts in subreddit: GlobalWarming\n",
      "Getting comments from posts in subreddit: GlobalWarmingisBunk\n",
      "Getting comments from posts in subreddit: Green\n",
      "Getting comments from posts in subreddit: GreenNewIdeas\n",
      "0 comments among all posts in subreddit: GreenPlanet\n",
      "0 comments among all posts in subreddit: GWB\n",
      "Getting comments from posts in subreddit: Rainforest\n",
      "Subreddit praw_output/posts/RealClimateSkeptics.tsv has no posts\n",
      "Getting comments from posts in subreddit: Republican\n",
      "Getting comments from posts in subreddit: republicans\n",
      "Getting comments from posts in subreddit: Restoration_Ecology\n",
      "Getting comments from posts in subreddit: science\n",
      "Getting comments from posts in subreddit: skeptic\n",
      "Getting comments from posts in subreddit: sustainability\n",
      "Getting comments from posts in subreddit: Sustainable_Energy\n",
      "Subreddit praw_output/posts/ThunbergSyndrome.tsv has no posts\n",
      "Getting comments from posts in subreddit: trueclimateskeptics\n",
      "0 comments among all posts in subreddit: WorldClimate\n"
     ]
    }
   ],
   "source": [
    "for subreddit_tsv in glob.glob('praw_output/posts/*.tsv'):\n",
    "    if os.path.exists('praw_output/post_comments/{}_COMMENTS.tsv'.format(subreddit_tsv.split('/')[-1][:-4])):\n",
    "        print('Already got comments for subreddit {}'.format(subreddit_tsv))\n",
    "    else:\n",
    "        subreddit_posts = pd.read_csv(subreddit_tsv,sep='\\t',header=0)\n",
    "        if len(subreddit_posts) > 0:\n",
    "            subreddit = str(subreddit_posts.iloc[0]['subreddit'])\n",
    "            posts_with_comments = subreddit_posts.loc[subreddit_posts.num_comments > 0]\n",
    "            if len(posts_with_comments) > 0:\n",
    "                print('Getting comments from posts in subreddit: {}'.format(subreddit))\n",
    "                for ix,row in posts_with_comments.iterrows():\n",
    "                    get_submission_comments(reddit,subreddit,row['id'])\n",
    "            else:\n",
    "                print('0 comments among all posts in subreddit: {}'.format(subreddit))\n",
    "        else:\n",
    "            print('Subreddit {} has no posts'.format(subreddit_tsv))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Use Pushshift API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The Pushshift API uses the requests library to make requests to 3 possible endpoints:\n",
    "    \n",
    "* /reddit/comment/search (corresponding to a comment)\n",
    "* /reddit/submission/search (corresponding to a post)\n",
    "* /reddit/subreddit/search (corresponding to a subreddit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The script below submits a request for a given query (i.e. keyword or set of keywords contained), a before and after date, and a given datatype (one of the 3 endpoints). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "code_folding": [
     2
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "api = PushshiftAPI()\n",
    "\n",
    "def getPushshiftData(query, after_str, before_str, datatype):\n",
    "    before_date = datetime.datetime.strptime(before_str, \"%m-%d-%Y\")\n",
    "    after_date = datetime.datetime.strptime(after_str, \"%m-%d-%Y\")\n",
    "    before_timestamp = int(datetime.datetime.timestamp(before_date))\n",
    "    after_timestamp = int(datetime.datetime.timestamp(after_date))\n",
    "    \n",
    "    query_prefix = 'title' if datatype == 'submission' else 'q'\n",
    "    url = 'https://api.pushshift.io/reddit/search/'+datatype+'/?'+\\\n",
    "            query_prefix+'='+str(query)+'&size=1000&after='+str(after_date)+\\\n",
    "            '&before='+str(before_date)\n",
    "    #print(url)\n",
    "    r = requests.get(url)\n",
    "    data = json.loads(r.text)\n",
    "    \n",
    "    return data['data']\n",
    "\n",
    "def getPushshiftDataForSub(subreddit, query, y2, m2, d2, y1, m1, d1, datatype, limit=10000):\n",
    "    \n",
    "    before = int(dt.datetime(y1,m1,d1,0,0).timestamp())\n",
    "    after = int(dt.datetime(y2,m2,d2,0,0).timestamp())\n",
    "    \n",
    "    if datatype=='submissions':\n",
    "        out = api.search_submissions(subreddit=subreddit, q=query, limit=limit, \n",
    "                             before=before, after=after)\n",
    "    else:\n",
    "        out = api.search_comments(subreddit=subreddit, q=query, limit=limit, \n",
    "                             before=before, after=after)\n",
    "        \n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "E.g., to get all posts containing 'climate change' between Jan. 1, 2020 and Feb. 1, 2020, we run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Not all PushShift shards are active. Query results may be incomplete.\n",
      "Not all PushShift shards are active. Query results may be incomplete.\n",
      "Not all PushShift shards are active. Query results may be incomplete.\n",
      "Not all PushShift shards are active. Query results may be incomplete.\n",
      "Not all PushShift shards are active. Query results may be incomplete.\n",
      "Not all PushShift shards are active. Query results may be incomplete.\n",
      "Not all PushShift shards are active. Query results may be incomplete.\n",
      "Not all PushShift shards are active. Query results may be incomplete.\n",
      "Not all PushShift shards are active. Query results may be incomplete.\n",
      "Not all PushShift shards are active. Query results may be incomplete.\n",
      "Not all PushShift shards are active. Query results may be incomplete.\n",
      "Not all PushShift shards are active. Query results may be incomplete.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total:: Success Rate: 100.00% - Requests: 101 - Batches: 11 - Items Remaining: 0\n"
     ]
    }
   ],
   "source": [
    "comments = getPushshiftDataForSub('changemyview',None,2014,1,1,2014,12,31,\n",
    "                                  'comments')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>author_flair_css_class</th>\n",
       "      <th>author_flair_text</th>\n",
       "      <th>body</th>\n",
       "      <th>controversiality</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>distinguished</th>\n",
       "      <th>gilded</th>\n",
       "      <th>id</th>\n",
       "      <th>link_id</th>\n",
       "      <th>...</th>\n",
       "      <th>reply_delay</th>\n",
       "      <th>retrieved_on</th>\n",
       "      <th>score</th>\n",
       "      <th>score_hidden</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>subreddit_id</th>\n",
       "      <th>author_created_utc</th>\n",
       "      <th>author_fullname</th>\n",
       "      <th>user_removed</th>\n",
       "      <th>edited</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[deleted]</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Why are your stance that women can't enjoy dar...</td>\n",
       "      <td>0</td>\n",
       "      <td>1397998076</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>cgx96ag</td>\n",
       "      <td>t3_23erso</td>\n",
       "      <td>...</td>\n",
       "      <td>119858</td>\n",
       "      <td>1433438423</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>changemyview</td>\n",
       "      <td>t5_2w2s8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>UncharminglyWitty</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>What about a kid walking home from school?</td>\n",
       "      <td>0</td>\n",
       "      <td>1397997927</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>cgx957p</td>\n",
       "      <td>t3_23hgnt</td>\n",
       "      <td>...</td>\n",
       "      <td>22288</td>\n",
       "      <td>1433438410</td>\n",
       "      <td>2</td>\n",
       "      <td>False</td>\n",
       "      <td>changemyview</td>\n",
       "      <td>t5_2w2s8</td>\n",
       "      <td>1.366138e+09</td>\n",
       "      <td>t2_bc9qn</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>garnteller</td>\n",
       "      <td>None</td>\n",
       "      <td>108Δ</td>\n",
       "      <td>A few thoughts:\\n\\n1. **Ownership of a sub** F...</td>\n",
       "      <td>0</td>\n",
       "      <td>1397997921</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>cgx956g</td>\n",
       "      <td>t3_23i0xv</td>\n",
       "      <td>...</td>\n",
       "      <td>13103</td>\n",
       "      <td>1433438409</td>\n",
       "      <td>2</td>\n",
       "      <td>False</td>\n",
       "      <td>changemyview</td>\n",
       "      <td>t5_2w2s8</td>\n",
       "      <td>1.386191e+09</td>\n",
       "      <td>t2_e7e7z</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>theivesinthenight</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Yes they do have to change their actions. You ...</td>\n",
       "      <td>0</td>\n",
       "      <td>1397997882</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>cgx94vw</td>\n",
       "      <td>t3_23ev1v</td>\n",
       "      <td>...</td>\n",
       "      <td>919</td>\n",
       "      <td>1433438404</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>changemyview</td>\n",
       "      <td>t5_2w2s8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-moose-</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>http://www.reddit.com/r/moosearchive/comments/...</td>\n",
       "      <td>0</td>\n",
       "      <td>1397997836</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>cgx94jp</td>\n",
       "      <td>t3_23dc1g</td>\n",
       "      <td>...</td>\n",
       "      <td>132438</td>\n",
       "      <td>1433438401</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>changemyview</td>\n",
       "      <td>t5_2w2s8</td>\n",
       "      <td>1.369286e+09</td>\n",
       "      <td>t2_brzdj</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              author author_flair_css_class author_flair_text  \\\n",
       "0          [deleted]                   None              None   \n",
       "1  UncharminglyWitty                   None              None   \n",
       "2         garnteller                   None              108Δ   \n",
       "3  theivesinthenight                   None              None   \n",
       "4            -moose-                   None              None   \n",
       "\n",
       "                                                body  controversiality  \\\n",
       "0  Why are your stance that women can't enjoy dar...                 0   \n",
       "1         What about a kid walking home from school?                 0   \n",
       "2  A few thoughts:\\n\\n1. **Ownership of a sub** F...                 0   \n",
       "3  Yes they do have to change their actions. You ...                 0   \n",
       "4  http://www.reddit.com/r/moosearchive/comments/...                 0   \n",
       "\n",
       "   created_utc distinguished  gilded       id    link_id  ...  reply_delay  \\\n",
       "0   1397998076          None       0  cgx96ag  t3_23erso  ...       119858   \n",
       "1   1397997927          None       0  cgx957p  t3_23hgnt  ...        22288   \n",
       "2   1397997921          None       0  cgx956g  t3_23i0xv  ...        13103   \n",
       "3   1397997882          None       0  cgx94vw  t3_23ev1v  ...          919   \n",
       "4   1397997836          None       0  cgx94jp  t3_23dc1g  ...       132438   \n",
       "\n",
       "  retrieved_on  score  score_hidden     subreddit  subreddit_id  \\\n",
       "0   1433438423      0         False  changemyview      t5_2w2s8   \n",
       "1   1433438410      2         False  changemyview      t5_2w2s8   \n",
       "2   1433438409      2         False  changemyview      t5_2w2s8   \n",
       "3   1433438404      1         False  changemyview      t5_2w2s8   \n",
       "4   1433438401      1         False  changemyview      t5_2w2s8   \n",
       "\n",
       "  author_created_utc author_fullname  user_removed edited  \n",
       "0                NaN             NaN           NaN    NaN  \n",
       "1       1.366138e+09        t2_bc9qn           NaN    NaN  \n",
       "2       1.386191e+09        t2_e7e7z           NaN    NaN  \n",
       "3                NaN             NaN           NaN    NaN  \n",
       "4       1.369286e+09        t2_brzdj           NaN    NaN  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments_df = pd.DataFrame(comments)\n",
    "# preview the comments data\n",
    "comments_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#comments_df['selftext'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     2
    ],
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Collect all posts w/ climate change keyword from CMV  \n",
    "\n",
    "# for start_year in range(2013,2022,1):\n",
    "#     for keyword in KEYWORDS_HI_PREC:\n",
    "#         save_dir = os.path.join('pmaw_output','submissions','changemyview',\n",
    "#                                 '1-1-{}_to_12-31-{}'.format(start_year,start_year))\n",
    "#         if not os.path.exists(save_dir):\n",
    "#             os.makedirs(save_dir)\n",
    "#         save_path = os.path.join(save_dir,'{}.csv'.format(keyword))\n",
    "#         if not os.path.exists(save_path):\n",
    "#             print(\"Missing {}, {}\".format(keyword,start_year))\n",
    "#             posts = getPushshiftDataForSub('changemyview',keyword,\n",
    "#                                            start_year,1,1,start_year,12,31,\n",
    "#                                            'submissions')\n",
    "#             posts_df = pd.DataFrame(posts)\n",
    "#             posts_df.to_csv(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of df, pre-deduplication: 90016\n",
      "Size of df, post-deduplication: 90001\n",
      "False    52075\n",
      "True     37926\n",
      "Name: changed_view, dtype: int64\n",
      "\n",
      "Saving deduplicated df of posts to: pmaw_output/post_comments/changemyview_background/        1-1-2010_to_09-24-2021.csv\n"
     ]
    }
   ],
   "source": [
    "# Collect all comments from CMV as background\n",
    "\n",
    "for start_year in range(2013,2022,1):\n",
    "    #for keyword in KEYWORDS_HI_PREC:\n",
    "        #save_dir = os.path.join('pmaw_output','submissions','changemyview_background',\n",
    "        #                        '1-1-{}_to_12-31-{}'.format(start_year,start_year))\n",
    "        #if not os.path.exists(save_dir):\n",
    "        #    os.makedirs(save_dir)\n",
    "    save_path = os.path.join('pmaw_output','post_comments','changemyview_background',\n",
    "                                '1-1-{}_to_12-31-{}.csv'.format(start_year,start_year))\n",
    "    if not os.path.exists(save_path):\n",
    "        print(\"Missing background posts for {}\".format(start_year))\n",
    "        posts = getPushshiftDataForSub('changemyview',None,\n",
    "                                       start_year,1,1,start_year,12,31,\n",
    "                                       'comments')\n",
    "        posts_df = pd.DataFrame(posts)\n",
    "        posts_df.to_csv(save_path)\n",
    "        \n",
    "# Append individual dataframes into one large one to share\n",
    "\n",
    "df = pd.DataFrame(columns=list(COLUMNS)+['keyword'])\n",
    "\n",
    "for start_year in range(2013,2022,1):\n",
    "    save_path = os.path.join('pmaw_output','post_comments','changemyview_background',\n",
    "                                '1-1-{}_to_12-31-{}.csv'.format(start_year,start_year))\n",
    "    df_ = pd.read_csv(save_path,index_col=0)\n",
    "    if len(df_) > 0:\n",
    "        missing_cols = set(df.columns).difference(set(df_.columns))\n",
    "        for missing_col in missing_cols:\n",
    "            df_[missing_col] = [None]*len(df_)\n",
    "        df = pd.concat([df,df_],ignore_index=True,axis=0)\n",
    "        \n",
    "# Deduplicate by ID \n",
    "print('Size of df, pre-deduplication:',len(df))\n",
    "df.drop_duplicates(subset='id',inplace=True)\n",
    "print('Size of df, post-deduplication:',len(df))\n",
    "\n",
    "# Annotate with whether comment awarded delta or not (if ∆ is present in `author_flair_text`)\n",
    "df['changed_view'] = df['author_flair_text'].apply(\n",
    "                            lambda x: '∆' in x if type(x) == str else False\n",
    ")\n",
    "print(df['changed_view'].value_counts())\n",
    "\n",
    "# UPDATE TO CURRENT DATE\n",
    "from datetime import date\n",
    "today = date.today()\n",
    "str_today = today.strftime(\"%m-%d-%Y\")\n",
    "print('\\nSaving deduplicated df of posts to: pmaw_output/post_comments/changemyview_background/\\\n",
    "        1-1-2010_to_{}.csv'.format(str_today))\n",
    "\n",
    "df.to_csv('pmaw_output/post_comments/changemyview_background/1-1-2010_to_{}.csv'.format(str_today))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/u/nlp/anaconda/main/anaconda3/envs/yiwei-climate/lib/python3.6/site-packages/IPython/core/interactiveshell.py:3072: DtypeWarning: Columns (23,34,61,62,63,68,69,71) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n",
      "/u/nlp/anaconda/main/anaconda3/envs/yiwei-climate/lib/python3.6/site-packages/IPython/core/interactiveshell.py:3072: DtypeWarning: Columns (67,69,70,71,72) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of df, pre-deduplication: 79925\n",
      "Size of df, post-deduplication: 79925\n",
      "False    72289\n",
      "True      7636\n",
      "Name: delta_history, dtype: int64\n",
      "\n",
      "Saving deduplicated df of posts to: pmaw_output/submissions/changemyview_background/1-1-2010_to_09-27-2021.csv\n"
     ]
    }
   ],
   "source": [
    "# Collect all posts from CMV as background--comments seem incomplete\n",
    "\n",
    "for start_year in range(2013,2022,1):\n",
    "    #for keyword in KEYWORDS_HI_PREC:\n",
    "        #save_dir = os.path.join('pmaw_output','submissions','changemyview_background',\n",
    "        #                        '1-1-{}_to_12-31-{}'.format(start_year,start_year))\n",
    "        #if not os.path.exists(save_dir):\n",
    "        #    os.makedirs(save_dir)\n",
    "    save_path = os.path.join('pmaw_output','submissions','changemyview_background',\n",
    "                                '1-1-{}_to_12-31-{}.csv'.format(start_year,start_year))\n",
    "    if not os.path.exists(save_path):\n",
    "        print(\"Missing background posts for {}\".format(start_year))\n",
    "        posts = getPushshiftDataForSub('changemyview',None,\n",
    "                                       start_year,1,1,start_year,12,31,\n",
    "                                       'submissions')\n",
    "        posts_df = pd.DataFrame(posts)\n",
    "        posts_df.to_csv(save_path)\n",
    "        \n",
    "\n",
    "# Append individual dataframes into one large one to share\n",
    "df = pd.read_csv(\n",
    "    'pmaw_output/post_comments/changemyview_background/1-1-2015_to_12-31-2015.csv',\n",
    "    index_col=0)\n",
    "COLUMNS = df.columns\n",
    "df = pd.DataFrame(columns=list(COLUMNS))\n",
    "\n",
    "for start_year in range(2013,2022,1):\n",
    "    save_path = os.path.join('pmaw_output','submissions','changemyview_background',\n",
    "                            '1-1-{}_to_12-31-{}.csv'.format(start_year,start_year))\n",
    "    df_ = pd.read_csv(save_path,index_col=0)\n",
    "    if len(df_) > 0:\n",
    "        missing_cols = set(df.columns).difference(set(df_.columns))\n",
    "        for missing_col in missing_cols:\n",
    "            df_[missing_col] = [None]*len(df_)\n",
    "        df = pd.concat([df,df_],ignore_index=True,axis=0)\n",
    "        \n",
    "# Deduplicate by ID \n",
    "print('Size of df, pre-deduplication:',len(df))\n",
    "df.drop_duplicates(subset='id',inplace=True)\n",
    "print('Size of df, post-deduplication:',len(df))\n",
    "\n",
    "# Annotate with whether author awarded delta or not (if ∆ is present in `author_flair_text`)\n",
    "df['delta_history'] = df['author_flair_text'].apply(\n",
    "                            lambda x: '∆' in x if type(x) == str else False\n",
    ")\n",
    "print(df['delta_history'].value_counts())\n",
    "\n",
    "# UPDATE TO CURRENT DATE\n",
    "from datetime import date\n",
    "today = date.today()\n",
    "str_today = today.strftime(\"%m-%d-%Y\")\n",
    "agg_save_path = 'pmaw_output/submissions/changemyview_background/1-1-2010_to_{}.csv'.format(str_today)\n",
    "print('\\nSaving deduplicated df of posts to: {}'.format(agg_save_path))\n",
    "\n",
    "df.to_csv(agg_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False    2784\n",
      "True      392\n",
      "Name: delta_history, dtype: int64\n",
      "\n",
      "Saving deduplicated df of posts to: pmaw_output/submissions/changemyview/1-1-2010_to_09-22-2021.csv\n"
     ]
    }
   ],
   "source": [
    "# Append individual dataframes into one large one to share\n",
    "\n",
    "df = pd.DataFrame(columns=list(COLUMNS)+['keyword'])\n",
    "\n",
    "for start_year in range(2013,2022,1):\n",
    "    for keyword in KEYWORDS_HI_PREC:\n",
    "        save_dir = os.path.join('pmaw_output','submissions','changemyview',\n",
    "                                '1-1-{}_to_12-31-{}'.format(start_year,start_year))\n",
    "        save_path = os.path.join(save_dir,'{}.csv'.format(keyword))\n",
    "        df_ = pd.read_csv(save_path,index_col=0)\n",
    "        if len(df_) > 0:\n",
    "            df_['keyword'] = [keyword]*len(df_)\n",
    "            missing_cols = set(df.columns).difference(set(df_.columns))\n",
    "            for missing_col in missing_cols:\n",
    "                df_[missing_col] = [None]*len(df_)\n",
    "            df = pd.concat([df,df_],ignore_index=True,axis=0)\n",
    "        \n",
    "# Deduplicate by ID \n",
    "print('Size of df, pre-deduplication:',len(df))\n",
    "df.drop_duplicates(subset='id',inplace=True)\n",
    "print('Size of df, post-deduplication:',len(df))\n",
    "\n",
    "# Annotate with whether author awarded delta or not (if ∆ is present in `author_flair_text`)\n",
    "df['delta_history'] = df['author_flair_text'].apply(\n",
    "                            lambda x: '∆' in x if type(x) == str else False\n",
    ")\n",
    "print(df['delta_history'].value_counts())\n",
    "\n",
    "# UPDATE TO CURRENT DATE\n",
    "from datetime import date\n",
    "today = date.today()\n",
    "str_today = today.strftime(\"%m-%d-%Y\")\n",
    "print('\\nSaving deduplicated df of posts to: pmaw_output/submissions/changemyview/1-1-2010_to_{}.csv'.format(str_today))\n",
    "\n",
    "df.to_csv('pmaw_output/submissions/changemyview/1-1-2010_to_{}.csv'.format(str_today))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # Inspect whether some keywords are low precision\n",
    "# for keyword in KEYWORDS_LONG:\n",
    "#     print(keyword)\n",
    "#     print(df.loc[df['keyword']==keyword]['url'].values[:10])\n",
    "#     print('=='*10+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # Check API rate limiting\n",
    "# import datetime\n",
    "# start = datetime.datetime.now()\n",
    "# for req_no in range(0,400):\n",
    "#     post = reddit.submission(id=df['id'].values[req_no])\n",
    "#     c_ids = [c for c in post.comments]\n",
    "#     print('Post comments:',c_ids)\n",
    "#     elapsed = (datetime.datetime.now()-start).total_seconds() / 60\n",
    "#     print('Elapsed minutes:', elapsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/u/nlp/anaconda/main/anaconda3/envs/yiwei-climate/lib/python3.6/site-packages/IPython/core/interactiveshell.py:3072: DtypeWarning: Columns (28,32,41,42,43,44,45,46,47,48,49,50,53,57,58,60,61,62,63,64,65,66,67,68,69,70,71,72,73,75,77,78,79,80,83,84,85,86,87,88,90,92,93,94,95,96,97,98,101,103,104) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('pmaw_output/submissions/changemyview_background/1-1-2010_to_09-27-2021.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('2snkev', 11485)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_id,list(df['id'].values).index(p_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2snkev'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['id'].values[11485]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "58"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(comments_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "code_folding": [
     12
    ],
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "(67376, 58)\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "100\n",
      "(68106, 58)\n",
      "110\n",
      "120\n",
      "130\n",
      "140\n",
      "150\n",
      "160\n",
      "170\n",
      "180\n",
      "190\n",
      "200\n",
      "(68845, 58)\n",
      "210\n",
      "220\n",
      "230\n",
      "240\n",
      "250\n",
      "260\n",
      "270\n",
      "280\n",
      "290\n",
      "300\n",
      "(69489, 58)\n",
      "310\n",
      "320\n",
      "330\n",
      "340\n",
      "350\n",
      "360\n",
      "370\n",
      "380\n",
      "390\n",
      "400\n",
      "(70291, 58)\n",
      "410\n",
      "420\n",
      "430\n",
      "440\n",
      "450\n",
      "460\n",
      "470\n",
      "480\n",
      "490\n",
      "500\n",
      "(71072, 58)\n",
      "510\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-8e4396d9dbda>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mcomment\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreddit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcomment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m             \u001b[0mcomments_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'author_flair_text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcomment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauthor_flair_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m             \u001b[0mcomments_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'author_flair_text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/u/nlp/anaconda/main/anaconda3/envs/yiwei-climate/lib/python3.6/site-packages/praw/models/reddit/base.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, attribute)\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0;34m\"\"\"Return the value of `attribute`.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mattribute\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"_\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetched\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattribute\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         raise AttributeError(\n",
      "\u001b[0;32m/u/nlp/anaconda/main/anaconda3/envs/yiwei-climate/lib/python3.6/site-packages/praw/models/reddit/comment.py\u001b[0m in \u001b[0;36m_fetch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetch_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"data\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/u/nlp/anaconda/main/anaconda3/envs/yiwei-climate/lib/python3.6/site-packages/praw/models/reddit/comment.py\u001b[0m in \u001b[0;36m_fetch_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    191\u001b[0m         \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfields\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetch_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m         \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAPI_PATH\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mfields\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 193\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reddit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"GET\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    194\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/u/nlp/anaconda/main/anaconda3/envs/yiwei-climate/lib/python3.6/site-packages/praw/reddit.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, path, params, data, files, json)\u001b[0m\n\u001b[1;32m    861\u001b[0m                 \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    862\u001b[0m                 \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 863\u001b[0;31m                 \u001b[0mjson\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    864\u001b[0m             )\n\u001b[1;32m    865\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mBadRequest\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/u/nlp/anaconda/main/anaconda3/envs/yiwei-climate/lib/python3.6/site-packages/prawcore/sessions.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, path, data, files, json, params, timeout)\u001b[0m\n\u001b[1;32m    335\u001b[0m             \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m             \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 337\u001b[0;31m             \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    338\u001b[0m         )\n\u001b[1;32m    339\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/u/nlp/anaconda/main/anaconda3/envs/yiwei-climate/lib/python3.6/site-packages/prawcore/sessions.py\u001b[0m in \u001b[0;36m_request_with_retries\u001b[0;34m(self, data, files, json, method, params, timeout, url, retry_strategy_state)\u001b[0m\n\u001b[1;32m    234\u001b[0m             \u001b[0mretry_strategy_state\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m             \u001b[0mtimeout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 236\u001b[0;31m             \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    237\u001b[0m         )\n\u001b[1;32m    238\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/u/nlp/anaconda/main/anaconda3/envs/yiwei-climate/lib/python3.6/site-packages/prawcore/sessions.py\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(self, data, files, json, method, params, retry_strategy_state, timeout, url)\u001b[0m\n\u001b[1;32m    193\u001b[0m                 \u001b[0mjson\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m                 \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 195\u001b[0;31m                 \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m             )\n\u001b[1;32m    197\u001b[0m             log.debug(\n",
      "\u001b[0;32m/u/nlp/anaconda/main/anaconda3/envs/yiwei-climate/lib/python3.6/site-packages/prawcore/rate_limit.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, request_function, set_header_callback, *args, **kwargs)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \"\"\"\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdelay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"headers\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset_header_callback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequest_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/u/nlp/anaconda/main/anaconda3/envs/yiwei-climate/lib/python3.6/site-packages/prawcore/rate_limit.py\u001b[0m in \u001b[0;36mdelay\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"Sleeping: {sleep_seconds:0.2f} seconds prior to call\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msleep_seconds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse_headers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Get all comments on posts within big posts df\n",
    "\n",
    "# define FIELDS; initialize comments dataframe\n",
    "with open('reddit_comment_fields.txt','r') as f:\n",
    "    FIELDS = set(f.read().splitlines())\n",
    "#comments_dict = defaultdict(list)\n",
    "\n",
    "for n,p_id in enumerate(df['id'].values[11485:]):\n",
    "    #print(p_id)\n",
    "    post = reddit.submission(id=p_id)\n",
    "    c_ids = [c.id for c in post.comments]\n",
    "    #print('Post comments:',c_ids)\n",
    "    for c_id in c_ids:\n",
    "        comment = reddit.comment(c_id)\n",
    "        try:\n",
    "            comments_dict['author_flair_text'].append(comment.author_flair_text)\n",
    "        except AttributeError:\n",
    "            comments_dict['author_flair_text'].append(None)\n",
    "        try:\n",
    "            comments_dict['treatment_tags'].append(comment.treatment_tags)\n",
    "        except AttributeError:\n",
    "            comments_dict['treatment_tags'].append(None)\n",
    "        try:\n",
    "            comments_dict['collapsed'].append(comment.collapsed)\n",
    "        except AttributeError:\n",
    "            comments_dict['collapsed'].append(None)\n",
    "        try:\n",
    "            comments_dict['subreddit_name_prefixed'].append(comment.subreddit_name_prefixed)\n",
    "        except AttributeError:\n",
    "            comments_dict['subreddit_name_prefixed'].append(None)\n",
    "        try:\n",
    "            comments_dict['controversiality'].append(comment.controversiality)\n",
    "        except AttributeError:\n",
    "            comments_dict['controversiality'].append(None)\n",
    "        try:\n",
    "            comments_dict['collapsed_because_crowd_control'].append(comment.collapsed_because_crowd_control)\n",
    "        except AttributeError:\n",
    "            comments_dict['collapsed_because_crowd_control'].append(None)\n",
    "        try:\n",
    "            comments_dict['mod_reports'].append(comment.mod_reports)\n",
    "        except AttributeError:\n",
    "            comments_dict['mod_reports'].append(None)\n",
    "        try:\n",
    "            comments_dict['subreddit_type'].append(comment.subreddit_type)\n",
    "        except AttributeError:\n",
    "            comments_dict['subreddit_type'].append(None)\n",
    "        try:\n",
    "            comments_dict['ups'].append(comment.ups)\n",
    "        except AttributeError:\n",
    "            comments_dict['ups'].append(None)\n",
    "        try:\n",
    "            comments_dict['_replies'].append(comment._replies)\n",
    "        except AttributeError:\n",
    "            comments_dict['_replies'].append(None)\n",
    "        try:\n",
    "            comments_dict['id'].append(comment.id)\n",
    "        except AttributeError:\n",
    "            comments_dict['id'].append(None)\n",
    "        try:\n",
    "            comments_dict['total_awards_received'].append(comment.total_awards_received)\n",
    "        except AttributeError:\n",
    "            comments_dict['total_awards_received'].append(None)\n",
    "        try:\n",
    "            comments_dict['approved_at_utc'].append(comment.approved_at_utc)\n",
    "        except AttributeError:\n",
    "            comments_dict['approved_at_utc'].append(None)\n",
    "        try:\n",
    "            comments_dict['author_is_blocked'].append(comment.author_is_blocked)\n",
    "        except AttributeError:\n",
    "            comments_dict['author_is_blocked'].append(None)\n",
    "        try:\n",
    "            comments_dict['comment_type'].append(comment.comment_type)\n",
    "        except AttributeError:\n",
    "            comments_dict['comment_type'].append(None)\n",
    "        try:\n",
    "            comments_dict['edited'].append(comment.edited)\n",
    "        except AttributeError:\n",
    "            comments_dict['edited'].append(None)\n",
    "        try:\n",
    "            comments_dict['mod_reason_by'].append(comment.mod_reason_by)\n",
    "        except AttributeError:\n",
    "            comments_dict['mod_reason_by'].append(None)\n",
    "        try:\n",
    "            comments_dict['banned_by'].append(comment.banned_by)\n",
    "        except AttributeError:\n",
    "            comments_dict['banned_by'].append(None)\n",
    "        try:\n",
    "            comments_dict['author_flair_type'].append(comment.author_flair_type)\n",
    "        except AttributeError:\n",
    "            comments_dict['author_flair_type'].append(None)\n",
    "        try:\n",
    "            comments_dict['removal_reason'].append(comment.removal_reason)\n",
    "        except AttributeError:\n",
    "            comments_dict['removal_reason'].append(None)\n",
    "        try:\n",
    "            comments_dict['link_id'].append(comment.link_id)\n",
    "        except AttributeError:\n",
    "            comments_dict['link_id'].append(None)\n",
    "        try:\n",
    "            comments_dict['likes'].append(comment.likes)\n",
    "        except AttributeError:\n",
    "            comments_dict['likes'].append(None)\n",
    "        try:\n",
    "            comments_dict['author_fullname'].append(comment.author_fullname)\n",
    "        except AttributeError:\n",
    "            comments_dict['author_fullname'].append(None)\n",
    "        try:\n",
    "            comments_dict['banned_at_utc'].append(comment.banned_at_utc)\n",
    "        except AttributeError:\n",
    "            comments_dict['banned_at_utc'].append(None)\n",
    "        try:\n",
    "            comments_dict['mod_reason_title'].append(comment.mod_reason_title)\n",
    "        except AttributeError:\n",
    "            comments_dict['mod_reason_title'].append(None)\n",
    "        try:\n",
    "            comments_dict['gilded'].append(comment.gilded)\n",
    "        except AttributeError:\n",
    "            comments_dict['gilded'].append(None)\n",
    "        try:\n",
    "            comments_dict['archived'].append(comment.archived)\n",
    "        except AttributeError:\n",
    "            comments_dict['archived'].append(None)\n",
    "        try:\n",
    "            comments_dict['collapsed_reason_code'].append(comment.collapsed_reason_code)\n",
    "        except AttributeError:\n",
    "            comments_dict['collapsed_reason_code'].append(None)\n",
    "        try:\n",
    "            comments_dict['no_follow'].append(comment.no_follow)\n",
    "        except AttributeError:\n",
    "            comments_dict['no_follow'].append(None)\n",
    "        try:\n",
    "            comments_dict['can_mod_post'].append(comment.can_mod_post)\n",
    "        except AttributeError:\n",
    "            comments_dict['can_mod_post'].append(None)\n",
    "        try:\n",
    "            comments_dict['created_utc'].append(comment.created_utc)\n",
    "        except AttributeError:\n",
    "            comments_dict['created_utc'].append(None)\n",
    "        try:\n",
    "            comments_dict['send_replies'].append(comment.send_replies)\n",
    "        except AttributeError:\n",
    "            comments_dict['send_replies'].append(None)\n",
    "        try:\n",
    "            comments_dict['parent_id'].append(comment.parent_id)\n",
    "        except AttributeError:\n",
    "            comments_dict['parent_id'].append(None)\n",
    "        try:\n",
    "            comments_dict['score'].append(comment.score)\n",
    "        except AttributeError:\n",
    "            comments_dict['score'].append(None)\n",
    "        try:\n",
    "            comments_dict['approved_by'].append(comment.approved_by)\n",
    "        except AttributeError:\n",
    "            comments_dict['approved_by'].append(None)\n",
    "        try:\n",
    "            comments_dict['author_premium'].append(comment.author_premium)\n",
    "        except AttributeError:\n",
    "            comments_dict['author_premium'].append(None)\n",
    "        try:\n",
    "            comments_dict['mod_note'].append(comment.mod_note)\n",
    "        except AttributeError:\n",
    "            comments_dict['mod_note'].append(None)\n",
    "        try:\n",
    "            comments_dict['all_awardings'].append(comment.all_awardings)\n",
    "        except AttributeError:\n",
    "            comments_dict['all_awardings'].append(None)\n",
    "        try:\n",
    "            comments_dict['subreddit_id'].append(comment.subreddit_id)\n",
    "        except AttributeError:\n",
    "            comments_dict['subreddit_id'].append(None)\n",
    "        try:\n",
    "            comments_dict['body'].append(comment.body)\n",
    "        except AttributeError:\n",
    "            comments_dict['body'].append(None)\n",
    "        try:\n",
    "            comments_dict['awarders'].append(comment.awarders)\n",
    "        except AttributeError:\n",
    "            comments_dict['awarders'].append(None)\n",
    "        try:\n",
    "            comments_dict['user_reports'].append(comment.user_reports)\n",
    "        except AttributeError:\n",
    "            comments_dict['user_reports'].append(None)\n",
    "        try:\n",
    "            comments_dict['name'].append(comment.name)\n",
    "        except AttributeError:\n",
    "            comments_dict['name'].append(None)\n",
    "        try:\n",
    "            comments_dict['downs'].append(comment.downs)\n",
    "        except AttributeError:\n",
    "            comments_dict['downs'].append(None)\n",
    "        try:\n",
    "            comments_dict['author_flair_richtext'].append(comment.author_flair_richtext)\n",
    "        except AttributeError:\n",
    "            comments_dict['author_flair_richtext'].append(None)\n",
    "        try:\n",
    "            comments_dict['is_submitter'].append(comment.is_submitter)\n",
    "        except AttributeError:\n",
    "            comments_dict['is_submitter'].append(None)\n",
    "        try:\n",
    "            comments_dict['collapsed_reason'].append(comment.collapsed_reason)\n",
    "        except AttributeError:\n",
    "            comments_dict['collapsed_reason'].append(None)\n",
    "        try:\n",
    "            comments_dict['distinguished'].append(comment.distinguished)\n",
    "        except AttributeError:\n",
    "            comments_dict['distinguished'].append(None)\n",
    "        try:\n",
    "            comments_dict['associated_award'].append(comment.associated_award)\n",
    "        except AttributeError:\n",
    "            comments_dict['associated_award'].append(None)\n",
    "        try:\n",
    "            comments_dict['stickied'].append(comment.stickied)\n",
    "        except AttributeError:\n",
    "            comments_dict['stickied'].append(None)\n",
    "        try:\n",
    "            comments_dict['can_gild'].append(comment.can_gild)\n",
    "        except AttributeError:\n",
    "            comments_dict['can_gild'].append(None)\n",
    "        try:\n",
    "            comments_dict['top_awarded_type'].append(comment.top_awarded_type)\n",
    "        except AttributeError:\n",
    "            comments_dict['top_awarded_type'].append(None)\n",
    "        try:\n",
    "            comments_dict['score_hidden'].append(comment.score_hidden)\n",
    "        except AttributeError:\n",
    "            comments_dict['score_hidden'].append(None)\n",
    "        try:\n",
    "            comments_dict['permalink'].append(comment.permalink)\n",
    "        except AttributeError:\n",
    "            comments_dict['permalink'].append(None)\n",
    "        try:\n",
    "            comments_dict['num_reports'].append(comment.num_reports)\n",
    "        except AttributeError:\n",
    "            comments_dict['num_reports'].append(None)\n",
    "        try:\n",
    "            comments_dict['locked'].append(comment.locked)\n",
    "        except AttributeError:\n",
    "            comments_dict['locked'].append(None)\n",
    "        try:\n",
    "            comments_dict['report_reasons'].append(comment.report_reasons)\n",
    "        except AttributeError:\n",
    "            comments_dict['report_reasons'].append(None)\n",
    "        try:\n",
    "            comments_dict['created'].append(comment.created)\n",
    "        except AttributeError:\n",
    "            comments_dict['created'].append(None)\n",
    "\n",
    "    if n % 10 == 0:\n",
    "        print(n)\n",
    "        \n",
    "    if n % 100 == 0:\n",
    "        comments_df = pd.DataFrame(comments_dict)\n",
    "        print(comments_df.shape)\n",
    "        comments_df.to_csv('comments_output/changemyview_background/from_posts_1-1-2010_to_9-28-2021.csv',\n",
    "                  index=False)\n",
    "\n",
    "comments_df = pd.DataFrame(comments_dict)\n",
    "print(comments_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    52180\n",
       "True     18892\n",
       "Name: changed_view, dtype: int64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# And annotate with whether comment received delta or not\n",
    "# (look at `author_flair_text`)\n",
    "comments_df['changed_view'] = comments_df['author_flair_text'].apply(\n",
    "    lambda x: '∆' in x if type(x) == str else False\n",
    ")\n",
    "comments_df['changed_view'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2364, 58)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "old_comments_df = pd.read_csv('comments_output/changemyview_background/from_posts_1-1-2010_to_9-24-2021.csv')\n",
    "old_comments_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    1800\n",
       "True      564\n",
       "Name: changed_view, dtype: int64"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "old_comments_df['changed_view'] = old_comments_df['author_flair_text'].apply(\n",
    "    lambda x: '∆' in x if type(x) == str else False\n",
    ")\n",
    "old_comments_df['changed_view'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of df, pre-deduplication: 73436\n",
      "Size of df, post-deduplication: 73420\n"
     ]
    }
   ],
   "source": [
    "# Append with old comments_df\n",
    "#old_comments_df = pd.read_csv('comments_output/changemyview_background/from_posts_1-1-2010_to_9-24-2021.csv')\n",
    "comments_df = pd.concat([comments_df,old_comments_df],axis=0,ignore_index=True)\n",
    "\n",
    "# Deduplicate by ID \n",
    "print('Size of df, pre-deduplication:',len(comments_df))\n",
    "comments_df.drop_duplicates(subset='id',inplace=True)\n",
    "print('Size of df, post-deduplication:',len(comments_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author_flair_text</th>\n",
       "      <th>treatment_tags</th>\n",
       "      <th>collapsed</th>\n",
       "      <th>subreddit_name_prefixed</th>\n",
       "      <th>controversiality</th>\n",
       "      <th>collapsed_because_crowd_control</th>\n",
       "      <th>mod_reports</th>\n",
       "      <th>subreddit_type</th>\n",
       "      <th>ups</th>\n",
       "      <th>_replies</th>\n",
       "      <th>...</th>\n",
       "      <th>stickied</th>\n",
       "      <th>can_gild</th>\n",
       "      <th>top_awarded_type</th>\n",
       "      <th>score_hidden</th>\n",
       "      <th>permalink</th>\n",
       "      <th>num_reports</th>\n",
       "      <th>locked</th>\n",
       "      <th>report_reasons</th>\n",
       "      <th>created</th>\n",
       "      <th>changed_view</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "      <td>r/changemyview</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "      <td>public</td>\n",
       "      <td>15</td>\n",
       "      <td>[]</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>/r/changemyview/comments/2avwph/cmv_all_indian...</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>1.405542e+09</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "      <td>r/changemyview</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "      <td>public</td>\n",
       "      <td>23</td>\n",
       "      <td>[]</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>/r/changemyview/comments/2avwph/cmv_all_indian...</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>1.405541e+09</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "      <td>r/changemyview</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "      <td>public</td>\n",
       "      <td>9</td>\n",
       "      <td>[]</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>/r/changemyview/comments/2avwph/cmv_all_indian...</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>1.405540e+09</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1∆</td>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "      <td>r/changemyview</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "      <td>public</td>\n",
       "      <td>3</td>\n",
       "      <td>[]</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>/r/changemyview/comments/2avwph/cmv_all_indian...</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>1.405572e+09</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "      <td>r/changemyview</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "      <td>public</td>\n",
       "      <td>2</td>\n",
       "      <td>[]</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>/r/changemyview/comments/2avwph/cmv_all_indian...</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>1.405542e+09</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 59 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  author_flair_text treatment_tags  collapsed subreddit_name_prefixed  \\\n",
       "0              None             []      False          r/changemyview   \n",
       "1              None             []      False          r/changemyview   \n",
       "2              None             []      False          r/changemyview   \n",
       "3                1∆             []      False          r/changemyview   \n",
       "4              None             []      False          r/changemyview   \n",
       "\n",
       "   controversiality collapsed_because_crowd_control mod_reports  \\\n",
       "0                 0                            None          []   \n",
       "1                 0                            None          []   \n",
       "2                 0                            None          []   \n",
       "3                 0                            None          []   \n",
       "4                 0                            None          []   \n",
       "\n",
       "  subreddit_type  ups _replies  ... stickied  can_gild top_awarded_type  \\\n",
       "0         public   15       []  ...    False     False             None   \n",
       "1         public   23       []  ...    False     False             None   \n",
       "2         public    9       []  ...    False     False             None   \n",
       "3         public    3       []  ...    False      True             None   \n",
       "4         public    2       []  ...    False     False             None   \n",
       "\n",
       "   score_hidden                                          permalink  \\\n",
       "0         False  /r/changemyview/comments/2avwph/cmv_all_indian...   \n",
       "1         False  /r/changemyview/comments/2avwph/cmv_all_indian...   \n",
       "2         False  /r/changemyview/comments/2avwph/cmv_all_indian...   \n",
       "3         False  /r/changemyview/comments/2avwph/cmv_all_indian...   \n",
       "4         False  /r/changemyview/comments/2avwph/cmv_all_indian...   \n",
       "\n",
       "  num_reports locked report_reasons       created changed_view  \n",
       "0        None  False           None  1.405542e+09        False  \n",
       "1        None  False           None  1.405541e+09        False  \n",
       "2        None  False           None  1.405540e+09        False  \n",
       "3        None  False           None  1.405572e+09         True  \n",
       "4        None  False           None  1.405542e+09        False  \n",
       "\n",
       "[5 rows x 59 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<class 'str'>    73420\n",
       "Name: body, dtype: int64"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments_df['body'].apply(lambda x: type(x)).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#comments_df['body'].values[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "comments_df.to_csv('comments_output/changemyview_background/from_posts_1-1-2010_to_9-28-2021_combined.csv',\n",
    "                  index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# OLD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "However, the API only returns 100 results at a time, so we need some additional wrapper scripts to iteratively retrieve all results. \n",
    "\n",
    "`collectSubData` gets specific fields we want from a post and `collectCommData` does the same but for a comment. Check out [this doc page](#https://pushshift.io/api-parameters/) for details on how you can modify these 2 scripts to change the fields you might be interested in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "code_folding": [
     0,
     55,
     56
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def collectSubData(subm,subs_dict):\n",
    "    try:\n",
    "        title = subm['title']\n",
    "    except KeyError:\n",
    "        title = None\n",
    "    try:\n",
    "        url = subm['url']\n",
    "    except KeyError:\n",
    "        url = None\n",
    "    try:\n",
    "        flair = subm['link_flair_text']\n",
    "    except KeyError:\n",
    "        flair = \"NaN\"   \n",
    "    try:\n",
    "        author = subm['author']\n",
    "    except KeyError:\n",
    "        author = None\n",
    "    sub_id = subm['id']\n",
    "    try:\n",
    "        score = subm['score']\n",
    "    except KeyError:\n",
    "        score = None\n",
    "    try:\n",
    "        created = datetime.datetime.fromtimestamp(subm['created_utc']) #1520561700.0\n",
    "    except KeyError:\n",
    "        created = None\n",
    "    try:\n",
    "        numComms = subm['num_comments']\n",
    "    except KeyError:\n",
    "        numComms = None\n",
    "    try:\n",
    "        permalink = subm['permalink']\n",
    "    except KeyError:\n",
    "        permalink = None\n",
    "    try:\n",
    "        is_vid = subm['is_video']\n",
    "    except KeyError:\n",
    "        is_vid = None\n",
    "    try:\n",
    "        upvote_ratio = subm['upvote_ratio']\n",
    "    except KeyError:\n",
    "        upvote_ratio = None\n",
    "    try:\n",
    "        text = subm['selftext'].strip().replace('\\t','').replace('\\n','')\n",
    "    except KeyError:\n",
    "        text = \"\"\n",
    "    try:\n",
    "        subreddit = subm['subreddit']\n",
    "    except KeyError:\n",
    "        subreddit = None\n",
    "    subData = {'id':sub_id,'title':title,'url':url,'author':author,'score':score,'date':created,\n",
    "                    'num_comments':numComms,'permalink':permalink,'flair':flair,'is_video':is_vid,\n",
    "                    'upvote_ratio':upvote_ratio,'text':text,'subreddit':subreddit}\n",
    "    subs_dict[sub_id] = subData\n",
    "    \n",
    "def collectCommData(subm,subs_dict): \n",
    "    try:\n",
    "        author = subm['author']\n",
    "    except KeyError:\n",
    "        author = None\n",
    "    sub_id = subm['id']\n",
    "    try:\n",
    "        link_id = subm['link_id']\n",
    "    except KeyError:\n",
    "        link_id = None\n",
    "    try:\n",
    "        score = subm['score']\n",
    "    except KeyError:\n",
    "        score = None\n",
    "    try:\n",
    "        created = datetime.datetime.fromtimestamp(subm['created_utc']) #1520561700.0\n",
    "    except KeyError:\n",
    "        created = None\n",
    "    try:\n",
    "        permalink = subm['permalink']\n",
    "    except KeyError:\n",
    "        permalink = None\n",
    "    try:\n",
    "        text = subm['body'].strip().replace('\\t','').replace('\\n','')\n",
    "    except KeyError:\n",
    "        text = \"\"\n",
    "    try:\n",
    "        subreddit = subm['subreddit']\n",
    "    except KeyError:\n",
    "        subreddit = None\n",
    "    subData = {'id':sub_id,'link_id':link_id,'author':author,'score':score,'date':created,\n",
    "                    'permalink':permalink,'text':text,'subreddit':subreddit}\n",
    "    subs_dict[sub_id] = subData"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Then, we can call `pushshift_wrapper` to run until all data has been gathered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def pushshift_wrapper(after_str, before_str, datatype, \n",
    "                      subreddit=None, query=None, keywords=None):\n",
    "    failed_requests = []\n",
    "    \n",
    "    if query is None:\n",
    "        query = '|'.join(keywords)\n",
    "    \n",
    "    subCount = 0\n",
    "    subStats = {}\n",
    "    print(\"Getting all submissions with query '{}' within subreddit {} from {} to {}\".format(\n",
    "        query,subreddit,after_str,before_str))\n",
    "    try:\n",
    "        data = getPushshiftDataForSub(subreddit, query, after_str, before_str, datatype)\n",
    "        # Will run until all posts have been gathered \n",
    "        # from the 'after' date up until before date\n",
    "        while len(data) > 0:\n",
    "            for submission in data:\n",
    "                if datatype == \"submission\":\n",
    "                    collectSubData(submission,subStats)\n",
    "                else:\n",
    "                    collectCommData(submission,subStats)\n",
    "                subCount+=1\n",
    "            # Calls getPushshiftData() with the created date of the last submission\n",
    "            #print(len(data))\n",
    "#             print(str(datetime.datetime.fromtimestamp(data[-1]['created_utc'])))\n",
    "#             after_timestamp = data[-1]['created_utc']\n",
    "            \n",
    "            new_after_str = str(datetime.datetime.fromtimestamp(data[-1]['created_utc'])).\\\n",
    "                            split()[0]\n",
    "            split_after_str = new_after_str.split('-')\n",
    "            new_after_str = '{}-{}-{}'.format(split_after_str[1],split_after_str[2],\n",
    "                                             split_after_str[0])\n",
    "            print(new_after_str)\n",
    "            try:\n",
    "                data = getPushshiftDataForSub(subreddit, query, \n",
    "                                              new_after_str, before_str, datatype)\n",
    "            except JSONDecodeError:\n",
    "                failed_requests.append((subreddit,query,after_str,before_str,datatype))\n",
    "\n",
    "        print('Num submissions:',subCount,len(subStats))\n",
    "\n",
    "        interim_df = pd.DataFrame(list(subStats.values()))\n",
    "        #print(interim_df)\n",
    "\n",
    "        datatype_prefix = 'posts' if datatype == 'submission' else 'post_comments'\n",
    "        out_dir = os.path.join('pushshift_output',datatype_prefix,subreddit,'{}_to_{}'.format(after_str,before_str))\n",
    "        failed_reqs_out_dir = os.path.join('pushshift_output','failed_requests',subreddit,\n",
    "                                                      '{}_{}'.format(after_str,before_str))\n",
    "        if not os.path.exists(out_dir):\n",
    "            os.makedirs(out_dir)\n",
    "        if not os.path.exists(failed_reqs_out_dir):\n",
    "            os.makedirs(failed_reqs_out_dir)\n",
    "            \n",
    "        if keywords is None:\n",
    "            interim_df.to_pickle(os.path.join(out_dir,'{}.pkl'.format(query)))\n",
    "            print('Saved query submissions to {}!'.format(os.path.join(out_dir,'{}.pkl'.format(query))))\n",
    "            pickle.dump(failed_requests,open(os.path.join(failed_reqs_out_dir,'{}.pkl'.format(query)),'wb'))\n",
    "        else:\n",
    "            interim_df.to_pickle(os.path.join(out_dir,'{}.pkl'.format('keywords_long')))\n",
    "            print('Saved query submissions to {}!'.format(os.path.join(out_dir,'{}.pkl'.format('keywords_long'))))\n",
    "            pickle.dump(failed_requests,open(os.path.join(failed_reqs_out_dir,'{}.pkl'.format('keywords_long')),'wb'))\n",
    "            \n",
    "    except JSONDecodeError:\n",
    "        failed_requests.append((query,after_str,before_str,datatype))\n",
    "        print(\"First request failed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting all submissions with query 'climate change' within subreddit changemyview from 01-01-2014 to 1-5-2014\n",
      "01-03-2014\n",
      "01-03-2014\n",
      "01-03-2014\n",
      "01-03-2014\n",
      "01-03-2014\n",
      "01-03-2014\n",
      "01-03-2014\n",
      "01-03-2014\n",
      "01-03-2014\n",
      "01-03-2014\n",
      "01-03-2014\n",
      "01-03-2014\n",
      "01-03-2014\n",
      "01-03-2014\n",
      "01-03-2014\n",
      "01-03-2014\n",
      "01-03-2014\n",
      "01-03-2014\n",
      "01-03-2014\n",
      "01-03-2014\n",
      "01-03-2014\n",
      "01-03-2014\n",
      "01-03-2014\n",
      "01-03-2014\n",
      "01-03-2014\n",
      "01-03-2014\n",
      "01-03-2014\n",
      "01-03-2014\n",
      "01-03-2014\n",
      "01-03-2014\n",
      "01-03-2014\n",
      "01-03-2014\n",
      "01-03-2014\n",
      "01-03-2014\n",
      "01-03-2014\n",
      "01-03-2014\n",
      "01-03-2014\n",
      "01-03-2014\n",
      "01-03-2014\n",
      "01-03-2014\n",
      "01-03-2014\n",
      "01-03-2014\n",
      "01-03-2014\n",
      "01-03-2014\n",
      "01-03-2014\n",
      "01-03-2014\n",
      "01-03-2014\n",
      "01-03-2014\n",
      "01-03-2014\n",
      "01-03-2014\n",
      "01-03-2014\n",
      "01-03-2014\n",
      "01-03-2014\n",
      "01-03-2014\n",
      "01-03-2014\n",
      "01-03-2014\n",
      "01-03-2014\n",
      "01-03-2014\n",
      "01-03-2014\n",
      "01-03-2014\n",
      "01-03-2014\n",
      "01-03-2014\n",
      "01-03-2014\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-71-1400cfbe7f1d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m pushshift_wrapper('01-01-2014','1-5-2014','submission',subreddit='changemyview',\n\u001b[0;32m----> 2\u001b[0;31m                  query='climate change')\n\u001b[0m",
      "\u001b[0;32m<ipython-input-70-9c0fe95f5103>\u001b[0m in \u001b[0;36mpushshift_wrapper\u001b[0;34m(after_str, before_str, datatype, subreddit, query, keywords)\u001b[0m\n\u001b[1;32m     34\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m                 data = getPushshiftDataForSub(subreddit, query, \n\u001b[0;32m---> 36\u001b[0;31m                                               new_after_str, before_str, datatype)\n\u001b[0m\u001b[1;32m     37\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mJSONDecodeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m                 \u001b[0mfailed_requests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubreddit\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mafter_str\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbefore_str\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdatatype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-17-64aed0d93794>\u001b[0m in \u001b[0;36mgetPushshiftDataForSub\u001b[0;34m(subreddit, query, after_str, before_str, datatype)\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;34m'&before='\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbefore_date\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;31m#print(url)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m     \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/u/nlp/anaconda/main/anaconda3/envs/yiwei-climate/lib/python3.6/site-packages/requests/api.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(url, params, **kwargs)\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetdefault\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'allow_redirects'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'get'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/u/nlp/anaconda/main/anaconda3/envs/yiwei-climate/lib/python3.6/site-packages/requests/api.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;31m# cases, and look like a memory leak in others.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0msessions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/u/nlp/anaconda/main/anaconda3/envs/yiwei-climate/lib/python3.6/site-packages/requests/sessions.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    531\u001b[0m         }\n\u001b[1;32m    532\u001b[0m         \u001b[0msend_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 533\u001b[0;31m         \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0msend_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    534\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    535\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/u/nlp/anaconda/main/anaconda3/envs/yiwei-climate/lib/python3.6/site-packages/requests/sessions.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    644\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    645\u001b[0m         \u001b[0;31m# Send the request\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 646\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madapter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    647\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    648\u001b[0m         \u001b[0;31m# Total elapsed time of the request (approximately)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/u/nlp/anaconda/main/anaconda3/envs/yiwei-climate/lib/python3.6/site-packages/requests/adapters.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    447\u001b[0m                     \u001b[0mdecode_content\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m                     \u001b[0mretries\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_retries\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 449\u001b[0;31m                     \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    450\u001b[0m                 )\n\u001b[1;32m    451\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/u/nlp/anaconda/main/anaconda3/envs/yiwei-climate/lib/python3.6/site-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    675\u001b[0m                 \u001b[0mbody\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    676\u001b[0m                 \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 677\u001b[0;31m                 \u001b[0mchunked\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mchunked\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    678\u001b[0m             )\n\u001b[1;32m    679\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/u/nlp/anaconda/main/anaconda3/envs/yiwei-climate/lib/python3.6/site-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    379\u001b[0m         \u001b[0;31m# Trigger any extra validation we need to do.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 381\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_conn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    382\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mSocketTimeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBaseSSLError\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m             \u001b[0;31m# Py2 raises this as a BaseSSLError, Py3 raises it as socket timeout.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/u/nlp/anaconda/main/anaconda3/envs/yiwei-climate/lib/python3.6/site-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_validate_conn\u001b[0;34m(self, conn)\u001b[0m\n\u001b[1;32m    976\u001b[0m         \u001b[0;31m# Force connect early to allow us to validate the connection.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    977\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"sock\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# AppEngine might not have  `.sock`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 978\u001b[0;31m             \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    979\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    980\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_verified\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/u/nlp/anaconda/main/anaconda3/envs/yiwei-climate/lib/python3.6/site-packages/urllib3/connection.py\u001b[0m in \u001b[0;36mconnect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    369\u001b[0m             \u001b[0mca_cert_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mca_cert_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    370\u001b[0m             \u001b[0mserver_hostname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mserver_hostname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 371\u001b[0;31m             \u001b[0mssl_context\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    372\u001b[0m         )\n\u001b[1;32m    373\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/u/nlp/anaconda/main/anaconda3/envs/yiwei-climate/lib/python3.6/site-packages/urllib3/util/ssl_.py\u001b[0m in \u001b[0;36mssl_wrap_socket\u001b[0;34m(sock, keyfile, certfile, cert_reqs, ca_certs, server_hostname, ssl_version, ciphers, ssl_context, ca_cert_dir, key_password, ca_cert_data)\u001b[0m\n\u001b[1;32m    350\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mca_certs\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mca_cert_dir\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mca_cert_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    351\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 352\u001b[0;31m             \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_verify_locations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mca_certs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mca_cert_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mca_cert_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    353\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mIOError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Platform-specific: Python 2.7\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mSSLError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "pushshift_wrapper('01-01-2014','1-5-2014','submission',subreddit='changemyview',\n",
    "                 query='climate change')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing global warming, 2010\n",
      "Getting all submissions with query 'global warming' within subreddit changemyview from 1-1-2010 to 12-31-2010\n",
      "Num submissions: 0 0\n",
      "Saved query submissions to pushshift_output/posts/changemyview/1-1-2010_to_12-31-2010/global warming.pkl!\n",
      "Missing climate change, 2010\n",
      "Getting all submissions with query 'climate change' within subreddit changemyview from 1-1-2010 to 12-31-2010\n",
      "Num submissions: 0 0\n",
      "Saved query submissions to pushshift_output/posts/changemyview/1-1-2010_to_12-31-2010/climate change.pkl!\n",
      "Missing carbon, 2010\n",
      "Getting all submissions with query 'carbon' within subreddit changemyview from 1-1-2010 to 12-31-2010\n",
      "Num submissions: 0 0\n",
      "Saved query submissions to pushshift_output/posts/changemyview/1-1-2010_to_12-31-2010/carbon.pkl!\n",
      "Missing fossil fuel, 2010\n",
      "Getting all submissions with query 'fossil fuel' within subreddit changemyview from 1-1-2010 to 12-31-2010\n",
      "Num submissions: 0 0\n",
      "Saved query submissions to pushshift_output/posts/changemyview/1-1-2010_to_12-31-2010/fossil fuel.pkl!\n",
      "Missing methane, 2010\n",
      "Getting all submissions with query 'methane' within subreddit changemyview from 1-1-2010 to 12-31-2010\n",
      "Num submissions: 0 0\n",
      "Saved query submissions to pushshift_output/posts/changemyview/1-1-2010_to_12-31-2010/methane.pkl!\n",
      "Missing environment, 2010\n",
      "Getting all submissions with query 'environment' within subreddit changemyview from 1-1-2010 to 12-31-2010\n",
      "Num submissions: 0 0\n",
      "Saved query submissions to pushshift_output/posts/changemyview/1-1-2010_to_12-31-2010/environment.pkl!\n",
      "Missing co2, 2010\n",
      "Getting all submissions with query 'co2' within subreddit changemyview from 1-1-2010 to 12-31-2010\n",
      "Num submissions: 0 0\n",
      "Saved query submissions to pushshift_output/posts/changemyview/1-1-2010_to_12-31-2010/co2.pkl!\n",
      "Missing climate crisis, 2010\n",
      "Getting all submissions with query 'climate crisis' within subreddit changemyview from 1-1-2010 to 12-31-2010\n",
      "Num submissions: 0 0\n",
      "Saved query submissions to pushshift_output/posts/changemyview/1-1-2010_to_12-31-2010/climate crisis.pkl!\n",
      "Missing climate emergency, 2010\n",
      "Getting all submissions with query 'climate emergency' within subreddit changemyview from 1-1-2010 to 12-31-2010\n",
      "Num submissions: 0 0\n",
      "Saved query submissions to pushshift_output/posts/changemyview/1-1-2010_to_12-31-2010/climate emergency.pkl!\n",
      "Missing extreme weather, 2010\n",
      "Getting all submissions with query 'extreme weather' within subreddit changemyview from 1-1-2010 to 12-31-2010\n",
      "First request failed\n",
      "Missing 2 degree, 2010\n",
      "Getting all submissions with query '2 degree' within subreddit changemyview from 1-1-2010 to 12-31-2010\n",
      "First request failed\n",
      "Missing sustainable, 2010\n",
      "Getting all submissions with query 'sustainable' within subreddit changemyview from 1-1-2010 to 12-31-2010\n",
      "First request failed\n",
      "Missing clean energy, 2010\n",
      "Getting all submissions with query 'clean energy' within subreddit changemyview from 1-1-2010 to 12-31-2010\n",
      "First request failed\n",
      "Missing renewable, 2010\n",
      "Getting all submissions with query 'renewable' within subreddit changemyview from 1-1-2010 to 12-31-2010\n",
      "Num submissions: 0 0\n",
      "Saved query submissions to pushshift_output/posts/changemyview/1-1-2010_to_12-31-2010/renewable.pkl!\n",
      "Missing cap and trade, 2010\n",
      "Getting all submissions with query 'cap and trade' within subreddit changemyview from 1-1-2010 to 12-31-2010\n",
      "Num submissions: 0 0\n",
      "Saved query submissions to pushshift_output/posts/changemyview/1-1-2010_to_12-31-2010/cap and trade.pkl!\n",
      "Missing sea level rise, 2010\n",
      "Getting all submissions with query 'sea level rise' within subreddit changemyview from 1-1-2010 to 12-31-2010\n",
      "Num submissions: 0 0\n",
      "Saved query submissions to pushshift_output/posts/changemyview/1-1-2010_to_12-31-2010/sea level rise.pkl!\n",
      "Missing environmental justice, 2010\n",
      "Getting all submissions with query 'environmental justice' within subreddit changemyview from 1-1-2010 to 12-31-2010\n",
      "Num submissions: 0 0\n",
      "Saved query submissions to pushshift_output/posts/changemyview/1-1-2010_to_12-31-2010/environmental justice.pkl!\n",
      "Missing climate justice, 2010\n",
      "Getting all submissions with query 'climate justice' within subreddit changemyview from 1-1-2010 to 12-31-2010\n",
      "First request failed\n",
      "Missing COP, 2010\n",
      "Getting all submissions with query 'COP' within subreddit changemyview from 1-1-2010 to 12-31-2010\n",
      "First request failed\n",
      "Missing IPCC, 2010\n",
      "Getting all submissions with query 'IPCC' within subreddit changemyview from 1-1-2010 to 12-31-2010\n",
      "Num submissions: 0 0\n",
      "Saved query submissions to pushshift_output/posts/changemyview/1-1-2010_to_12-31-2010/IPCC.pkl!\n",
      "Missing deforestation, 2010\n",
      "Getting all submissions with query 'deforestation' within subreddit changemyview from 1-1-2010 to 12-31-2010\n",
      "Num submissions: 0 0\n",
      "Saved query submissions to pushshift_output/posts/changemyview/1-1-2010_to_12-31-2010/deforestation.pkl!\n",
      "Missing permafrost, 2010\n",
      "Getting all submissions with query 'permafrost' within subreddit changemyview from 1-1-2010 to 12-31-2010\n",
      "Num submissions: 0 0\n",
      "Saved query submissions to pushshift_output/posts/changemyview/1-1-2010_to_12-31-2010/permafrost.pkl!\n",
      "Missing glacier, 2010\n",
      "Getting all submissions with query 'glacier' within subreddit changemyview from 1-1-2010 to 12-31-2010\n",
      "Num submissions: 0 0\n",
      "Saved query submissions to pushshift_output/posts/changemyview/1-1-2010_to_12-31-2010/glacier.pkl!\n",
      "Missing drought, 2010\n",
      "Getting all submissions with query 'drought' within subreddit changemyview from 1-1-2010 to 12-31-2010\n",
      "Num submissions: 0 0\n",
      "Saved query submissions to pushshift_output/posts/changemyview/1-1-2010_to_12-31-2010/drought.pkl!\n",
      "Missing ecosystem, 2010\n",
      "Getting all submissions with query 'ecosystem' within subreddit changemyview from 1-1-2010 to 12-31-2010\n",
      "Num submissions: 0 0\n",
      "Saved query submissions to pushshift_output/posts/changemyview/1-1-2010_to_12-31-2010/ecosystem.pkl!\n",
      "Missing greenhouse gas, 2010\n",
      "Getting all submissions with query 'greenhouse gas' within subreddit changemyview from 1-1-2010 to 12-31-2010\n",
      "Num submissions: 0 0\n",
      "Saved query submissions to pushshift_output/posts/changemyview/1-1-2010_to_12-31-2010/greenhouse gas.pkl!\n",
      "Missing greenhouse effect, 2010\n",
      "Getting all submissions with query 'greenhouse effect' within subreddit changemyview from 1-1-2010 to 12-31-2010\n",
      "Num submissions: 0 0\n",
      "Saved query submissions to pushshift_output/posts/changemyview/1-1-2010_to_12-31-2010/greenhouse effect.pkl!\n",
      "Missing green new deal, 2010\n",
      "Getting all submissions with query 'green new deal' within subreddit changemyview from 1-1-2010 to 12-31-2010\n",
      "First request failed\n",
      "Missing EPA, 2010\n",
      "Getting all submissions with query 'EPA' within subreddit changemyview from 1-1-2010 to 12-31-2010\n",
      "Num submissions: 0 0\n",
      "Saved query submissions to pushshift_output/posts/changemyview/1-1-2010_to_12-31-2010/EPA.pkl!\n",
      "Missing global warming, 2011\n",
      "Getting all submissions with query 'global warming' within subreddit changemyview from 1-1-2011 to 12-31-2011\n",
      "First request failed\n",
      "Missing climate change, 2011\n",
      "Getting all submissions with query 'climate change' within subreddit changemyview from 1-1-2011 to 12-31-2011\n",
      "First request failed\n",
      "Missing carbon, 2011\n",
      "Getting all submissions with query 'carbon' within subreddit changemyview from 1-1-2011 to 12-31-2011\n",
      "First request failed\n",
      "Missing fossil fuel, 2011\n",
      "Getting all submissions with query 'fossil fuel' within subreddit changemyview from 1-1-2011 to 12-31-2011\n",
      "Num submissions: 0 0\n",
      "Saved query submissions to pushshift_output/posts/changemyview/1-1-2011_to_12-31-2011/fossil fuel.pkl!\n",
      "Missing methane, 2011\n",
      "Getting all submissions with query 'methane' within subreddit changemyview from 1-1-2011 to 12-31-2011\n",
      "First request failed\n",
      "Missing environment, 2011\n",
      "Getting all submissions with query 'environment' within subreddit changemyview from 1-1-2011 to 12-31-2011\n",
      "First request failed\n",
      "Missing co2, 2011\n",
      "Getting all submissions with query 'co2' within subreddit changemyview from 1-1-2011 to 12-31-2011\n",
      "First request failed\n",
      "Missing climate crisis, 2011\n",
      "Getting all submissions with query 'climate crisis' within subreddit changemyview from 1-1-2011 to 12-31-2011\n",
      "First request failed\n",
      "Missing climate emergency, 2011\n",
      "Getting all submissions with query 'climate emergency' within subreddit changemyview from 1-1-2011 to 12-31-2011\n",
      "Num submissions: 0 0\n",
      "Saved query submissions to pushshift_output/posts/changemyview/1-1-2011_to_12-31-2011/climate emergency.pkl!\n",
      "Missing extreme weather, 2011\n",
      "Getting all submissions with query 'extreme weather' within subreddit changemyview from 1-1-2011 to 12-31-2011\n",
      "First request failed\n",
      "Missing 2 degree, 2011\n",
      "Getting all submissions with query '2 degree' within subreddit changemyview from 1-1-2011 to 12-31-2011\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num submissions: 0 0\n",
      "Saved query submissions to pushshift_output/posts/changemyview/1-1-2011_to_12-31-2011/2 degree.pkl!\n",
      "Missing sustainable, 2011\n",
      "Getting all submissions with query 'sustainable' within subreddit changemyview from 1-1-2011 to 12-31-2011\n",
      "Num submissions: 0 0\n",
      "Saved query submissions to pushshift_output/posts/changemyview/1-1-2011_to_12-31-2011/sustainable.pkl!\n",
      "Missing clean energy, 2011\n",
      "Getting all submissions with query 'clean energy' within subreddit changemyview from 1-1-2011 to 12-31-2011\n",
      "First request failed\n",
      "Missing renewable, 2011\n",
      "Getting all submissions with query 'renewable' within subreddit changemyview from 1-1-2011 to 12-31-2011\n",
      "Num submissions: 0 0\n",
      "Saved query submissions to pushshift_output/posts/changemyview/1-1-2011_to_12-31-2011/renewable.pkl!\n",
      "Missing cap and trade, 2011\n",
      "Getting all submissions with query 'cap and trade' within subreddit changemyview from 1-1-2011 to 12-31-2011\n",
      "First request failed\n",
      "Missing sea level rise, 2011\n",
      "Getting all submissions with query 'sea level rise' within subreddit changemyview from 1-1-2011 to 12-31-2011\n",
      "First request failed\n",
      "Missing environmental justice, 2011\n",
      "Getting all submissions with query 'environmental justice' within subreddit changemyview from 1-1-2011 to 12-31-2011\n",
      "First request failed\n",
      "Missing climate justice, 2011\n",
      "Getting all submissions with query 'climate justice' within subreddit changemyview from 1-1-2011 to 12-31-2011\n",
      "Num submissions: 0 0\n",
      "Saved query submissions to pushshift_output/posts/changemyview/1-1-2011_to_12-31-2011/climate justice.pkl!\n",
      "Missing COP, 2011\n",
      "Getting all submissions with query 'COP' within subreddit changemyview from 1-1-2011 to 12-31-2011\n",
      "First request failed\n",
      "Missing IPCC, 2011\n",
      "Getting all submissions with query 'IPCC' within subreddit changemyview from 1-1-2011 to 12-31-2011\n",
      "First request failed\n",
      "Missing deforestation, 2011\n",
      "Getting all submissions with query 'deforestation' within subreddit changemyview from 1-1-2011 to 12-31-2011\n",
      "First request failed\n",
      "Missing permafrost, 2011\n",
      "Getting all submissions with query 'permafrost' within subreddit changemyview from 1-1-2011 to 12-31-2011\n",
      "Num submissions: 0 0\n",
      "Saved query submissions to pushshift_output/posts/changemyview/1-1-2011_to_12-31-2011/permafrost.pkl!\n",
      "Missing glacier, 2011\n",
      "Getting all submissions with query 'glacier' within subreddit changemyview from 1-1-2011 to 12-31-2011\n",
      "First request failed\n",
      "Missing drought, 2011\n",
      "Getting all submissions with query 'drought' within subreddit changemyview from 1-1-2011 to 12-31-2011\n",
      "Num submissions: 0 0\n",
      "Saved query submissions to pushshift_output/posts/changemyview/1-1-2011_to_12-31-2011/drought.pkl!\n",
      "Missing ecosystem, 2011\n",
      "Getting all submissions with query 'ecosystem' within subreddit changemyview from 1-1-2011 to 12-31-2011\n",
      "Num submissions: 0 0\n",
      "Saved query submissions to pushshift_output/posts/changemyview/1-1-2011_to_12-31-2011/ecosystem.pkl!\n",
      "Missing greenhouse gas, 2011\n",
      "Getting all submissions with query 'greenhouse gas' within subreddit changemyview from 1-1-2011 to 12-31-2011\n",
      "Num submissions: 0 0\n",
      "Saved query submissions to pushshift_output/posts/changemyview/1-1-2011_to_12-31-2011/greenhouse gas.pkl!\n",
      "Missing greenhouse effect, 2011\n",
      "Getting all submissions with query 'greenhouse effect' within subreddit changemyview from 1-1-2011 to 12-31-2011\n",
      "Num submissions: 0 0\n",
      "Saved query submissions to pushshift_output/posts/changemyview/1-1-2011_to_12-31-2011/greenhouse effect.pkl!\n",
      "Missing green new deal, 2011\n",
      "Getting all submissions with query 'green new deal' within subreddit changemyview from 1-1-2011 to 12-31-2011\n",
      "Num submissions: 0 0\n",
      "Saved query submissions to pushshift_output/posts/changemyview/1-1-2011_to_12-31-2011/green new deal.pkl!\n",
      "Missing EPA, 2011\n",
      "Getting all submissions with query 'EPA' within subreddit changemyview from 1-1-2011 to 12-31-2011\n",
      "Num submissions: 0 0\n",
      "Saved query submissions to pushshift_output/posts/changemyview/1-1-2011_to_12-31-2011/EPA.pkl!\n",
      "Missing global warming, 2012\n",
      "Getting all submissions with query 'global warming' within subreddit changemyview from 1-1-2012 to 12-31-2012\n",
      "First request failed\n",
      "Missing climate change, 2012\n",
      "Getting all submissions with query 'climate change' within subreddit changemyview from 1-1-2012 to 12-31-2012\n",
      "First request failed\n",
      "Missing carbon, 2012\n",
      "Getting all submissions with query 'carbon' within subreddit changemyview from 1-1-2012 to 12-31-2012\n",
      "First request failed\n",
      "Missing fossil fuel, 2012\n",
      "Getting all submissions with query 'fossil fuel' within subreddit changemyview from 1-1-2012 to 12-31-2012\n",
      "Num submissions: 0 0\n",
      "Saved query submissions to pushshift_output/posts/changemyview/1-1-2012_to_12-31-2012/fossil fuel.pkl!\n",
      "Missing methane, 2012\n",
      "Getting all submissions with query 'methane' within subreddit changemyview from 1-1-2012 to 12-31-2012\n",
      "First request failed\n",
      "Missing environment, 2012\n",
      "Getting all submissions with query 'environment' within subreddit changemyview from 1-1-2012 to 12-31-2012\n",
      "First request failed\n",
      "Missing co2, 2012\n",
      "Getting all submissions with query 'co2' within subreddit changemyview from 1-1-2012 to 12-31-2012\n",
      "First request failed\n",
      "Missing climate crisis, 2012\n",
      "Getting all submissions with query 'climate crisis' within subreddit changemyview from 1-1-2012 to 12-31-2012\n",
      "First request failed\n",
      "Missing climate emergency, 2012\n",
      "Getting all submissions with query 'climate emergency' within subreddit changemyview from 1-1-2012 to 12-31-2012\n",
      "First request failed\n",
      "Missing extreme weather, 2012\n",
      "Getting all submissions with query 'extreme weather' within subreddit changemyview from 1-1-2012 to 12-31-2012\n",
      "Num submissions: 0 0\n",
      "Saved query submissions to pushshift_output/posts/changemyview/1-1-2012_to_12-31-2012/extreme weather.pkl!\n",
      "Missing 2 degree, 2012\n",
      "Getting all submissions with query '2 degree' within subreddit changemyview from 1-1-2012 to 12-31-2012\n",
      "First request failed\n",
      "Missing sustainable, 2012\n",
      "Getting all submissions with query 'sustainable' within subreddit changemyview from 1-1-2012 to 12-31-2012\n",
      "First request failed\n",
      "Missing clean energy, 2012\n",
      "Getting all submissions with query 'clean energy' within subreddit changemyview from 1-1-2012 to 12-31-2012\n",
      "First request failed\n",
      "Missing renewable, 2012\n",
      "Getting all submissions with query 'renewable' within subreddit changemyview from 1-1-2012 to 12-31-2012\n",
      "First request failed\n",
      "Missing cap and trade, 2012\n",
      "Getting all submissions with query 'cap and trade' within subreddit changemyview from 1-1-2012 to 12-31-2012\n",
      "First request failed\n",
      "Missing sea level rise, 2012\n",
      "Getting all submissions with query 'sea level rise' within subreddit changemyview from 1-1-2012 to 12-31-2012\n",
      "First request failed\n",
      "Missing environmental justice, 2012\n",
      "Getting all submissions with query 'environmental justice' within subreddit changemyview from 1-1-2012 to 12-31-2012\n",
      "First request failed\n",
      "Missing climate justice, 2012\n",
      "Getting all submissions with query 'climate justice' within subreddit changemyview from 1-1-2012 to 12-31-2012\n",
      "Num submissions: 0 0\n",
      "Saved query submissions to pushshift_output/posts/changemyview/1-1-2012_to_12-31-2012/climate justice.pkl!\n",
      "Missing COP, 2012\n",
      "Getting all submissions with query 'COP' within subreddit changemyview from 1-1-2012 to 12-31-2012\n",
      "First request failed\n",
      "Missing IPCC, 2012\n",
      "Getting all submissions with query 'IPCC' within subreddit changemyview from 1-1-2012 to 12-31-2012\n",
      "First request failed\n",
      "Missing deforestation, 2012\n",
      "Getting all submissions with query 'deforestation' within subreddit changemyview from 1-1-2012 to 12-31-2012\n",
      "First request failed\n",
      "Missing permafrost, 2012\n",
      "Getting all submissions with query 'permafrost' within subreddit changemyview from 1-1-2012 to 12-31-2012\n",
      "First request failed\n",
      "Missing glacier, 2012\n",
      "Getting all submissions with query 'glacier' within subreddit changemyview from 1-1-2012 to 12-31-2012\n",
      "First request failed\n",
      "Missing drought, 2012\n",
      "Getting all submissions with query 'drought' within subreddit changemyview from 1-1-2012 to 12-31-2012\n",
      "Num submissions: 0 0\n",
      "Saved query submissions to pushshift_output/posts/changemyview/1-1-2012_to_12-31-2012/drought.pkl!\n",
      "Missing ecosystem, 2012\n",
      "Getting all submissions with query 'ecosystem' within subreddit changemyview from 1-1-2012 to 12-31-2012\n",
      "First request failed\n",
      "Missing greenhouse gas, 2012\n",
      "Getting all submissions with query 'greenhouse gas' within subreddit changemyview from 1-1-2012 to 12-31-2012\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First request failed\n",
      "Missing greenhouse effect, 2012\n",
      "Getting all submissions with query 'greenhouse effect' within subreddit changemyview from 1-1-2012 to 12-31-2012\n",
      "First request failed\n",
      "Missing green new deal, 2012\n",
      "Getting all submissions with query 'green new deal' within subreddit changemyview from 1-1-2012 to 12-31-2012\n",
      "First request failed\n",
      "Missing EPA, 2012\n",
      "Getting all submissions with query 'EPA' within subreddit changemyview from 1-1-2012 to 12-31-2012\n",
      "First request failed\n",
      "Missing global warming, 2013\n",
      "Getting all submissions with query 'global warming' within subreddit changemyview from 1-1-2013 to 12-31-2013\n",
      "Num submissions: 0 0\n",
      "Saved query submissions to pushshift_output/posts/changemyview/1-1-2013_to_12-31-2013/global warming.pkl!\n",
      "Missing climate change, 2013\n",
      "Getting all submissions with query 'climate change' within subreddit changemyview from 1-1-2013 to 12-31-2013\n",
      "First request failed\n",
      "Missing carbon, 2013\n",
      "Getting all submissions with query 'carbon' within subreddit changemyview from 1-1-2013 to 12-31-2013\n",
      "First request failed\n",
      "Missing fossil fuel, 2013\n",
      "Getting all submissions with query 'fossil fuel' within subreddit changemyview from 1-1-2013 to 12-31-2013\n",
      "First request failed\n",
      "Missing methane, 2013\n",
      "Getting all submissions with query 'methane' within subreddit changemyview from 1-1-2013 to 12-31-2013\n",
      "Num submissions: 0 0\n",
      "Saved query submissions to pushshift_output/posts/changemyview/1-1-2013_to_12-31-2013/methane.pkl!\n",
      "Missing environment, 2013\n",
      "Getting all submissions with query 'environment' within subreddit changemyview from 1-1-2013 to 12-31-2013\n",
      "First request failed\n",
      "Missing co2, 2013\n",
      "Getting all submissions with query 'co2' within subreddit changemyview from 1-1-2013 to 12-31-2013\n",
      "First request failed\n",
      "Missing climate crisis, 2013\n",
      "Getting all submissions with query 'climate crisis' within subreddit changemyview from 1-1-2013 to 12-31-2013\n",
      "First request failed\n",
      "Missing climate emergency, 2013\n",
      "Getting all submissions with query 'climate emergency' within subreddit changemyview from 1-1-2013 to 12-31-2013\n",
      "First request failed\n",
      "Missing extreme weather, 2013\n",
      "Getting all submissions with query 'extreme weather' within subreddit changemyview from 1-1-2013 to 12-31-2013\n",
      "Num submissions: 0 0\n",
      "Saved query submissions to pushshift_output/posts/changemyview/1-1-2013_to_12-31-2013/extreme weather.pkl!\n",
      "Missing 2 degree, 2013\n",
      "Getting all submissions with query '2 degree' within subreddit changemyview from 1-1-2013 to 12-31-2013\n",
      "First request failed\n",
      "Missing sustainable, 2013\n",
      "Getting all submissions with query 'sustainable' within subreddit changemyview from 1-1-2013 to 12-31-2013\n",
      "Num submissions: 0 0\n",
      "Saved query submissions to pushshift_output/posts/changemyview/1-1-2013_to_12-31-2013/sustainable.pkl!\n",
      "Missing clean energy, 2013\n",
      "Getting all submissions with query 'clean energy' within subreddit changemyview from 1-1-2013 to 12-31-2013\n",
      "Num submissions: 0 0\n",
      "Saved query submissions to pushshift_output/posts/changemyview/1-1-2013_to_12-31-2013/clean energy.pkl!\n",
      "Missing renewable, 2013\n",
      "Getting all submissions with query 'renewable' within subreddit changemyview from 1-1-2013 to 12-31-2013\n",
      "Num submissions: 0 0\n",
      "Saved query submissions to pushshift_output/posts/changemyview/1-1-2013_to_12-31-2013/renewable.pkl!\n",
      "Missing cap and trade, 2013\n",
      "Getting all submissions with query 'cap and trade' within subreddit changemyview from 1-1-2013 to 12-31-2013\n",
      "Num submissions: 0 0\n",
      "Saved query submissions to pushshift_output/posts/changemyview/1-1-2013_to_12-31-2013/cap and trade.pkl!\n",
      "Missing sea level rise, 2013\n",
      "Getting all submissions with query 'sea level rise' within subreddit changemyview from 1-1-2013 to 12-31-2013\n",
      "Num submissions: 0 0\n",
      "Saved query submissions to pushshift_output/posts/changemyview/1-1-2013_to_12-31-2013/sea level rise.pkl!\n",
      "Missing environmental justice, 2013\n",
      "Getting all submissions with query 'environmental justice' within subreddit changemyview from 1-1-2013 to 12-31-2013\n",
      "Num submissions: 0 0\n",
      "Saved query submissions to pushshift_output/posts/changemyview/1-1-2013_to_12-31-2013/environmental justice.pkl!\n",
      "Missing climate justice, 2013\n",
      "Getting all submissions with query 'climate justice' within subreddit changemyview from 1-1-2013 to 12-31-2013\n",
      "Num submissions: 0 0\n",
      "Saved query submissions to pushshift_output/posts/changemyview/1-1-2013_to_12-31-2013/climate justice.pkl!\n",
      "Missing COP, 2013\n",
      "Getting all submissions with query 'COP' within subreddit changemyview from 1-1-2013 to 12-31-2013\n",
      "Num submissions: 0 0\n",
      "Saved query submissions to pushshift_output/posts/changemyview/1-1-2013_to_12-31-2013/COP.pkl!\n",
      "Missing IPCC, 2013\n",
      "Getting all submissions with query 'IPCC' within subreddit changemyview from 1-1-2013 to 12-31-2013\n",
      "First request failed\n",
      "Missing deforestation, 2013\n",
      "Getting all submissions with query 'deforestation' within subreddit changemyview from 1-1-2013 to 12-31-2013\n",
      "First request failed\n",
      "Missing permafrost, 2013\n",
      "Getting all submissions with query 'permafrost' within subreddit changemyview from 1-1-2013 to 12-31-2013\n",
      "Num submissions: 0 0\n",
      "Saved query submissions to pushshift_output/posts/changemyview/1-1-2013_to_12-31-2013/permafrost.pkl!\n",
      "Missing glacier, 2013\n",
      "Getting all submissions with query 'glacier' within subreddit changemyview from 1-1-2013 to 12-31-2013\n",
      "First request failed\n",
      "Missing drought, 2013\n",
      "Getting all submissions with query 'drought' within subreddit changemyview from 1-1-2013 to 12-31-2013\n",
      "First request failed\n",
      "Missing ecosystem, 2013\n",
      "Getting all submissions with query 'ecosystem' within subreddit changemyview from 1-1-2013 to 12-31-2013\n",
      "First request failed\n",
      "Missing greenhouse gas, 2013\n",
      "Getting all submissions with query 'greenhouse gas' within subreddit changemyview from 1-1-2013 to 12-31-2013\n",
      "First request failed\n",
      "Missing greenhouse effect, 2013\n",
      "Getting all submissions with query 'greenhouse effect' within subreddit changemyview from 1-1-2013 to 12-31-2013\n",
      "First request failed\n",
      "Missing green new deal, 2013\n",
      "Getting all submissions with query 'green new deal' within subreddit changemyview from 1-1-2013 to 12-31-2013\n",
      "First request failed\n",
      "Missing EPA, 2013\n",
      "Getting all submissions with query 'EPA' within subreddit changemyview from 1-1-2013 to 12-31-2013\n",
      "Num submissions: 0 0\n",
      "Saved query submissions to pushshift_output/posts/changemyview/1-1-2013_to_12-31-2013/EPA.pkl!\n",
      "Missing global warming, 2014\n",
      "Getting all submissions with query 'global warming' within subreddit changemyview from 1-1-2014 to 12-31-2014\n",
      "First request failed\n",
      "Missing climate change, 2014\n",
      "Getting all submissions with query 'climate change' within subreddit changemyview from 1-1-2014 to 12-31-2014\n",
      "First request failed\n",
      "Missing carbon, 2014\n",
      "Getting all submissions with query 'carbon' within subreddit changemyview from 1-1-2014 to 12-31-2014\n",
      "First request failed\n",
      "Missing fossil fuel, 2014\n",
      "Getting all submissions with query 'fossil fuel' within subreddit changemyview from 1-1-2014 to 12-31-2014\n",
      "First request failed\n",
      "Missing methane, 2014\n",
      "Getting all submissions with query 'methane' within subreddit changemyview from 1-1-2014 to 12-31-2014\n",
      "First request failed\n",
      "Missing environment, 2014\n",
      "Getting all submissions with query 'environment' within subreddit changemyview from 1-1-2014 to 12-31-2014\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-55-f759d09e4aca>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Missing {}, {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeyword\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstart_year\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m             pushshift_wrapper('1-1-{}'.format(start_year),'12-31-{}'.format(start_year),\n\u001b[0;32m---> 11\u001b[0;31m                               'submission',subreddit='changemyview',query=keyword)\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-54-2caa3d1e9dbc>\u001b[0m in \u001b[0;36mpushshift_wrapper\u001b[0;34m(after_str, before_str, datatype, subreddit, query, keywords)\u001b[0m\n\u001b[1;32m     28\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m                 data = getPushshiftDataForSub(subreddit, query, \n\u001b[0;32m---> 30\u001b[0;31m                                               after_str, before_str, datatype)\n\u001b[0m\u001b[1;32m     31\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mJSONDecodeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m                 \u001b[0mfailed_requests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubreddit\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mafter_str\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbefore_str\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdatatype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-17-64aed0d93794>\u001b[0m in \u001b[0;36mgetPushshiftDataForSub\u001b[0;34m(subreddit, query, after_str, before_str, datatype)\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;34m'&before='\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbefore_date\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;31m#print(url)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m     \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/u/nlp/anaconda/main/anaconda3/envs/yiwei-climate/lib/python3.6/site-packages/requests/api.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(url, params, **kwargs)\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetdefault\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'allow_redirects'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'get'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/u/nlp/anaconda/main/anaconda3/envs/yiwei-climate/lib/python3.6/site-packages/requests/api.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;31m# cases, and look like a memory leak in others.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0msessions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/u/nlp/anaconda/main/anaconda3/envs/yiwei-climate/lib/python3.6/site-packages/requests/sessions.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    531\u001b[0m         }\n\u001b[1;32m    532\u001b[0m         \u001b[0msend_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 533\u001b[0;31m         \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0msend_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    534\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    535\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/u/nlp/anaconda/main/anaconda3/envs/yiwei-climate/lib/python3.6/site-packages/requests/sessions.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    644\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    645\u001b[0m         \u001b[0;31m# Send the request\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 646\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madapter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    647\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    648\u001b[0m         \u001b[0;31m# Total elapsed time of the request (approximately)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/u/nlp/anaconda/main/anaconda3/envs/yiwei-climate/lib/python3.6/site-packages/requests/adapters.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    447\u001b[0m                     \u001b[0mdecode_content\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m                     \u001b[0mretries\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_retries\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 449\u001b[0;31m                     \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    450\u001b[0m                 )\n\u001b[1;32m    451\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/u/nlp/anaconda/main/anaconda3/envs/yiwei-climate/lib/python3.6/site-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    675\u001b[0m                 \u001b[0mbody\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    676\u001b[0m                 \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 677\u001b[0;31m                 \u001b[0mchunked\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mchunked\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    678\u001b[0m             )\n\u001b[1;32m    679\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/u/nlp/anaconda/main/anaconda3/envs/yiwei-climate/lib/python3.6/site-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    424\u001b[0m                     \u001b[0;31m# Python 3 (including for exceptions like SystemExit).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    425\u001b[0m                     \u001b[0;31m# Otherwise it looks like a bug in the code.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 426\u001b[0;31m                     \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    427\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mSocketTimeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBaseSSLError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSocketError\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_raise_timeout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mread_timeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/u/nlp/anaconda/main/anaconda3/envs/yiwei-climate/lib/python3.6/site-packages/urllib3/packages/six.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
      "\u001b[0;32m/u/nlp/anaconda/main/anaconda3/envs/yiwei-climate/lib/python3.6/site-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    419\u001b[0m                 \u001b[0;31m# Python 3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    420\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 421\u001b[0;31m                     \u001b[0mhttplib_response\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetresponse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    422\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    423\u001b[0m                     \u001b[0;31m# Remove the TypeError from the exception chain in\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/u/nlp/anaconda/main/anaconda3/envs/yiwei-climate/lib/python3.6/http/client.py\u001b[0m in \u001b[0;36mgetresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1329\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1330\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1331\u001b[0;31m                 \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbegin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1332\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mConnectionError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1333\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/u/nlp/anaconda/main/anaconda3/envs/yiwei-climate/lib/python3.6/http/client.py\u001b[0m in \u001b[0;36mbegin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    319\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mUnknownProtocol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mversion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 321\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheaders\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparse_headers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    322\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebuglevel\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/u/nlp/anaconda/main/anaconda3/envs/yiwei-climate/lib/python3.6/http/client.py\u001b[0m in \u001b[0;36mparse_headers\u001b[0;34m(fp, _class)\u001b[0m\n\u001b[1;32m    205\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m         \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_MAXLINE\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 207\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0m_MAXLINE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    208\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mLineTooLong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"header line\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m         \u001b[0mheaders\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Collect all posts w/ climate change keyword from CMV  \n",
    "\n",
    "for start_year in range(2010,2015,1):\n",
    "    end_year = start_year+1\n",
    "    for keyword in KEYWORDS_LONG:\n",
    "        if not os.path.exists(os.path.join('pushshift_output','posts','changemyview'\n",
    "                                           '1-1-{}_to_12-31-{}'.format(start_year,start_year),\n",
    "                                           '{}.pkl'.format(keyword))):\n",
    "            print(\"Missing {}, {}\".format(keyword,start_year))\n",
    "            pushshift_wrapper('1-1-{}'.format(start_year),'12-31-{}'.format(start_year),\n",
    "                              'submission',subreddit='changemyview',query=keyword)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "out = getPushshiftDataForSub('changemyview', 'the', '1-1-{}'.format(start_year),\n",
    "                       '12-31-{}'.format(start_year), 'submission')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1-1-2010'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'1-1-{}'.format(start_year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# for start_year in range(2015,2021,1):\n",
    "#     end_year = start_year+1\n",
    "#     pushshift_wrapper('1-1-{}'.format(start_year),'12-31-{}'.format(start_year),'submission',\n",
    "#                       query=None,keywords=keywords_missing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Get all comments attached to a post"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## PRAW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Get IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "419100"
      ]
     },
     "execution_count": 410,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "posts_with_comments_ids = pickle.load(open('output/posts_with_comments_ids.pkl','rb'))\n",
    "print(len(posts_with_comments_ids))\n",
    "sub_ids_to_fetch = list(posts_with_comments_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# with open('output/comment_ids_per_post.tsv','w') as f:\n",
    "#     f.write(\"{}\\t{}\\n\".format('post_id','comment_ids'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def praw_get_comments(sub_id):\n",
    "    try:\n",
    "        post = reddit.submission(id=sub_id)\n",
    "        post_author = post.author\n",
    "        post_title = post.title\n",
    "        post_comms = list(post.__dict__['_comments_by_id'].keys())\n",
    "        #print(len(post_comms))\n",
    "        comments_per_post[sub_id] = post_comms\n",
    "\n",
    "        with open('output/comment_ids_per_post.tsv','a') as f:\n",
    "            f.write(\"{}\\t{}\\n\".format(sub_id,','.join(post_comms)))\n",
    "    except Forbidden:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "175000\n",
      "176000\n",
      "177000\n",
      "178000\n",
      "179000\n",
      "180000\n",
      "181000\n",
      "182000\n",
      "183000\n",
      "184000\n",
      "185000\n",
      "186000\n",
      "187000\n",
      "188000\n",
      "189000\n",
      "190000\n",
      "191000\n",
      "192000\n",
      "193000\n",
      "194000\n",
      "195000\n",
      "196000\n",
      "197000\n",
      "198000\n",
      "199000\n",
      "200000\n",
      "201000\n",
      "202000\n",
      "203000\n",
      "204000\n",
      "205000\n",
      "206000\n",
      "207000\n",
      "208000\n",
      "209000\n",
      "210000\n",
      "211000\n",
      "212000\n",
      "213000\n",
      "214000\n",
      "215000\n",
      "216000\n",
      "217000\n",
      "218000\n",
      "219000\n",
      "220000\n",
      "221000\n",
      "222000\n",
      "223000\n",
      "224000\n",
      "225000\n",
      "226000\n",
      "227000\n",
      "228000\n",
      "229000\n",
      "230000\n",
      "231000\n",
      "232000\n",
      "233000\n",
      "234000\n",
      "235000\n",
      "236000\n",
      "237000\n",
      "238000\n",
      "239000\n",
      "240000\n",
      "241000\n",
      "242000\n",
      "243000\n",
      "244000\n",
      "245000\n",
      "246000\n",
      "247000\n",
      "248000\n",
      "249000\n",
      "250000\n",
      "251000\n",
      "252000\n",
      "253000\n",
      "254000\n",
      "255000\n",
      "256000\n",
      "257000\n",
      "258000\n",
      "259000\n",
      "260000\n",
      "261000\n",
      "262000\n",
      "263000\n",
      "264000\n",
      "265000\n",
      "266000\n",
      "267000\n",
      "268000\n",
      "269000\n",
      "270000\n",
      "271000\n",
      "272000\n",
      "273000\n",
      "274000\n",
      "275000\n",
      "276000\n",
      "277000\n",
      "278000\n",
      "279000\n",
      "280000\n",
      "281000\n",
      "282000\n",
      "283000\n",
      "284000\n",
      "285000\n",
      "286000\n",
      "287000\n",
      "288000\n",
      "289000\n",
      "290000\n",
      "291000\n",
      "292000\n",
      "293000\n",
      "294000\n",
      "295000\n",
      "296000\n",
      "297000\n",
      "298000\n",
      "299000\n",
      "300000\n",
      "301000\n",
      "302000\n",
      "303000\n",
      "304000\n",
      "305000\n",
      "306000\n",
      "307000\n",
      "308000\n",
      "309000\n",
      "310000\n",
      "311000\n",
      "312000\n",
      "313000\n",
      "314000\n",
      "315000\n",
      "316000\n",
      "317000\n",
      "318000\n",
      "319000\n",
      "320000\n",
      "321000\n",
      "322000\n",
      "323000\n",
      "324000\n",
      "325000\n",
      "326000\n",
      "327000\n",
      "328000\n",
      "329000\n",
      "330000\n",
      "331000\n",
      "332000\n",
      "333000\n",
      "334000\n",
      "335000\n",
      "336000\n",
      "337000\n",
      "338000\n",
      "339000\n",
      "340000\n",
      "341000\n",
      "342000\n",
      "343000\n",
      "344000\n",
      "345000\n",
      "346000\n",
      "347000\n",
      "348000\n",
      "349000\n",
      "350000\n",
      "351000\n",
      "352000\n",
      "353000\n",
      "354000\n",
      "355000\n",
      "356000\n",
      "357000\n",
      "358000\n",
      "359000\n",
      "360000\n",
      "361000\n",
      "362000\n",
      "363000\n",
      "364000\n",
      "365000\n",
      "366000\n",
      "367000\n",
      "368000\n",
      "369000\n",
      "370000\n",
      "371000\n",
      "372000\n",
      "373000\n",
      "374000\n",
      "375000\n",
      "376000\n",
      "377000\n",
      "378000\n",
      "379000\n",
      "380000\n",
      "381000\n",
      "382000\n",
      "383000\n",
      "384000\n",
      "385000\n",
      "386000\n",
      "387000\n",
      "388000\n",
      "389000\n",
      "390000\n",
      "391000\n",
      "392000\n",
      "393000\n",
      "394000\n",
      "395000\n",
      "396000\n",
      "397000\n",
      "398000\n",
      "399000\n",
      "400000\n",
      "401000\n",
      "402000\n",
      "403000\n",
      "404000\n",
      "405000\n",
      "406000\n",
      "407000\n",
      "408000\n",
      "409000\n",
      "410000\n",
      "411000\n",
      "412000\n",
      "413000\n",
      "414000\n",
      "415000\n",
      "416000\n",
      "417000\n",
      "418000\n",
      "419000\n"
     ]
    }
   ],
   "source": [
    "for ix_sub_id in range(174623,len(sub_ids_to_fetch)):\n",
    "    sub_id = sub_ids_to_fetch[ix_sub_id]\n",
    "    praw_get_comments(sub_id)\n",
    "    \n",
    "    if ix_sub_id % 1000 == 0:\n",
    "        print(ix_sub_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(174623, '92efdf')"
      ]
     },
     "execution_count": 460,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ix_sub_id,sub_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11720"
      ]
     },
     "execution_count": 447,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(comments_per_post)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>post_id</th>\n",
       "      <th>comment_ids</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>anbg0</td>\n",
       "      <td>t1_c0ignhg,t1_c0ignz0,t1_c0igq3n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>apn8b</td>\n",
       "      <td>t1_c0irhej,t1_c0is8sw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>aprgq</td>\n",
       "      <td>t1_c0isfnb,t1_c0is90g,t1_c0iscd2,t1_c0isbol,t1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>aprgv</td>\n",
       "      <td>t1_c0is3y4,t1_c0ism1k,t1_c0isyft,t1_c0isvkx,t1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>aprie</td>\n",
       "      <td>t1_c0is4by,t1_c0islml,t1_c0ismfv,t1_c0isk7q,t1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>403412</th>\n",
       "      <td>3yuldj</td>\n",
       "      <td>t1_cygrqj0,t1_cygrhs5,t1_cygtcrx,t1_cygtrvf,t1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>403413</th>\n",
       "      <td>3yulsl</td>\n",
       "      <td>t1_cylxj3h,t1_cygqso3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>403414</th>\n",
       "      <td>3yup0j</td>\n",
       "      <td>t1_cyhjhnw,t1_cylxiyc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>403415</th>\n",
       "      <td>3yuytd</td>\n",
       "      <td>t1_cygu9uo,t1_cygx6g5,t1_cyhpqys,t1_cygxgig,t1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>403416</th>\n",
       "      <td>3yve0p</td>\n",
       "      <td>t1_cyh9a2i</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>403417 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       post_id                                        comment_ids\n",
       "0        anbg0                   t1_c0ignhg,t1_c0ignz0,t1_c0igq3n\n",
       "1        apn8b                              t1_c0irhej,t1_c0is8sw\n",
       "2        aprgq  t1_c0isfnb,t1_c0is90g,t1_c0iscd2,t1_c0isbol,t1...\n",
       "3        aprgv  t1_c0is3y4,t1_c0ism1k,t1_c0isyft,t1_c0isvkx,t1...\n",
       "4        aprie  t1_c0is4by,t1_c0islml,t1_c0ismfv,t1_c0isk7q,t1...\n",
       "...        ...                                                ...\n",
       "403412  3yuldj  t1_cygrqj0,t1_cygrhs5,t1_cygtcrx,t1_cygtrvf,t1...\n",
       "403413  3yulsl                              t1_cylxj3h,t1_cygqso3\n",
       "403414  3yup0j                              t1_cyhjhnw,t1_cylxiyc\n",
       "403415  3yuytd  t1_cygu9uo,t1_cygx6g5,t1_cyhpqys,t1_cygxgig,t1...\n",
       "403416  3yve0p                                         t1_cyh9a2i\n",
       "\n",
       "[403417 rows x 2 columns]"
      ]
     },
     "execution_count": 462,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv('output/comment_ids_per_post.tsv',sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Get text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Personally I am more concerned about the fact that methane is an extremely effective greenhouse gas.  '"
      ]
     },
     "execution_count": 405,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comment = reddit.comment(\"c0st842\")\n",
    "comment.body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "comment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "comment_ids_per_post = pd.read_csv('output/comment_ids_per_post.tsv',sep='\\t')\n",
    "comment_ids = comment_ids_per_post['comment_ids']\n",
    "all_comment_ids = [x.split(',') for x in comment_ids]\n",
    "all_comment_ids = [item for sublist in all_comment_ids for item in sublist]\n",
    "unique_comment_ids = set(all_comment_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print(len(all_comment_ids),len(unique_comment_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Dataframe of \n",
    "# comment_id | text\n",
    "\n",
    "with open('output/text_per_comment.tsv','w') as f:\n",
    "    f.write('{}\\t{}\\n'.format('comment_id','text'))\n",
    "    \n",
    "unique_comment_ids = list(unique_comment_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "for c_id_index in range(len(unique_comment_ids)):\n",
    "    c_id = unique_comment_ids[c_id_index]\n",
    "    comm = reddit.comment(c_id)\n",
    "    comment_body = strip_whitespace(comm.body)\n",
    "    \n",
    "    with open('output/text_per_comment.tsv','a') as f:\n",
    "        f.write('{}\\t{}\\n'.format(c_id,comment_body))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
