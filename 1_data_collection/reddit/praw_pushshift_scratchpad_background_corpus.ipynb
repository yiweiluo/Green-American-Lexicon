{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from pprint import pprint\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import praw\n",
    "from praw.models import Submission\n",
    "#from psaw import PushshiftAPI\n",
    "from prawcore.exceptions import Forbidden\n",
    "from praw.exceptions import ClientException\n",
    "import csv\n",
    "import os\n",
    "from urllib.error import HTTPError\n",
    "import glob\n",
    "import requests\n",
    "import json\n",
    "from json import JSONDecodeError\n",
    "import datetime\n",
    "import pickle\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "_RE_COMBINE_WHITESPACE = re.compile(r\"\\s+\")\n",
    "\n",
    "def strip_whitespace(text):\n",
    "    return _RE_COMBINE_WHITESPACE.sub(\" \", text).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "code_folding": [
     0,
     50
    ]
   },
   "outputs": [],
   "source": [
    "def get_submissions(reddit_instance,subreddit_str):\n",
    "    \n",
    "    if not os.path.exists('praw_output'):\n",
    "        os.mkdir('praw_output')\n",
    "    \n",
    "    subreddit = reddit_instance.subreddit(subreddit_str)\n",
    "    print('Getting submissions and comments from: {}'.format(subreddit.display_name))  \n",
    "    try:\n",
    "        title = subreddit.title\n",
    "        desc = subreddit.description\n",
    "\n",
    "        if not os.path.exists('subreddits.tsv'):\n",
    "            with open('subreddits.tsv','w') as f:\n",
    "                csvwriter = csv.writer(f, delimiter='\\t')\n",
    "                csvwriter.writerow([subreddit.display_name,title,desc])\n",
    "        else:\n",
    "            with open('subreddits.tsv','a') as f:\n",
    "                csvwriter = csv.writer(f, delimiter='\\t')\n",
    "                csvwriter.writerow([subreddit.display_name,title,desc])\n",
    "\n",
    "        # Write header\n",
    "        with open(os.path.join('praw_output','{}.tsv'.format(subreddit.display_name)), 'w', newline='\\n') as csvfile:\n",
    "            csvwriter = csv.writer(csvfile, delimiter='\\t',\n",
    "                                    quotechar='|', quoting=csv.QUOTE_MINIMAL)\n",
    "            csvwriter.writerow(['title','author','date','is_video','id','num_downs','num_ups','upvote_ratio',\n",
    "                               'num_comments','score','text','subreddit'])\n",
    "\n",
    "        # Write body\n",
    "        for submission in subreddit.new(limit=None):\n",
    "            sub_title = submission.title\n",
    "            sub_author = submission.author.name if submission.author is not None else -1\n",
    "            sub_date = submission.created\n",
    "            sub_is_vid = submission.is_video\n",
    "            sub_id = submission.id\n",
    "            sub_downvotes = submission.downs\n",
    "            sub_upvotes = submission.ups\n",
    "            sub_upvote_ratio = submission.upvote_ratio\n",
    "            sub_num_comments = submission.num_comments\n",
    "            sub_score = submission.score\n",
    "            sub_text = submission.selftext.strip().replace('\\t','').replace('\\n','')\n",
    "            sub_subreddit = submission.subreddit.display_name\n",
    "            with open(os.path.join('praw_output','{}.tsv'.format(subreddit.display_name)), 'a', newline='\\n') as csvfile:\n",
    "                csvwriter = csv.writer(csvfile, delimiter='\\t',\n",
    "                                    quotechar='|', quoting=csv.QUOTE_MINIMAL)\n",
    "                csvwriter.writerow([sub_title,sub_author,sub_date,sub_is_vid,sub_id,sub_downvotes,\n",
    "                                   sub_upvotes,sub_upvote_ratio,sub_num_comments,sub_score,sub_text,sub_subreddit])\n",
    "    except HTTPError as e:\n",
    "        if e.code == 403:\n",
    "            print('Forbidden: private subreddit.')\n",
    "            \n",
    "def get_submission_comments(reddit_instance,subreddit,submission_id):\n",
    "    \n",
    "    submission = Submission(reddit_instance,id=submission_id)\n",
    "    \n",
    "    try:\n",
    "        submission.comments.replace_more(limit=0)\n",
    "        all_comments = submission.comments.list()\n",
    "\n",
    "        # Write header\n",
    "        with open(os.path.join('praw_output','post_comments','{}_COMMENTS.tsv'.format(subreddit)), 'w', newline='\\n') as csvfile:\n",
    "            csvwriter = csv.writer(csvfile, delimiter='\\t',\n",
    "                                    quotechar='|', quoting=csv.QUOTE_MINIMAL)\n",
    "            csvwriter.writerow(['submission_id','author','text','date','id','controversiality','num_downs','num_ups',\n",
    "                               'num_likes','score','subreddit'])\n",
    "\n",
    "        # Write body\n",
    "        for comment in all_comments:\n",
    "            sub_id = comment._submission.id\n",
    "            assert sub_id == submission_id\n",
    "            author_name = comment.author.name if comment.author is not None else -1\n",
    "            comment_body = comment.body.strip().replace('\\t','').replace('\\n','')\n",
    "            date_created = comment.created\n",
    "            comment_id = comment.id\n",
    "            controversiality = comment.controversiality\n",
    "            num_downs = comment.downs\n",
    "            num_ups = comment.ups\n",
    "            num_likes = comment.likes\n",
    "            score = comment.score\n",
    "            subreddit_name = comment.subreddit.display_name\n",
    "            #print(subreddit_name,subreddit)\n",
    "            assert subreddit_name == subreddit\n",
    "            \n",
    "            with open(os.path.join('praw_output','post_comments','{}_COMMENTS.tsv'.format(subreddit_name)), 'a', newline='\\n') as csvfile:\n",
    "                csvwriter = csv.writer(csvfile, delimiter='\\t',\n",
    "                                    quotechar='|', quoting=csv.QUOTE_MINIMAL)\n",
    "                csvwriter.writerow([sub_id,author_name,comment_body,date_created,comment_id,controversiality,\n",
    "                                   num_downs,num_ups,num_likes,score,subreddit_name])\n",
    "    except HTTPError as e:\n",
    "        if e.code == 403:\n",
    "            print('Forbidden: private subreddit.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "DISK_IO_DIR = \"/john11/scr1/yiweil\"\n",
    "if not os.path.exists(DISK_IO_DIR):\n",
    "    os.mkdir(DISK_IO_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load sets of keywords, subreddits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../keywords_high_precision.txt','r') as f:\n",
    "    KEYWORDS_HI_PREC = f.read().splitlines()\n",
    "KEYWORDS_SHORT = set([\"climate change\",\"global warming\",\"carbon\",\"co2\",\"methane\",\n",
    "                  \"green\",\"environment\",\"fossil fuel\"])\n",
    "#my_keywords = set(['climate change', 'global warming', 'fossil fuel', 'methane', 'carbon', 'co2'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in climate-related subreddits:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "SUBREDDITS = pd.read_csv('CLIMATE_SUBREDDITS.txt',sep='\\t',header=0)\n",
    "SUBREDDITS.stance.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SUBREDDITS.loc[SUBREDDITS.stance=='neut']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(72, 72)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(SUBREDDITS['subreddit'])),len(SUBREDDITS['subreddit'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "SUBREDDITS_LIST = list(SUBREDDITS['subreddit'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddit_name</th>\n",
       "      <th>initials</th>\n",
       "      <th>What is the subreddit about?</th>\n",
       "      <th>Unnamed: 3</th>\n",
       "      <th>Can you infer a general political leaning of the subreddit?</th>\n",
       "      <th>cc_stance</th>\n",
       "      <th>Unnamed: 6</th>\n",
       "      <th>What other impressions do you have about the subreddit?</th>\n",
       "      <th>Unnamed: 8</th>\n",
       "      <th>Unnamed: 9</th>\n",
       "      <th>Unnamed: 10</th>\n",
       "      <th>Other notes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>r/politics</td>\n",
       "      <td>yiwei</td>\n",
       "      <td>general political news (in the US?)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>pretty liberal--stuff defending AOC, in favor ...</td>\n",
       "      <td>p</td>\n",
       "      <td>NaN</td>\n",
       "      <td>a fair amount of sarcasm in the comment thread...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>posts mostly just links to news articles</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>r/worldnews</td>\n",
       "      <td>yiwei</td>\n",
       "      <td>world news excluding US events</td>\n",
       "      <td>NaN</td>\n",
       "      <td>possibly liberal--stuff about COVID is aligned...</td>\n",
       "      <td>p</td>\n",
       "      <td>NaN</td>\n",
       "      <td>some debate and opposing views in comment thre...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>r/AskReddit</td>\n",
       "      <td>yiwei</td>\n",
       "      <td>place to ask and answer \"thought-provoking\" qu...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>there are definitely liberals here (https://ww...</td>\n",
       "      <td>p</td>\n",
       "      <td>NaN</td>\n",
       "      <td>main purpose may be for entertainment/humor?</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>r/Futurology</td>\n",
       "      <td>yiwei</td>\n",
       "      <td>discussion about the future of humanity, civil...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>there is posting from liberal news outlets (th...</td>\n",
       "      <td>p</td>\n",
       "      <td>NaN</td>\n",
       "      <td>a lot of AI-related posts</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>r/news</td>\n",
       "      <td>yiwei</td>\n",
       "      <td>a lot of general news, primarily US but also t...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>pretty liberal I think, see https://www.reddit...</td>\n",
       "      <td>p</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  subreddit_name initials                       What is the subreddit about?  \\\n",
       "0     r/politics    yiwei                general political news (in the US?)   \n",
       "1    r/worldnews    yiwei                     world news excluding US events   \n",
       "2    r/AskReddit    yiwei  place to ask and answer \"thought-provoking\" qu...   \n",
       "3   r/Futurology    yiwei  discussion about the future of humanity, civil...   \n",
       "4         r/news    yiwei  a lot of general news, primarily US but also t...   \n",
       "\n",
       "   Unnamed: 3 Can you infer a general political leaning of the subreddit?  \\\n",
       "0         NaN  pretty liberal--stuff defending AOC, in favor ...            \n",
       "1         NaN  possibly liberal--stuff about COVID is aligned...            \n",
       "2         NaN  there are definitely liberals here (https://ww...            \n",
       "3         NaN  there is posting from liberal news outlets (th...            \n",
       "4         NaN  pretty liberal I think, see https://www.reddit...            \n",
       "\n",
       "  cc_stance  Unnamed: 6  \\\n",
       "0         p         NaN   \n",
       "1         p         NaN   \n",
       "2         p         NaN   \n",
       "3         p         NaN   \n",
       "4         p         NaN   \n",
       "\n",
       "  What other impressions do you have about the subreddit?  Unnamed: 8  \\\n",
       "0  a fair amount of sarcasm in the comment thread...              NaN   \n",
       "1  some debate and opposing views in comment thre...              NaN   \n",
       "2       main purpose may be for entertainment/humor?              NaN   \n",
       "3                          a lot of AI-related posts              NaN   \n",
       "4                                                NaN              NaN   \n",
       "\n",
       "   Unnamed: 9  Unnamed: 10                               Other notes  \n",
       "0         NaN          NaN  posts mostly just links to news articles  \n",
       "1         NaN          NaN                                       NaN  \n",
       "2         NaN          NaN                                       NaN  \n",
       "3         NaN          NaN                                       NaN  \n",
       "4         NaN          NaN                                       NaN  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_common_subs_df = pd.read_csv('most_common_subreddits.tsv',sep='\\t',header=0)\n",
    "most_common_subs_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['politics',\n",
       " 'worldnews',\n",
       " 'AskReddit',\n",
       " 'Futurology',\n",
       " 'news',\n",
       " 'science',\n",
       " 'The_Donald',\n",
       " 'todayilearned',\n",
       " 'collapse',\n",
       " 'canada',\n",
       " 'environment',\n",
       " 'pics',\n",
       " 'climateskeptics',\n",
       " 'explainlikeimfive',\n",
       " 'PlantedTank',\n",
       " 'Homebrewing',\n",
       " 'australia',\n",
       " 'conspiracy',\n",
       " 'europe',\n",
       " 'EcoInternet',\n",
       " 'TalkativePeople',\n",
       " 'climate ',\n",
       " 'askscience',\n",
       " 'AutoNewspaper',\n",
       " 'newsbotbot',\n",
       " 'Showerthoughts',\n",
       " 'POLITIC',\n",
       " 'energy',\n",
       " 'bicycling',\n",
       " 'climatechange',\n",
       " 'thinkpad',\n",
       " 'mechmarket',\n",
       " 'autotldr',\n",
       " 'spacex',\n",
       " 'Aquariums',\n",
       " 'Conservative',\n",
       " 'skeptic',\n",
       " 'space',\n",
       " 'technology',\n",
       " 'EvolveSustain',\n",
       " 'NoStupidQuestions',\n",
       " 'unpopularopinion',\n",
       " 'EverythingScience',\n",
       " 'BreakingNews24hr',\n",
       " 'changemyview',\n",
       " 'neoliberal',\n",
       " 'atheism',\n",
       " 'Libertarian',\n",
       " 'ChapoTrapHouse',\n",
       " 'IAmA',\n",
       " 'dataisbeautiful',\n",
       " 'PoliticalDiscussion',\n",
       " 'AdviceAnimals',\n",
       " 'PoliticalHumor',\n",
       " 'TrueReddit']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_common_subs = [x[2:] for x in most_common_subs_df.subreddit_name]\n",
    "most_common_subs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Create PRAW reddit instance to get posts and comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "reddit = praw.Reddit(client_id='1sbu376RCBiWRw',\n",
    "                     client_secret='NbqiHMPiKicBXvgfrID-xVNktZM',\n",
    "                     user_agent='mac:cc_framing:v1 (by /u/emma_cc_research)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_listing_use_sort': True,\n",
       " 'name': 'DragonFireDon',\n",
       " '_reddit': <praw.reddit.Reddit at 0x7f570b244320>,\n",
       " '_fetched': False}"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Can it know if a user is a bot?\n",
    "vars(reddit.redditor(\"DragonFireDon\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reddit.redditor(\"AutoModerator\").link_karma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_listing_use_sort': True,\n",
       " 'name': 'AutoModerator',\n",
       " '_reddit': <praw.reddit.Reddit at 0x7f570b244320>,\n",
       " '_fetched': False}"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reddit.redditor(\"AutoModerator\").__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Redditor(name='emma_cc_research')"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reddit.redditor(\"emma_cc_research\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Get posts from r/spambotwatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting submissions and comments from: spambotwatch\n"
     ]
    }
   ],
   "source": [
    "get_submissions(reddit,'spambotwatch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#get_submission_comments(reddit,'spambotwatch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0           JEEVAN BOBY (u/jeevanbobyvallickad) - Reddit\n",
       "1                                 overview for funnynova\n",
       "2      Spams Dating Tips Websites all with Stolen Con...\n",
       "3                                 overview for poopcake5\n",
       "4                                   overview for sutei_m\n",
       "                             ...                        \n",
       "190                               overview for royboy204\n",
       "191                                 overview for mlg7732\n",
       "192                            overview for 0wned_alover\n",
       "193                                 overview for xrox333\n",
       "194                                overview for frede933\n",
       "Name: title, Length: 195, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spambotwatch_df = pd.read_csv('praw_output/spambotwatch.tsv',sep='\\t',header=0)\n",
    "spambotwatch_df.title"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Get posts from all subreddits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting submissions and comments from: AskTrumpSupporters\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(SUBREDDITS['subreddit'])-1,len(SUBREDDITS['subreddit'])):\n",
    "    SUBREDDIT = SUBREDDITS_LIST[i]\n",
    "    get_submissions(reddit,SUBREDDIT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Inspect output: tsv of subreddits and meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>climateskeptics</td>\n",
       "      <td>Climate Skeptics: Trying to see through the al...</td>\n",
       "      <td>Seeing past hyperbole, alarmism and environmen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>skeptic</td>\n",
       "      <td>skeptic</td>\n",
       "      <td>## [Click this link to Read the Rules](http://...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>climatechange</td>\n",
       "      <td>A place for a rational discussion on a divisiv...</td>\n",
       "      <td>This is a place for the rational discussion of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>climate</td>\n",
       "      <td>Information about the world's climate</td>\n",
       "      <td>Real and accurate data about the Earth's clima...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>science</td>\n",
       "      <td>Reddit Science</td>\n",
       "      <td># [Submission Rules](https://www.reddit.com/r/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>republicans</td>\n",
       "      <td>Republicans - RNC - GOP: Grand Old Party</td>\n",
       "      <td>Republican, RNC and GOP news, issues, gossip, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>askaconservative</td>\n",
       "      <td>Ask A Conservative: Ask Conservatives And Repu...</td>\n",
       "      <td>#[Ask a Conservative](/r/askaconservative)\\n\\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>Conservative</td>\n",
       "      <td>Conservative</td>\n",
       "      <td>#####\\n**[Join us on discord.](https://discord...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>conservatives</td>\n",
       "      <td>conservatives</td>\n",
       "      <td>Conservatism (from, conservare, \"to preserve\")...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>AskTrumpSupporters</td>\n",
       "      <td>AskTrumpSupporters</td>\n",
       "      <td>#We are not a typical subreddit. Read the [ful...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>68 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     0                                                  1  \\\n",
       "0      climateskeptics  Climate Skeptics: Trying to see through the al...   \n",
       "1              skeptic                                            skeptic   \n",
       "2        climatechange  A place for a rational discussion on a divisiv...   \n",
       "3              climate              Information about the world's climate   \n",
       "4              science                                     Reddit Science   \n",
       "..                 ...                                                ...   \n",
       "63         republicans           Republicans - RNC - GOP: Grand Old Party   \n",
       "64    askaconservative  Ask A Conservative: Ask Conservatives And Repu...   \n",
       "65        Conservative                                       Conservative   \n",
       "66       conservatives                                      conservatives   \n",
       "67  AskTrumpSupporters                                 AskTrumpSupporters   \n",
       "\n",
       "                                                    2  \n",
       "0   Seeing past hyperbole, alarmism and environmen...  \n",
       "1   ## [Click this link to Read the Rules](http://...  \n",
       "2   This is a place for the rational discussion of...  \n",
       "3   Real and accurate data about the Earth's clima...  \n",
       "4   # [Submission Rules](https://www.reddit.com/r/...  \n",
       "..                                                ...  \n",
       "63  Republican, RNC and GOP news, issues, gossip, ...  \n",
       "64  #[Ask a Conservative](/r/askaconservative)\\n\\n...  \n",
       "65  #####\\n**[Join us on discord.](https://discord...  \n",
       "66  Conservatism (from, conservare, \"to preserve\")...  \n",
       "67  #We are not a typical subreddit. Read the [ful...  \n",
       "\n",
       "[68 rows x 3 columns]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv('subreddits.tsv',sep='\\t',header=None).drop_duplicates(0,keep='first')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Inspect output: tsv of one subreddit's posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('praw_output/350.tsv',sep='\\t',header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['title', 'author', 'date', 'is_video', 'id', 'num_downs', 'num_ups',\n",
       "       'upvote_ratio', 'num_comments', 'score', 'text', 'subreddit'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    127\n",
       "Name: is_video, dtype: int64"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.is_video.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "350    127\n",
       "Name: subreddit, dtype: int64"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.subreddit.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#df.head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Get comments for all posts with non-zero num comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "code_folding": [
     1
    ],
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already got comments for subreddit praw_output/posts/350.tsv\n",
      "Already got comments for subreddit praw_output/posts/350ppm.tsv\n",
      "Already got comments for subreddit praw_output/posts/askaconservative.tsv\n",
      "Already got comments for subreddit praw_output/posts/AskTrumpSupporters.tsv\n",
      "Already got comments for subreddit praw_output/posts/carboncapture.tsv\n",
      "Already got comments for subreddit praw_output/posts/carbontax.tsv\n",
      "Already got comments for subreddit praw_output/posts/ccfunding.tsv\n",
      "Already got comments for subreddit praw_output/posts/climate.tsv\n",
      "Already got comments for subreddit praw_output/posts/climate_activism.tsv\n",
      "Already got comments for subreddit praw_output/posts/climate_discussion.tsv\n",
      "Already got comments for subreddit praw_output/posts/climate_science.tsv\n",
      "Already got comments for subreddit praw_output/posts/ClimateActionPlan.tsv\n",
      "Already got comments for subreddit praw_output/posts/climatechange.tsv\n",
      "Already got comments for subreddit praw_output/posts/ClimateChangeCancer.tsv\n",
      "Already got comments for subreddit praw_output/posts/climatecmv.tsv\n",
      "Already got comments for subreddit praw_output/posts/ClimateCrisis.tsv\n",
      "Already got comments for subreddit praw_output/posts/climatedebate.tsv\n",
      "Already got comments for subreddit praw_output/posts/climatedebate2.tsv\n",
      "Already got comments for subreddit praw_output/posts/climategate.tsv\n",
      "0 comments among all posts in subreddit: ClimateMobilization\n",
      "Already got comments for subreddit praw_output/posts/ClimateOffensive.tsv\n",
      "Already got comments for subreddit praw_output/posts/climatepredicts.tsv\n",
      "Already got comments for subreddit praw_output/posts/ClimatePreparation.tsv\n",
      "Already got comments for subreddit praw_output/posts/climatesecurity.tsv\n",
      "Already got comments for subreddit praw_output/posts/climateskeptics.tsv\n",
      "Already got comments for subreddit praw_output/posts/ClimateSkepticScience.tsv\n",
      "Already got comments for subreddit praw_output/posts/ClimateSplattergate.tsv\n",
      "0 comments among all posts in subreddit: climatestasis\n",
      "Already got comments for subreddit praw_output/posts/Climatology.tsv\n",
      "Already got comments for subreddit praw_output/posts/climatskeptics.tsv\n",
      "Already got comments for subreddit praw_output/posts/CollapseSurvival.tsv\n",
      "Already got comments for subreddit praw_output/posts/conservation.tsv\n",
      "Already got comments for subreddit praw_output/posts/Conservative.tsv\n",
      "Already got comments for subreddit praw_output/posts/conservatives.tsv\n",
      "Subreddit praw_output/posts/DebateAClimateSkeptic.tsv has no posts\n",
      "Getting comments from posts in subreddit: deep_ecology\n",
      "Getting comments from posts in subreddit: drought\n",
      "Getting comments from posts in subreddit: EarthDisaster\n",
      "Getting comments from posts in subreddit: EarthScience\n",
      "Getting comments from posts in subreddit: EcoInternet\n",
      "Getting comments from posts in subreddit: ecology\n",
      "Subreddit praw_output/posts/ecologycenter.tsv has no posts\n",
      "Getting comments from posts in subreddit: EcoRestoration\n",
      "Getting comments from posts in subreddit: energy\n",
      "Getting comments from posts in subreddit: enviroaction\n",
      "Getting comments from posts in subreddit: Enviroment\n",
      "Getting comments from posts in subreddit: environment\n",
      "Getting comments from posts in subreddit: environmental_science\n",
      "Subreddit praw_output/posts/Global_Warming.tsv has no posts\n",
      "Getting comments from posts in subreddit: GlobalClimateChange\n",
      "Getting comments from posts in subreddit: GlobalWarming\n",
      "Getting comments from posts in subreddit: GlobalWarmingisBunk\n",
      "Getting comments from posts in subreddit: Green\n",
      "Getting comments from posts in subreddit: GreenNewIdeas\n",
      "0 comments among all posts in subreddit: GreenPlanet\n",
      "0 comments among all posts in subreddit: GWB\n",
      "Getting comments from posts in subreddit: Rainforest\n",
      "Subreddit praw_output/posts/RealClimateSkeptics.tsv has no posts\n",
      "Getting comments from posts in subreddit: Republican\n",
      "Getting comments from posts in subreddit: republicans\n",
      "Getting comments from posts in subreddit: Restoration_Ecology\n",
      "Getting comments from posts in subreddit: science\n",
      "Getting comments from posts in subreddit: skeptic\n",
      "Getting comments from posts in subreddit: sustainability\n",
      "Getting comments from posts in subreddit: Sustainable_Energy\n",
      "Subreddit praw_output/posts/ThunbergSyndrome.tsv has no posts\n",
      "Getting comments from posts in subreddit: trueclimateskeptics\n",
      "0 comments among all posts in subreddit: WorldClimate\n"
     ]
    }
   ],
   "source": [
    "for subreddit_tsv in glob.glob('praw_output/posts/*.tsv'):\n",
    "    if os.path.exists('praw_output/post_comments/{}_COMMENTS.tsv'.format(subreddit_tsv.split('/')[-1][:-4])):\n",
    "        print('Already got comments for subreddit {}'.format(subreddit_tsv))\n",
    "    else:\n",
    "        subreddit_posts = pd.read_csv(subreddit_tsv,sep='\\t',header=0)\n",
    "        if len(subreddit_posts) > 0:\n",
    "            subreddit = str(subreddit_posts.iloc[0]['subreddit'])\n",
    "            posts_with_comments = subreddit_posts.loc[subreddit_posts.num_comments > 0]\n",
    "            if len(posts_with_comments) > 0:\n",
    "                print('Getting comments from posts in subreddit: {}'.format(subreddit))\n",
    "                for ix,row in posts_with_comments.iterrows():\n",
    "                    get_submission_comments(reddit,subreddit,row['id'])\n",
    "            else:\n",
    "                print('0 comments among all posts in subreddit: {}'.format(subreddit))\n",
    "        else:\n",
    "            print('Subreddit {} has no posts'.format(subreddit_tsv))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use Pushshift API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "code_folding": [
     0,
     10,
     18,
     74
    ]
   },
   "outputs": [],
   "source": [
    "def getPushshiftData(query, after, before, datatype):\n",
    "    query_prefix = 'title' if datatype == 'submission' else 'q'\n",
    "    url = 'https://api.pushshift.io/reddit/search/'+datatype+'/?'+\\\n",
    "    query_prefix+'='+str(query)+'&size=1000&after='+str(after)+'&before='+str(before)\n",
    "    #print(url)\n",
    "    r = requests.get(url)\n",
    "    data = json.loads(r.text)\n",
    "    return data['data']\n",
    "\n",
    "\n",
    "def getPushshiftSubreddit(subreddit, after, before, datatype):\n",
    "    url = 'https://api.pushshift.io/reddit/search/{}/?subreddit={}&size=1000&after={}&before={}'.format(\n",
    "        datatype,subreddit,after,before)\n",
    "    r = requests.get(url)\n",
    "    data = json.loads(r.text)\n",
    "    return data['data']\n",
    "\n",
    "\n",
    "def collectSubData(subm,subs_dict):\n",
    "    try:\n",
    "        title = subm['title']\n",
    "    except KeyError:\n",
    "        title = None\n",
    "    try:\n",
    "        url = subm['url']\n",
    "    except KeyError:\n",
    "        url = None\n",
    "    try:\n",
    "        flair = subm['link_flair_text']\n",
    "    except KeyError:\n",
    "        flair = \"NaN\"   \n",
    "    try:\n",
    "        author = subm['author']\n",
    "    except KeyError:\n",
    "        author = None\n",
    "    sub_id = subm['id']\n",
    "    try:\n",
    "        score = subm['score']\n",
    "    except KeyError:\n",
    "        score = None\n",
    "    try:\n",
    "        created = datetime.datetime.fromtimestamp(subm['created_utc']) #1520561700.0\n",
    "    except KeyError:\n",
    "        created = None\n",
    "    try:\n",
    "        numComms = subm['num_comments']\n",
    "    except KeyError:\n",
    "        numComms = None\n",
    "    try:\n",
    "        permalink = subm['permalink']\n",
    "    except KeyError:\n",
    "        permalink = None\n",
    "    try:\n",
    "        is_vid = subm['is_video']\n",
    "    except KeyError:\n",
    "        is_vid = None\n",
    "    try:\n",
    "        upvote_ratio = subm['upvote_ratio']\n",
    "    except KeyError:\n",
    "        upvote_ratio = None\n",
    "    try:\n",
    "        text = subm['selftext'].strip().replace('\\t','').replace('\\n','')\n",
    "    except KeyError:\n",
    "        text = \"\"\n",
    "    try:\n",
    "        subreddit = subm['subreddit']\n",
    "    except KeyError:\n",
    "        subreddit = None\n",
    "    subData = {'id':sub_id,'title':title,'url':url,'author':author,'score':score,'date':created,\n",
    "                    'num_comments':numComms,'permalink':permalink,'flair':flair,'is_video':is_vid,\n",
    "                    'upvote_ratio':upvote_ratio,'text':text,'subreddit':subreddit}\n",
    "    subs_dict[sub_id] = subData\n",
    "    \n",
    "    \n",
    "def collectCommData(subm,subs_dict): \n",
    "    try:\n",
    "        author = subm['author']\n",
    "    except KeyError:\n",
    "        author = None\n",
    "    sub_id = subm['id']\n",
    "    try:\n",
    "        link_id = subm['link_id']\n",
    "    except KeyError:\n",
    "        link_id = None\n",
    "    try:\n",
    "        score = subm['score']\n",
    "    except KeyError:\n",
    "        score = None\n",
    "    try:\n",
    "        created = datetime.datetime.fromtimestamp(subm['created_utc']) #1520561700.0\n",
    "    except KeyError:\n",
    "        created = None\n",
    "    try:\n",
    "        permalink = subm['permalink']\n",
    "    except KeyError:\n",
    "        permalink = None\n",
    "    try:\n",
    "        text = subm['body'].strip().replace('\\t','').replace('\\n','')\n",
    "    except KeyError:\n",
    "        text = \"\"\n",
    "    try:\n",
    "        subreddit = subm['subreddit']\n",
    "    except KeyError:\n",
    "        subreddit = None\n",
    "    subData = {'id':sub_id,'link_id':link_id,'author':author,'score':score,'date':created,\n",
    "                    'permalink':permalink,'text':text,'subreddit':subreddit}\n",
    "    subs_dict[sub_id] = subData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'global warming|climate change|carbon dioxide|co2|methane|fossil fuel|climate crisis|climate emergency|extreme weather|clean energy|renewable energy|cap and trade|sea level rise|IPCC|deforestation|permafrost|greenhouse gas|greenhouse effect|green new deal|environmentalism|EPA'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multiple_search_term = '|'.join(KEYWORDS_HI_PREC)\n",
    "multiple_search_term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "before_date = datetime.datetime.strptime(\"07-22-2020\", \"%m-%d-%Y\")\n",
    "after_date = datetime.datetime.strptime(\"07-20-2020\", \"%m-%d-%Y\")\n",
    "before_timestamp = int(datetime.datetime.timestamp(before_date))\n",
    "after_timestamp = int(datetime.datetime.timestamp(after_date))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = getPushshiftData(multiple_search_term,after_timestamp,before_timestamp,\"comment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = getPushshiftData(multiple_search_term,after_timestamp,before_timestamp,\"submission\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#test_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def pushshift_wrapper(after_str, before_str, datatype, query=None, keywords=None):\n",
    "    failed_requests = []\n",
    "    \n",
    "    if query == 'ALL':\n",
    "        query = '|'.join(keywords)\n",
    "    \n",
    "    subCount = 0\n",
    "    subStats = {}\n",
    "    before_date = datetime.datetime.strptime(before_str, \"%m-%d-%Y\")\n",
    "    after_date = datetime.datetime.strptime(after_str, \"%m-%d-%Y\")\n",
    "    before_timestamp = int(datetime.datetime.timestamp(before_date))\n",
    "    after_timestamp = int(datetime.datetime.timestamp(after_date))\n",
    "#     print(\"Getting all submissions with query '{}' from {} to {}\".format(query,\n",
    "#                                                                          after_str,before_str))\n",
    "    print(\"Getting all {} from subreddit '{}' from {} to {}\".format(datatype,query,\n",
    "                                                                         after_str,before_str))\n",
    "    try:\n",
    "        #data = getPushshiftData(query, after_timestamp, before_timestamp, datatype)\n",
    "        data = getPushshiftSubreddit(query, after_timestamp, before_timestamp, datatype)\n",
    "        # Will run until all posts have been gathered \n",
    "        # from the 'after' date up until before date\n",
    "        while (len(data) > 0) and (after_timestamp < 1546345000):\n",
    "            for submission in data:\n",
    "                if datatype == \"submission\":\n",
    "                    collectSubData(submission,subStats)\n",
    "                else:\n",
    "                    collectCommData(submission,subStats)\n",
    "                subCount+=1\n",
    "            # Calls getPushshiftData() with the created date of the last submission\n",
    "            #print(len(data))\n",
    "            #print(str(datetime.datetime.fromtimestamp(data[-1]['created_utc'])))\n",
    "            after_timestamp = data[-1]['created_utc']\n",
    "            print(after_timestamp)\n",
    "            try:\n",
    "                #data = getPushshiftData(query, after_timestamp, before_timestamp, datatype)\n",
    "                data = getPushshiftSubreddit(query, after_timestamp, before_timestamp, datatype)\n",
    "            except JSONDecodeError:\n",
    "                failed_requests.append((query,after_timestamp,before_timestamp,datatype))\n",
    "\n",
    "        print('Num submissions:',subCount,len(subStats))\n",
    "\n",
    "        interim_df = pd.DataFrame(list(subStats.values()))\n",
    "        #print(interim_df)\n",
    "\n",
    "        datatype_prefix = 'posts' if datatype == 'submission' else 'post_comments'\n",
    "        out_dir = os.path.join('pushshift_output','pushshift_output_background',datatype_prefix,'{}_to_{}'.format(after_str,before_str))\n",
    "        failed_reqs_out_dir = os.path.join('pushshift_output','pushshift_output_background','failed_requests',\n",
    "                                                      '{}_{}'.format(after_str,before_str))\n",
    "        if not os.path.exists(out_dir):\n",
    "            os.makedirs(out_dir)\n",
    "        if not os.path.exists(failed_reqs_out_dir):\n",
    "            os.makedirs(failed_reqs_out_dir)\n",
    "            \n",
    "        if True:#keywords is None:\n",
    "            interim_df.to_pickle(os.path.join(out_dir,'{}.pkl'.format(query)))\n",
    "            print('Saved subreddit posts to {}!'.format(os.path.join(out_dir,'{}.pkl'.format(query))))\n",
    "            pickle.dump(failed_requests,open(os.path.join(failed_reqs_out_dir,'{}.pkl'.format(query)),'wb'))\n",
    "        else:\n",
    "            interim_df.to_pickle(os.path.join(out_dir,'{}.pkl'.format('keywords_long')))\n",
    "            print('Saved subreddit posts to {}!'.format(os.path.join(out_dir,'{}.pkl'.format('keywords_long'))))\n",
    "            pickle.dump(failed_requests,open(os.path.join(failed_reqs_out_dir,'{}.pkl'.format('keywords_long')),'wb'))\n",
    "            \n",
    "    except JSONDecodeError:\n",
    "        failed_requests.append((query,after_timestamp,before_timestamp,datatype))\n",
    "        print(\"First request failed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "55"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(most_common_subs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#most_common_subs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for start_year in range(2019,2021,1):\n",
    "    for sub in most_common_subs:\n",
    "        if not os.path.exists(os.path.join('pushshift_output','pushshift_output_background','post_comments',\n",
    "                                           '1-1-{}_to_12-31-{}'.format(start_year,start_year),\n",
    "                                           '{}.pkl'.format(sub))):\n",
    "            print(\"Missing {}, {}\".format(sub,start_year))\n",
    "            pushshift_wrapper('1-1-{}'.format(start_year),'12-31-{}'.format(start_year),'comment',\n",
    "            query=sub)\n",
    "# print('************')    \n",
    "# for start_year in range(2010,2015,1):\n",
    "#     end_year = start_year+1\n",
    "#     #for keyword in KEYWORDS_LONG:\n",
    "#     for sub in reversed(most_common_subs):\n",
    "#         if not os.path.exists(os.path.join('output','pushshift_output_background','post_comments',\n",
    "#                                            '1-1-{}_to_12-31-{}'.format(start_year,start_year),\n",
    "#                                            '{}.pkl'.format(sub))):\n",
    "#             print(\"Missing {}, {}\".format(sub,start_year))\n",
    "#             #pushshift_wrapper('1-1-{}'.format(start_year),'12-31-{}'.format(start_year),'comment',query=sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ehzvrv'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Aggregate into one big df\n",
    "df_ = pd.read_pickle(\n",
    "    os.path.join('pushshift_output','pushshift_output_background','post_comments',\n",
    "                 '1-1-2019_to_12-31-2019','politics.pkl'))\n",
    "columns_ = df_.columns\n",
    "df = pd.DataFrame(columns=list(columns_))\n",
    "\n",
    "for df_fname in glob.glob(os.path.join('pushshift_output','pushshift_output_background',\n",
    "                                       'post_comments','1-1-2019_to_12-31-2019','*.pkl')):\n",
    "    df_ = pd.read_pickle(df_fname)\n",
    "    df = pd.concat([df,df_],ignore_index=True,axis=0)\n",
    "        \n",
    "# Deduplicate by ID \n",
    "print('Size of df, pre-deduplication:',len(df))\n",
    "df.drop_duplicates(subset='id',inplace=True)\n",
    "print('Size of df, post-deduplication:',len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_io.TextIOWrapper name='/john11/scr1/yiweil/pushshift_output/pushshift_output_background/spacy_processed/ck6q4fg.json' mode='w' encoding='UTF-8'>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'False'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "row['id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    73420\n",
       "True         3\n",
       "Name: id, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['id'].apply(lambda x: x == 'False').value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/john11/scr1/yiweil/pushshift_output/pushshift_output_background/spacy_processed/False.json'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_dir+'/{}.json'.format(p_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving spaCy output to: /john11/scr1/yiweil/pushshift_output/pushshift_output_background/spacy_processed\n",
      "Already processed 5546 items.\n",
      "0\n",
      "500\n",
      "1000\n",
      "1500\n",
      "2000\n",
      "2500\n",
      "3000\n",
      "3500\n",
      "4000\n",
      "4500\n",
      "5000\n",
      "5500\n",
      "6000\n",
      "6500\n",
      "7000\n",
      "7500\n",
      "8000\n",
      "8500\n",
      "9000\n",
      "9500\n",
      "10000\n",
      "10500\n",
      "11000\n",
      "11500\n",
      "12000\n",
      "12500\n",
      "13000\n",
      "13500\n",
      "14000\n",
      "14500\n",
      "15000\n",
      "15500\n",
      "16000\n",
      "16500\n",
      "17000\n",
      "17500\n",
      "18000\n",
      "18500\n",
      "19000\n",
      "19500\n",
      "20000\n",
      "20500\n",
      "21000\n",
      "21500\n",
      "22000\n",
      "22500\n",
      "23000\n",
      "23500\n",
      "24000\n",
      "24500\n",
      "25000\n",
      "25500\n",
      "26000\n",
      "26500\n",
      "27000\n",
      "27500\n",
      "28000\n",
      "28500\n",
      "29000\n",
      "29500\n",
      "30000\n",
      "30500\n",
      "31000\n",
      "31500\n",
      "32000\n",
      "32500\n",
      "33000\n",
      "33500\n",
      "34000\n",
      "34500\n",
      "35000\n",
      "35500\n",
      "36000\n",
      "36500\n",
      "37000\n",
      "37500\n",
      "38000\n",
      "38500\n",
      "39000\n",
      "39500\n",
      "40000\n",
      "40500\n",
      "41000\n",
      "41500\n",
      "42000\n",
      "42500\n",
      "43000\n",
      "43500\n",
      "44000\n",
      "44500\n",
      "45000\n",
      "45500\n",
      "46000\n",
      "46500\n",
      "47000\n",
      "47500\n",
      "48000\n",
      "48500\n",
      "49000\n",
      "49500\n",
      "50000\n",
      "50500\n",
      "51000\n",
      "51500\n",
      "52000\n",
      "52500\n",
      "53000\n",
      "53500\n",
      "54000\n",
      "54500\n",
      "55000\n",
      "55500\n",
      "56000\n",
      "56500\n",
      "57000\n",
      "57500\n",
      "58000\n",
      "58500\n",
      "59000\n",
      "59500\n",
      "60000\n",
      "60500\n",
      "61000\n",
      "61500\n",
      "62000\n",
      "62500\n",
      "63000\n",
      "63500\n",
      "64000\n",
      "64500\n",
      "65000\n",
      "65500\n",
      "66000\n",
      "66500\n",
      "67000\n",
      "67500\n",
      "68000\n",
      "68500\n",
      "69000\n",
      "69500\n",
      "70000\n",
      "70500\n",
      "71000\n",
      "71500\n",
      "72000\n",
      "72500\n",
      "73000\n"
     ]
    }
   ],
   "source": [
    "# # Aggregate into one big df\n",
    "# df = pd.read_csv('comments_output/changemyview_background/from_posts_1-1-2010_to_9-28-2021_combined.csv')\n",
    "\n",
    "# # Process w/ spaCy\n",
    "# import spacy\n",
    "# nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# save each post's output as \n",
    "# `'pushshift_output/pushshift_output_background/spacy_processed/{}.json'.format(post_id)`\n",
    "save_dir = os.path.join(DISK_IO_DIR,'pushshift_output','pushshift_output_background','spacy_processed')\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)   \n",
    "print(\"Saving spaCy output to: {}\".format(save_dir))\n",
    "                 \n",
    "processed = set(glob.glob(os.path.join(save_dir,'*.json')))\n",
    "print('Already processed {} items.'.format(len(processed)))\n",
    "for ix_row,row in df.iterrows():\n",
    "    p_id = row['id']\n",
    "    save_file = '{}.json'.format(p_id)\n",
    "    if p_id != 'False' and os.path.join(save_dir,save_file) not in processed:\n",
    "        text = row['body']\n",
    "        if type(text) == str and len(text) > 0:\n",
    "            json_out = defaultdict(list)\n",
    "            doc = nlp(text)\n",
    "            for token in doc:\n",
    "                json_out['token'].append(token.text)\n",
    "                json_out['lemma'].append(token.lemma_)\n",
    "                json_out['pos'].append(token.pos_)\n",
    "                json_out['dep'].append(token.dep_)\n",
    "                json_out['head'].append(token.head.text)\n",
    "                json_out['children'].append([child.text for child in token.children])\n",
    "\n",
    "            with open(os.path.join(save_dir,save_file), 'w') as outfile:\n",
    "                json.dump(json_out, outfile)\n",
    "            \n",
    "    if ix_row % 500 == 0:\n",
    "        print(ix_row)\n",
    "\n",
    "# load lemmas into lists; get Counters, etc. for LOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# for start_year in range(2015,2021,1):\n",
    "#     end_year = start_year+1\n",
    "#     for sub in reversed(most_common_subs):\n",
    "#         if not os.path.exists(os.path.join('output','pushshift_output_background','post_comments',\n",
    "#                                            '1-1-{}_to_12-31-{}'.format(start_year,start_year),\n",
    "#                                            '{}.pkl'.format(sub))):\n",
    "#             print(\"Missing {}, {}\".format(sub,start_year))\n",
    "#             #pushshift_wrapper('1-1-{}'.format(start_year),'12-31-{}'.format(start_year),'comment',query=sub)\n",
    "#             pushshift_wrapper('1-1-{}'.format(start_year),'12-31-{}'.format(start_year),'submission',query=sub)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Get all comments attached to a post"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## PRAW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Get IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "419100"
      ]
     },
     "execution_count": 410,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "posts_with_comments_ids = pickle.load(open('output/posts_with_comments_ids.pkl','rb'))\n",
    "print(len(posts_with_comments_ids))\n",
    "sub_ids_to_fetch = list(posts_with_comments_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# with open('output/comment_ids_per_post.tsv','w') as f:\n",
    "#     f.write(\"{}\\t{}\\n\".format('post_id','comment_ids'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def praw_get_comments(sub_id):\n",
    "    try:\n",
    "        post = reddit.submission(id=sub_id)\n",
    "        post_author = post.author\n",
    "        post_title = post.title\n",
    "        post_comms = list(post.__dict__['_comments_by_id'].keys())\n",
    "        #print(len(post_comms))\n",
    "        comments_per_post[sub_id] = post_comms\n",
    "\n",
    "        with open('output/comment_ids_per_post.tsv','a') as f:\n",
    "            f.write(\"{}\\t{}\\n\".format(sub_id,','.join(post_comms)))\n",
    "    except Forbidden:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12000\n",
      "13000\n",
      "14000\n",
      "15000\n",
      "16000\n",
      "17000\n",
      "18000\n",
      "19000\n",
      "20000\n",
      "21000\n",
      "22000\n",
      "23000\n",
      "24000\n",
      "25000\n",
      "26000\n"
     ]
    }
   ],
   "source": [
    "for ix_sub_id in range(11950,len(sub_ids_to_fetch)):\n",
    "    sub_id = sub_ids_to_fetch[ix_sub_id]\n",
    "    praw_get_comments(sub_id)\n",
    "    \n",
    "    if ix_sub_id % 1000 == 0:\n",
    "        print(ix_sub_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11950, '4q96ti')"
      ]
     },
     "execution_count": 452,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ix_sub_id,sub_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11720"
      ]
     },
     "execution_count": 447,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(comments_per_post)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Get text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "comment_ids_per_post = pd.read_csv('output/comment_ids_per_post.tsv',sep='\\t')\n",
    "comment_ids_per_post = comment_ids_per_post.loc[comment_ids_per_post.comment_ids.apply(lambda x: \n",
    "                                                                                      isinstance(x,str))]\n",
    "comment_ids_per_post = comment_ids_per_post.loc[comment_ids_per_post.comment_ids.apply(lambda x: \n",
    "                                                                                      len(x) > 0)]\n",
    "comment_ids = comment_ids_per_post['comment_ids']\n",
    "all_comment_ids = [x.split(',') for x in comment_ids]\n",
    "all_comment_ids = [item for sublist in all_comment_ids for item in sublist]\n",
    "unique_comment_ids = set(all_comment_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "393227 393227\n"
     ]
    }
   ],
   "source": [
    "print(len(all_comment_ids),len(unique_comment_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Dataframe of \n",
    "# comment_id | text\n",
    "\n",
    "with open('output/text_per_comment.tsv','w') as f:\n",
    "    f.write('{}\\t{}\\n'.format('comment_id','text'))\n",
    "    \n",
    "unique_comment_ids = list(unique_comment_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "112577"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c_id_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113000\n",
      "114000\n",
      "115000\n",
      "116000\n",
      "117000\n",
      "118000\n",
      "119000\n",
      "120000\n",
      "121000\n",
      "122000\n",
      "123000\n",
      "124000\n",
      "125000\n",
      "126000\n",
      "127000\n",
      "128000\n",
      "129000\n",
      "130000\n",
      "131000\n",
      "132000\n",
      "133000\n",
      "134000\n",
      "135000\n",
      "136000\n",
      "137000\n",
      "138000\n",
      "139000\n",
      "140000\n",
      "141000\n",
      "142000\n",
      "143000\n",
      "144000\n",
      "145000\n",
      "146000\n",
      "147000\n",
      "148000\n",
      "149000\n",
      "150000\n",
      "151000\n",
      "152000\n",
      "153000\n",
      "154000\n",
      "155000\n",
      "156000\n",
      "157000\n",
      "158000\n",
      "159000\n",
      "160000\n",
      "161000\n",
      "162000\n",
      "163000\n",
      "164000\n",
      "165000\n",
      "166000\n",
      "167000\n",
      "168000\n",
      "169000\n",
      "170000\n",
      "171000\n",
      "172000\n",
      "173000\n",
      "174000\n"
     ]
    }
   ],
   "source": [
    "for c_id_index in range(112577,len(unique_comment_ids)):\n",
    "    \n",
    "    c_id = unique_comment_ids[c_id_index].split('_')[-1]\n",
    "    comm = reddit.comment(c_id)\n",
    "    try:\n",
    "        comment_body = strip_whitespace(comm.body)\n",
    "\n",
    "        with open('output/text_per_comment.tsv','a') as f:\n",
    "            f.write('{}\\t{}\\n'.format(c_id,comment_body))\n",
    "    except ClientException:\n",
    "        pass\n",
    "\n",
    "    if c_id_index % 1000 == 0:\n",
    "        print(c_id_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_df = pd.read_csv('output/text_per_comment.tsv',sep='\\t',header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<class 'str'>    100\n",
       "Name: comment_id, dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.comment_id.apply(lambda x: type(x)).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7    100\n",
       "Name: comment_id, dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.comment_id.apply(lambda x: len(x)).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<class 'str'>    100\n",
       "Name: text, dtype: int64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.text.apply(lambda x: type(x)).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Pushshift -- doesn't seem to work >:("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def getComments(sub_id):\n",
    "    url = 'https://api.pushshift.io/reddit/submission/comment_ids/{}'.format(sub_id)\n",
    "    r = requests.get(url)\n",
    "    sub_comm_data = json.loads(r.text)\n",
    "    return sub_comm_data['data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def pushshift_comment_wrapper(link_id, after_str, before_str, datatype):\n",
    "    failed_requests = []\n",
    "    \n",
    "    subCount = 0\n",
    "    subStats = {}\n",
    "    before_date = datetime.datetime.strptime(before_str, \"%m-%d-%Y\")\n",
    "    after_date = datetime.datetime.strptime(after_str, \"%m-%d-%Y\")\n",
    "    before_timestamp = int(datetime.datetime.timestamp(before_date))\n",
    "    after_timestamp = int(datetime.datetime.timestamp(after_date))\n",
    "    print(\"Getting all comments with link_id '{}' from {} to {}\".format(link_id,after_str,before_str))\n",
    "    try:\n",
    "        data = getComments(link_id, after_timestamp, before_timestamp, datatype)\n",
    "        # Will run until all posts have been gathered \n",
    "        # from the 'after' date up until before date\n",
    "        while len(data) > 0:\n",
    "            for submission in data:\n",
    "                if datatype == \"submission\":\n",
    "                    collectSubData(submission,subStats)\n",
    "                else:\n",
    "                    collectCommData(submission,subStats)\n",
    "                subCount+=1\n",
    "            # Calls getPushshiftData() with the created date of the last submission\n",
    "            #print(len(data))\n",
    "            #print(str(datetime.datetime.fromtimestamp(data[-1]['created_utc'])))\n",
    "            after_timestamp = data[-1]['created_utc']\n",
    "            #print(after_timestamp)\n",
    "            try:\n",
    "                data = getComments(link_id, after_timestamp, before_timestamp, datatype)\n",
    "            except JSONDecodeError:\n",
    "                failed_requests.append((link_id,after_timestamp,before_timestamp,datatype))\n",
    "\n",
    "        print('Num submissions:',subCount,len(subStats))\n",
    "\n",
    "        interim_df = pd.DataFrame(list(subStats.values()))\n",
    "        #print(interim_df)\n",
    "\n",
    "        datatype_prefix = 'posts' if datatype == 'submission' else 'linked_post_comments'\n",
    "        out_dir = os.path.join('output','pushshift_output',datatype_prefix,'{}_to_{}'.format(after_str,before_str))\n",
    "        failed_reqs_out_dir = os.path.join('pushshift_output','failed_requests',\n",
    "                                                      '{}_{}'.format(after_str,before_str))\n",
    "        if not os.path.exists(out_dir):\n",
    "            os.mkdir(out_dir)\n",
    "        if not os.path.exists(failed_reqs_out_dir):\n",
    "            os.mkdir(failed_reqs_out_dir)\n",
    "            \n",
    "        interim_df.to_pickle(os.path.join(out_dir,'{}.pkl'.format(link_id)))\n",
    "        print('Saved query submissions to {}!'.format(os.path.join(out_dir,'{}.pkl'.format(link_id))))\n",
    "        pickle.dump(failed_requests,open(os.path.join(failed_reqs_out_dir,'{}.pkl'.format(link_id)),'wb'))\n",
    "            \n",
    "    except JSONDecodeError:\n",
    "        failed_requests.append((query,after_timestamp,before_timestamp,datatype))\n",
    "        print(\"First request failed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 390,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getComments(sub_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "url = 'https://api.pushshift.io/reddit/submission/search/?ids={}&limit=1000'.format(sub_id)\n",
    "r = requests.get(url)\n",
    "sub_data = json.loads(r.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 392,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sub_data['data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#sub_data['data'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "url = 'https://api.pushshift.io/reddit/comment/search/?id=43u5cw'\n",
    "r = requests.get(url)\n",
    "comm_data = json.loads(r.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#comm_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "url = 'https://api.pushshift.io/reddit/submission/comment_ids/{}'.format(sub_id)\n",
    "r = requests.get(url)\n",
    "sub_comm_data = json.loads(r.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 396,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub_comm_data['data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "url = 'https://api.pushshift.io/reddit/comment/search/?link_id={}'.format(sub_id)\n",
    "r = requests.get(url)\n",
    "comm_from_sub_data = json.loads(r.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 398,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comm_from_sub_data['data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 301,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comm_ids = [x['id'] for x in comm_from_sub_data['data']]\n",
    "comm_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 286,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(comm_ids).issubset(set(sub_comm_data['data']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25, 71)"
      ]
     },
     "execution_count": 287,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(comm_ids),len(sub_comm_data['data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
