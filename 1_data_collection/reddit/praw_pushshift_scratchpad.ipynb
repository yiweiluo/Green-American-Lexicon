{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
      "  import pandas.util.testing as tm\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "from pprint import pprint\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import praw\n",
    "from praw.models import Submission\n",
    "from psaw import PushshiftAPI\n",
    "from prawcore.exceptions import Forbidden,NotFound\n",
    "import csv\n",
    "import os\n",
    "from urllib.error import HTTPError\n",
    "import glob\n",
    "import requests\n",
    "import json\n",
    "from json import JSONDecodeError\n",
    "import datetime\n",
    "import pickle\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "_RE_COMBINE_WHITESPACE = re.compile(r\"\\s+\")\n",
    "\n",
    "def strip_whitespace(text):\n",
    "    return _RE_COMBINE_WHITESPACE.sub(\" \", text).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": [
     0,
     50
    ]
   },
   "outputs": [],
   "source": [
    "def get_submissions(reddit_instance,subreddit_str):\n",
    "    \n",
    "    if not os.path.exists('praw_output'):\n",
    "        os.mkdir('praw_output')\n",
    "    \n",
    "    subreddit = reddit_instance.subreddit(subreddit_str)\n",
    "    print('Getting submissions and comments from: {}'.format(subreddit.display_name))  \n",
    "    try:\n",
    "        title = subreddit.title\n",
    "        desc = subreddit.description\n",
    "\n",
    "        if not os.path.exists('subreddits.tsv'):\n",
    "            with open('subreddits.tsv','w') as f:\n",
    "                csvwriter = csv.writer(f, delimiter='\\t')\n",
    "                csvwriter.writerow([subreddit.display_name,title,desc])\n",
    "        else:\n",
    "            with open('subreddits.tsv','a') as f:\n",
    "                csvwriter = csv.writer(f, delimiter='\\t')\n",
    "                csvwriter.writerow([subreddit.display_name,title,desc])\n",
    "\n",
    "        # Write header\n",
    "        with open(os.path.join('praw_output','{}.tsv'.format(subreddit.display_name)), 'w', newline='\\n') as csvfile:\n",
    "            csvwriter = csv.writer(csvfile, delimiter='\\t',\n",
    "                                    quotechar='|', quoting=csv.QUOTE_MINIMAL)\n",
    "            csvwriter.writerow(['title','author','date','is_video','id','num_downs','num_ups','upvote_ratio',\n",
    "                               'num_comments','score','text','subreddit'])\n",
    "\n",
    "        # Write body\n",
    "        for submission in subreddit.new(limit=None):\n",
    "            sub_title = submission.title\n",
    "            sub_author = submission.author.name if submission.author is not None else -1\n",
    "            sub_date = submission.created\n",
    "            sub_is_vid = submission.is_video\n",
    "            sub_id = submission.id\n",
    "            sub_downvotes = submission.downs\n",
    "            sub_upvotes = submission.ups\n",
    "            sub_upvote_ratio = submission.upvote_ratio\n",
    "            sub_num_comments = submission.num_comments\n",
    "            sub_score = submission.score\n",
    "            sub_text = submission.selftext.strip().replace('\\t','').replace('\\n','')\n",
    "            sub_subreddit = submission.subreddit.display_name\n",
    "            with open(os.path.join('praw_output','{}.tsv'.format(subreddit.display_name)), 'a', newline='\\n') as csvfile:\n",
    "                csvwriter = csv.writer(csvfile, delimiter='\\t',\n",
    "                                    quotechar='|', quoting=csv.QUOTE_MINIMAL)\n",
    "                csvwriter.writerow([sub_title,sub_author,sub_date,sub_is_vid,sub_id,sub_downvotes,\n",
    "                                   sub_upvotes,sub_upvote_ratio,sub_num_comments,sub_score,sub_text,sub_subreddit])\n",
    "    except HTTPError as e:\n",
    "        if e.code == 403:\n",
    "            print('Forbidden: private subreddit.')\n",
    "            \n",
    "def get_submission_comments(reddit_instance,subreddit,submission_id):\n",
    "    \n",
    "    submission = Submission(reddit_instance,id=submission_id)\n",
    "    \n",
    "    try:\n",
    "        submission.comments.replace_more(limit=0)\n",
    "        all_comments = submission.comments.list()\n",
    "\n",
    "        # Write header\n",
    "        with open(os.path.join('praw_output','post_comments','{}_COMMENTS.tsv'.format(subreddit)), 'w', newline='\\n') as csvfile:\n",
    "            csvwriter = csv.writer(csvfile, delimiter='\\t',\n",
    "                                    quotechar='|', quoting=csv.QUOTE_MINIMAL)\n",
    "            csvwriter.writerow(['submission_id','author','text','date','id','controversiality','num_downs','num_ups',\n",
    "                               'num_likes','score','subreddit'])\n",
    "\n",
    "        # Write body\n",
    "        for comment in all_comments:\n",
    "            sub_id = comment._submission.id\n",
    "            assert sub_id == submission_id\n",
    "            author_name = comment.author.name if comment.author is not None else -1\n",
    "            comment_body = comment.body.strip().replace('\\t','').replace('\\n','')\n",
    "            date_created = comment.created\n",
    "            comment_id = comment.id\n",
    "            controversiality = comment.controversiality\n",
    "            num_downs = comment.downs\n",
    "            num_ups = comment.ups\n",
    "            num_likes = comment.likes\n",
    "            score = comment.score\n",
    "            subreddit_name = comment.subreddit.display_name\n",
    "            #print(subreddit_name,subreddit)\n",
    "            assert subreddit_name == subreddit\n",
    "            \n",
    "            with open(os.path.join('praw_output','post_comments','{}_COMMENTS.tsv'.format(subreddit_name)), 'a', newline='\\n') as csvfile:\n",
    "                csvwriter = csv.writer(csvfile, delimiter='\\t',\n",
    "                                    quotechar='|', quoting=csv.QUOTE_MINIMAL)\n",
    "                csvwriter.writerow([sub_id,author_name,comment_body,date_created,comment_id,controversiality,\n",
    "                                   num_downs,num_ups,num_likes,score,subreddit_name])\n",
    "    except HTTPError as e:\n",
    "        if e.code == 403:\n",
    "            print('Forbidden: private subreddit.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../keywords_long.txt','r') as f:\n",
    "    KEYWORDS_LONG = f.read().splitlines()\n",
    "KEYWORDS_SHORT = set([\"climate change\",\"global warming\",\"carbon\",\"co2\",\"methane\",\n",
    "                  \"green\",\"environment\",\"fossil fuel\"])\n",
    "#my_keywords = set(['climate change', 'global warming', 'fossil fuel', 'methane', 'carbon', 'co2'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in climate-related subreddits:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pro     42\n",
       "anti    20\n",
       "neut    10\n",
       "Name: stance, dtype: int64"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SUBREDDITS = pd.read_csv('CLIMATE_SUBREDDITS.txt',sep='\\t',header=0)\n",
    "SUBREDDITS.stance.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddit</th>\n",
       "      <th>stance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>science</td>\n",
       "      <td>neut</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>energy</td>\n",
       "      <td>neut</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>climatecmv</td>\n",
       "      <td>neut</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>Republican</td>\n",
       "      <td>neut</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>republicans</td>\n",
       "      <td>neut</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>askaconservative</td>\n",
       "      <td>neut</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>Conservative</td>\n",
       "      <td>neut</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>conservatives</td>\n",
       "      <td>neut</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>TrueConservativism</td>\n",
       "      <td>neut</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>AskTrumpSupporters</td>\n",
       "      <td>neut</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             subreddit stance\n",
       "4              science   neut\n",
       "19              energy   neut\n",
       "52          climatecmv   neut\n",
       "65          Republican   neut\n",
       "66         republicans   neut\n",
       "67    askaconservative   neut\n",
       "68        Conservative   neut\n",
       "69       conservatives   neut\n",
       "70  TrueConservativism   neut\n",
       "71  AskTrumpSupporters   neut"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SUBREDDITS.loc[SUBREDDITS.stance=='neut']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(72, 72)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(SUBREDDITS['subreddit'])),len(SUBREDDITS['subreddit'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "SUBREDDITS_LIST = list(SUBREDDITS['subreddit'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create PRAW reddit instance to get posts and comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit = praw.Reddit(client_id='1sbu376RCBiWRw',\n",
    "                     client_secret='NbqiHMPiKicBXvgfrID-xVNktZM',\n",
    "                     user_agent='mac:cc_framing:v1 (by /u/emma_cc_research)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "non_niche_subs = set(['AskReddit','environment','politics','worldnews','climateskeptics',\n",
    "                     'Showerthoughts','climate','askscience','The_Donald','science','EcoInternet',\n",
    "                     'collapse','explainlikeimfive','conspiracy','NoStupidQuestions','australia',\n",
    "                     'unpopularopinion','climatechange','news','energy','canada','Conservative',\n",
    "                     'skeptic','todayilearned','shittyaskscience','ChapoTrapHouse','CanadaPolitics',\n",
    "                     'EverythingScience','worldpolitics','europe','AskScienceDiscussion',\n",
    "                     'ClimateOffensive','changemyview','ClimateActionPlan','AskTrumpSupporters',\n",
    "                     'GlobalWarming','GlobalClimateChange','esist','Green'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_id = 'aqnhy'\n",
    "post = reddit.submission(id=p_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'http://www.sciencedaily.com/releases/2010/01/100114081543.htm'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "post.url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "post.is_self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[deleted]'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "post.selftext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not found: ChapoTrapHouse\n",
      "Not found: The_Donald\n"
     ]
    }
   ],
   "source": [
    "n_members = {}\n",
    "for sub_name in non_niche_subs:\n",
    "    try:\n",
    "        n_members[sub_name] = reddit.subreddit(sub_name).subscribers\n",
    "    except NotFound:\n",
    "        print('Not found:',sub_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# According to https://people.com/politics/reddit-bans-the-donald-chapotraphouse-groups/\n",
    "n_members['ChapoTrapHouse'] = 150000\n",
    "n_members['The_Donald'] = 800000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(n_members,open('num_members_curated_subreddits.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_reddit': <praw.reddit.Reddit at 0x11c21d630>,\n",
       " '_fetched': False,\n",
       " '_listing_use_sort': True,\n",
       " 'name': 'DragonFireDon'}"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Can it know if a user is a bot?\n",
    "vars(reddit.redditor(\"DragonFireDon\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13230"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reddit.redditor(\"AutoModerator\").link_karma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_reddit': <praw.reddit.Reddit at 0x1177ea240>,\n",
       " '_fetched': False,\n",
       " '_listing_use_sort': True,\n",
       " 'name': 'AutoModerator'}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reddit.redditor(\"AutoModerator\").__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Redditor(name='emma_cc_research')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reddit.redditor(\"emma_cc_research\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get posts from r/spambotwatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting submissions and comments from: spambotwatch\n"
     ]
    }
   ],
   "source": [
    "get_submissions(reddit,'spambotwatch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get_submission_comments(reddit,'spambotwatch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0           JEEVAN BOBY (u/jeevanbobyvallickad) - Reddit\n",
       "1                                 overview for funnynova\n",
       "2      Spams Dating Tips Websites all with Stolen Con...\n",
       "3                                 overview for poopcake5\n",
       "4                                   overview for sutei_m\n",
       "                             ...                        \n",
       "190                               overview for royboy204\n",
       "191                                 overview for mlg7732\n",
       "192                            overview for 0wned_alover\n",
       "193                                 overview for xrox333\n",
       "194                                overview for frede933\n",
       "Name: title, Length: 195, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spambotwatch_df = pd.read_csv('praw_output/spambotwatch.tsv',sep='\\t',header=0)\n",
    "spambotwatch_df.title"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Get posts from all subreddits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting submissions and comments from: AskTrumpSupporters\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(SUBREDDITS['subreddit'])-1,len(SUBREDDITS['subreddit'])):\n",
    "    SUBREDDIT = SUBREDDITS_LIST[i]\n",
    "    get_submissions(reddit,SUBREDDIT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Inspect output: tsv of subreddits and meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>climateskeptics</td>\n",
       "      <td>Climate Skeptics: Trying to see through the al...</td>\n",
       "      <td>Seeing past hyperbole, alarmism and environmen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>skeptic</td>\n",
       "      <td>skeptic</td>\n",
       "      <td>## [Click this link to Read the Rules](http://...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>climatechange</td>\n",
       "      <td>A place for a rational discussion on a divisiv...</td>\n",
       "      <td>This is a place for the rational discussion of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>climate</td>\n",
       "      <td>Information about the world's climate</td>\n",
       "      <td>Real and accurate data about the Earth's clima...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>science</td>\n",
       "      <td>Reddit Science</td>\n",
       "      <td># [Submission Rules](https://www.reddit.com/r/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>republicans</td>\n",
       "      <td>Republicans - RNC - GOP: Grand Old Party</td>\n",
       "      <td>Republican, RNC and GOP news, issues, gossip, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>askaconservative</td>\n",
       "      <td>Ask A Conservative: Ask Conservatives And Repu...</td>\n",
       "      <td>#[Ask a Conservative](/r/askaconservative)\\n\\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>Conservative</td>\n",
       "      <td>Conservative</td>\n",
       "      <td>#####\\n**[Join us on discord.](https://discord...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>conservatives</td>\n",
       "      <td>conservatives</td>\n",
       "      <td>Conservatism (from, conservare, \"to preserve\")...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>AskTrumpSupporters</td>\n",
       "      <td>AskTrumpSupporters</td>\n",
       "      <td>#We are not a typical subreddit. Read the [ful...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>68 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     0                                                  1  \\\n",
       "0      climateskeptics  Climate Skeptics: Trying to see through the al...   \n",
       "1              skeptic                                            skeptic   \n",
       "2        climatechange  A place for a rational discussion on a divisiv...   \n",
       "3              climate              Information about the world's climate   \n",
       "4              science                                     Reddit Science   \n",
       "..                 ...                                                ...   \n",
       "63         republicans           Republicans - RNC - GOP: Grand Old Party   \n",
       "64    askaconservative  Ask A Conservative: Ask Conservatives And Repu...   \n",
       "65        Conservative                                       Conservative   \n",
       "66       conservatives                                      conservatives   \n",
       "67  AskTrumpSupporters                                 AskTrumpSupporters   \n",
       "\n",
       "                                                    2  \n",
       "0   Seeing past hyperbole, alarmism and environmen...  \n",
       "1   ## [Click this link to Read the Rules](http://...  \n",
       "2   This is a place for the rational discussion of...  \n",
       "3   Real and accurate data about the Earth's clima...  \n",
       "4   # [Submission Rules](https://www.reddit.com/r/...  \n",
       "..                                                ...  \n",
       "63  Republican, RNC and GOP news, issues, gossip, ...  \n",
       "64  #[Ask a Conservative](/r/askaconservative)\\n\\n...  \n",
       "65  #####\\n**[Join us on discord.](https://discord...  \n",
       "66  Conservatism (from, conservare, \"to preserve\")...  \n",
       "67  #We are not a typical subreddit. Read the [ful...  \n",
       "\n",
       "[68 rows x 3 columns]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv('subreddits.tsv',sep='\\t',header=None).drop_duplicates(0,keep='first')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Inspect output: tsv of one subreddit's posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('praw_output/350.tsv',sep='\\t',header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['title', 'author', 'date', 'is_video', 'id', 'num_downs', 'num_ups',\n",
       "       'upvote_ratio', 'num_comments', 'score', 'text', 'subreddit'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    127\n",
       "Name: is_video, dtype: int64"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.is_video.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "350    127\n",
       "Name: subreddit, dtype: int64"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.subreddit.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#df.head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Get comments for all posts with non-zero num comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "code_folding": [
     1
    ],
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already got comments for subreddit praw_output/posts/350.tsv\n",
      "Already got comments for subreddit praw_output/posts/350ppm.tsv\n",
      "Already got comments for subreddit praw_output/posts/askaconservative.tsv\n",
      "Already got comments for subreddit praw_output/posts/AskTrumpSupporters.tsv\n",
      "Already got comments for subreddit praw_output/posts/carboncapture.tsv\n",
      "Already got comments for subreddit praw_output/posts/carbontax.tsv\n",
      "Already got comments for subreddit praw_output/posts/ccfunding.tsv\n",
      "Already got comments for subreddit praw_output/posts/climate.tsv\n",
      "Already got comments for subreddit praw_output/posts/climate_activism.tsv\n",
      "Already got comments for subreddit praw_output/posts/climate_discussion.tsv\n",
      "Already got comments for subreddit praw_output/posts/climate_science.tsv\n",
      "Already got comments for subreddit praw_output/posts/ClimateActionPlan.tsv\n",
      "Already got comments for subreddit praw_output/posts/climatechange.tsv\n",
      "Already got comments for subreddit praw_output/posts/ClimateChangeCancer.tsv\n",
      "Already got comments for subreddit praw_output/posts/climatecmv.tsv\n",
      "Already got comments for subreddit praw_output/posts/ClimateCrisis.tsv\n",
      "Already got comments for subreddit praw_output/posts/climatedebate.tsv\n",
      "Already got comments for subreddit praw_output/posts/climatedebate2.tsv\n",
      "Already got comments for subreddit praw_output/posts/climategate.tsv\n",
      "0 comments among all posts in subreddit: ClimateMobilization\n",
      "Already got comments for subreddit praw_output/posts/ClimateOffensive.tsv\n",
      "Already got comments for subreddit praw_output/posts/climatepredicts.tsv\n",
      "Already got comments for subreddit praw_output/posts/ClimatePreparation.tsv\n",
      "Already got comments for subreddit praw_output/posts/climatesecurity.tsv\n",
      "Already got comments for subreddit praw_output/posts/climateskeptics.tsv\n",
      "Already got comments for subreddit praw_output/posts/ClimateSkepticScience.tsv\n",
      "Already got comments for subreddit praw_output/posts/ClimateSplattergate.tsv\n",
      "0 comments among all posts in subreddit: climatestasis\n",
      "Already got comments for subreddit praw_output/posts/Climatology.tsv\n",
      "Already got comments for subreddit praw_output/posts/climatskeptics.tsv\n",
      "Already got comments for subreddit praw_output/posts/CollapseSurvival.tsv\n",
      "Already got comments for subreddit praw_output/posts/conservation.tsv\n",
      "Already got comments for subreddit praw_output/posts/Conservative.tsv\n",
      "Already got comments for subreddit praw_output/posts/conservatives.tsv\n",
      "Subreddit praw_output/posts/DebateAClimateSkeptic.tsv has no posts\n",
      "Getting comments from posts in subreddit: deep_ecology\n",
      "Getting comments from posts in subreddit: drought\n",
      "Getting comments from posts in subreddit: EarthDisaster\n",
      "Getting comments from posts in subreddit: EarthScience\n",
      "Getting comments from posts in subreddit: EcoInternet\n",
      "Getting comments from posts in subreddit: ecology\n",
      "Subreddit praw_output/posts/ecologycenter.tsv has no posts\n",
      "Getting comments from posts in subreddit: EcoRestoration\n",
      "Getting comments from posts in subreddit: energy\n",
      "Getting comments from posts in subreddit: enviroaction\n",
      "Getting comments from posts in subreddit: Enviroment\n",
      "Getting comments from posts in subreddit: environment\n",
      "Getting comments from posts in subreddit: environmental_science\n",
      "Subreddit praw_output/posts/Global_Warming.tsv has no posts\n",
      "Getting comments from posts in subreddit: GlobalClimateChange\n",
      "Getting comments from posts in subreddit: GlobalWarming\n",
      "Getting comments from posts in subreddit: GlobalWarmingisBunk\n",
      "Getting comments from posts in subreddit: Green\n",
      "Getting comments from posts in subreddit: GreenNewIdeas\n",
      "0 comments among all posts in subreddit: GreenPlanet\n",
      "0 comments among all posts in subreddit: GWB\n",
      "Getting comments from posts in subreddit: Rainforest\n",
      "Subreddit praw_output/posts/RealClimateSkeptics.tsv has no posts\n",
      "Getting comments from posts in subreddit: Republican\n",
      "Getting comments from posts in subreddit: republicans\n",
      "Getting comments from posts in subreddit: Restoration_Ecology\n",
      "Getting comments from posts in subreddit: science\n",
      "Getting comments from posts in subreddit: skeptic\n",
      "Getting comments from posts in subreddit: sustainability\n",
      "Getting comments from posts in subreddit: Sustainable_Energy\n",
      "Subreddit praw_output/posts/ThunbergSyndrome.tsv has no posts\n",
      "Getting comments from posts in subreddit: trueclimateskeptics\n",
      "0 comments among all posts in subreddit: WorldClimate\n"
     ]
    }
   ],
   "source": [
    "for subreddit_tsv in glob.glob('praw_output/posts/*.tsv'):\n",
    "    if os.path.exists('praw_output/post_comments/{}_COMMENTS.tsv'.format(subreddit_tsv.split('/')[-1][:-4])):\n",
    "        print('Already got comments for subreddit {}'.format(subreddit_tsv))\n",
    "    else:\n",
    "        subreddit_posts = pd.read_csv(subreddit_tsv,sep='\\t',header=0)\n",
    "        if len(subreddit_posts) > 0:\n",
    "            subreddit = str(subreddit_posts.iloc[0]['subreddit'])\n",
    "            posts_with_comments = subreddit_posts.loc[subreddit_posts.num_comments > 0]\n",
    "            if len(posts_with_comments) > 0:\n",
    "                print('Getting comments from posts in subreddit: {}'.format(subreddit))\n",
    "                for ix,row in posts_with_comments.iterrows():\n",
    "                    get_submission_comments(reddit,subreddit,row['id'])\n",
    "            else:\n",
    "                print('0 comments among all posts in subreddit: {}'.format(subreddit))\n",
    "        else:\n",
    "            print('Subreddit {} has no posts'.format(subreddit_tsv))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Use Pushshift API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'global warming|climate change|carbon|fossil fuel|methane|environment|co2|climate crisis|climate emergency|extreme weather|2 degree|sustainable|clean energy|renewable|cap and trade|sea level rise|environmental justice|climate justice|COP|IPCC|deforestation|permafrost|glacier|drought|ecosystem|greenhouse gas|greenhouse effect|green new deal|EPA'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multiple_search_term = '|'.join(KEYWORDS_LONG)\n",
    "multiple_search_term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "before_date = datetime.datetime.strptime(\"07-22-2020\", \"%m-%d-%Y\")\n",
    "after_date = datetime.datetime.strptime(\"07-20-2020\", \"%m-%d-%Y\")\n",
    "before_timestamp = int(datetime.datetime.timestamp(before_date))\n",
    "after_timestamp = int(datetime.datetime.timestamp(after_date))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "test_data = getPushshiftData(multiple_search_term,after_timestamp,before_timestamp,\"comment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "test_data = getPushshiftData(multiple_search_term,after_timestamp,before_timestamp,\"submission\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# test_data[0\n",
    "#         ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "code_folding": [
     0,
     10,
     66
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def getPushshiftData(query, after, before, datatype):\n",
    "    query_prefix = 'title' if datatype == 'submission' else 'q'\n",
    "    url = 'https://api.pushshift.io/reddit/search/'+datatype+'/?'+\\\n",
    "    query_prefix+'='+str(query)+'&size=1000&after='+str(after)+'&before='+str(before)\n",
    "    #print(url)\n",
    "    r = requests.get(url)\n",
    "    data = json.loads(r.text)\n",
    "    return data['data']\n",
    "\n",
    "\n",
    "def collectSubData(subm,subs_dict):\n",
    "    try:\n",
    "        title = subm['title']\n",
    "    except KeyError:\n",
    "        title = None\n",
    "    try:\n",
    "        url = subm['url']\n",
    "    except KeyError:\n",
    "        url = None\n",
    "    try:\n",
    "        flair = subm['link_flair_text']\n",
    "    except KeyError:\n",
    "        flair = \"NaN\"   \n",
    "    try:\n",
    "        author = subm['author']\n",
    "    except KeyError:\n",
    "        author = None\n",
    "    sub_id = subm['id']\n",
    "    try:\n",
    "        score = subm['score']\n",
    "    except KeyError:\n",
    "        score = None\n",
    "    try:\n",
    "        created = datetime.datetime.fromtimestamp(subm['created_utc']) #1520561700.0\n",
    "    except KeyError:\n",
    "        created = None\n",
    "    try:\n",
    "        numComms = subm['num_comments']\n",
    "    except KeyError:\n",
    "        numComms = None\n",
    "    try:\n",
    "        permalink = subm['permalink']\n",
    "    except KeyError:\n",
    "        permalink = None\n",
    "    try:\n",
    "        is_vid = subm['is_video']\n",
    "    except KeyError:\n",
    "        is_vid = None\n",
    "    try:\n",
    "        upvote_ratio = subm['upvote_ratio']\n",
    "    except KeyError:\n",
    "        upvote_ratio = None\n",
    "    try:\n",
    "        text = subm['selftext'].strip().replace('\\t','').replace('\\n','')\n",
    "    except KeyError:\n",
    "        text = \"\"\n",
    "    try:\n",
    "        subreddit = subm['subreddit']\n",
    "    except KeyError:\n",
    "        subreddit = None\n",
    "    subData = {'id':sub_id,'title':title,'url':url,'author':author,'score':score,'date':created,\n",
    "                    'num_comments':numComms,'permalink':permalink,'flair':flair,'is_video':is_vid,\n",
    "                    'upvote_ratio':upvote_ratio,'text':text,'subreddit':subreddit}\n",
    "    subs_dict[sub_id] = subData\n",
    "    \n",
    "    \n",
    "def collectCommData(subm,subs_dict): \n",
    "    try:\n",
    "        author = subm['author']\n",
    "    except KeyError:\n",
    "        author = None\n",
    "    sub_id = subm['id']\n",
    "    try:\n",
    "        link_id = subm['link_id']\n",
    "    except KeyError:\n",
    "        link_id = None\n",
    "    try:\n",
    "        score = subm['score']\n",
    "    except KeyError:\n",
    "        score = None\n",
    "    try:\n",
    "        created = datetime.datetime.fromtimestamp(subm['created_utc']) #1520561700.0\n",
    "    except KeyError:\n",
    "        created = None\n",
    "    try:\n",
    "        permalink = subm['permalink']\n",
    "    except KeyError:\n",
    "        permalink = None\n",
    "    try:\n",
    "        text = subm['body'].strip().replace('\\t','').replace('\\n','')\n",
    "    except KeyError:\n",
    "        text = \"\"\n",
    "    try:\n",
    "        subreddit = subm['subreddit']\n",
    "    except KeyError:\n",
    "        subreddit = None\n",
    "    subData = {'id':sub_id,'link_id':link_id,'author':author,'score':score,'date':created,\n",
    "                    'permalink':permalink,'text':text,'subreddit':subreddit}\n",
    "    subs_dict[sub_id] = subData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def pushshift_wrapper(after_str, before_str, datatype, query=None, keywords=None):\n",
    "    failed_requests = []\n",
    "    \n",
    "    if query is None:\n",
    "        query = '|'.join(keywords)\n",
    "    \n",
    "    subCount = 0\n",
    "    subStats = {}\n",
    "    before_date = datetime.datetime.strptime(before_str, \"%m-%d-%Y\")\n",
    "    after_date = datetime.datetime.strptime(after_str, \"%m-%d-%Y\")\n",
    "    before_timestamp = int(datetime.datetime.timestamp(before_date))\n",
    "    after_timestamp = int(datetime.datetime.timestamp(after_date))\n",
    "    print(\"Getting all submissions with query '{}' from {} to {}\".format(query,\n",
    "                                                                         after_str,before_str))\n",
    "    try:\n",
    "        data = getPushshiftData(query, after_timestamp, before_timestamp, datatype)\n",
    "        # Will run until all posts have been gathered \n",
    "        # from the 'after' date up until before date\n",
    "        while len(data) > 0:\n",
    "            for submission in data:\n",
    "                if datatype == \"submission\":\n",
    "                    collectSubData(submission,subStats)\n",
    "                else:\n",
    "                    collectCommData(submission,subStats)\n",
    "                subCount+=1\n",
    "            # Calls getPushshiftData() with the created date of the last submission\n",
    "            #print(len(data))\n",
    "            #print(str(datetime.datetime.fromtimestamp(data[-1]['created_utc'])))\n",
    "            after_timestamp = data[-1]['created_utc']\n",
    "            #print(after_timestamp)\n",
    "            try:\n",
    "                data = getPushshiftData(query, after_timestamp, before_timestamp, datatype)\n",
    "            except JSONDecodeError:\n",
    "                failed_requests.append((query,after_timestamp,before_timestamp,datatype))\n",
    "\n",
    "        print('Num submissions:',subCount,len(subStats))\n",
    "\n",
    "        interim_df = pd.DataFrame(list(subStats.values()))\n",
    "        #print(interim_df)\n",
    "\n",
    "        datatype_prefix = 'posts' if datatype == 'submission' else 'post_comments'\n",
    "        out_dir = os.path.join('pushshift_output',datatype_prefix,'{}_to_{}'.format(after_str,before_str))\n",
    "        failed_reqs_out_dir = os.path.join('pushshift_output','failed_requests',\n",
    "                                                      '{}_{}'.format(after_str,before_str))\n",
    "        if not os.path.exists(out_dir):\n",
    "            os.mkdir(out_dir)\n",
    "        if not os.path.exists(failed_reqs_out_dir):\n",
    "            os.mkdir(failed_reqs_out_dir)\n",
    "            \n",
    "        if keywords is None:\n",
    "            interim_df.to_pickle(os.path.join(out_dir,'{}.pkl'.format(query)))\n",
    "            print('Saved query submissions to {}!'.format(os.path.join(out_dir,'{}.pkl'.format(query))))\n",
    "            pickle.dump(failed_requests,open(os.path.join(failed_reqs_out_dir,'{}.pkl'.format(query)),'wb'))\n",
    "        else:\n",
    "            interim_df.to_pickle(os.path.join(out_dir,'{}.pkl'.format('keywords_long')))\n",
    "            print('Saved query submissions to {}!'.format(os.path.join(out_dir,'{}.pkl'.format('keywords_long'))))\n",
    "            pickle.dump(failed_requests,open(os.path.join(failed_reqs_out_dir,'{}.pkl'.format('keywords_long')),'wb'))\n",
    "            \n",
    "    except JSONDecodeError:\n",
    "        failed_requests.append((query,after_timestamp,before_timestamp,datatype))\n",
    "        print(\"First request failed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#keywords_missing = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# for start_year in range(2010,2015,1):\n",
    "#     end_year = start_year+1\n",
    "#     for keyword in KEYWORDS_LONG:\n",
    "#         if not os.path.exists(os.path.join('pushshift_output','posts',\n",
    "#                                            '1-1-{}_to_12-31-{}'.format(start_year,start_year),\n",
    "#                                            '{}.pkl'.format(keyword))):\n",
    "#             print(\"Missing {}, {}\".format(keyword,start_year))\n",
    "#             keywords_missing.append(keyword)\n",
    "#             #pushshift_wrapper('1-1-{}'.format(start_year),'12-31-{}'.format(start_year),'submission',\n",
    "#             #query=None,keywords=keywords_missing)\n",
    "# print('************')    \n",
    "# for start_year in range(2010,2015,1):\n",
    "#     end_year = start_year+1\n",
    "#     for keyword in KEYWORDS_LONG:\n",
    "#         if not os.path.exists(os.path.join('pushshift_output','post_comments',\n",
    "#                                            '1-1-{}_to_12-31-{}'.format(start_year,start_year),\n",
    "#                                            '{}.pkl'.format(keyword))):\n",
    "#             print(\"Missing {}, {}\".format(keyword,start_year))\n",
    "#             #pushshift_wrapper(keyword,'1-1-{}'.format(start_year),'12-31-{}'.format(start_year),'comment')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# for start_year in range(2015,2021,1):\n",
    "#     end_year = start_year+1\n",
    "#     pushshift_wrapper('1-1-{}'.format(start_year),'12-31-{}'.format(start_year),'submission',\n",
    "#                       query=None,keywords=keywords_missing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get all comments attached to a post"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PRAW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "419100"
      ]
     },
     "execution_count": 410,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "posts_with_comments_ids = pickle.load(open('output/posts_with_comments_ids.pkl','rb'))\n",
    "print(len(posts_with_comments_ids))\n",
    "sub_ids_to_fetch = list(posts_with_comments_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('output/comment_ids_per_post.tsv','w') as f:\n",
    "#     f.write(\"{}\\t{}\\n\".format('post_id','comment_ids'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def praw_get_comments(sub_id):\n",
    "    try:\n",
    "        post = reddit.submission(id=sub_id)\n",
    "        post_author = post.author\n",
    "        post_title = post.title\n",
    "        post_comms = list(post.__dict__['_comments_by_id'].keys())\n",
    "        #print(len(post_comms))\n",
    "        comments_per_post[sub_id] = post_comms\n",
    "\n",
    "        with open('output/comment_ids_per_post.tsv','a') as f:\n",
    "            f.write(\"{}\\t{}\\n\".format(sub_id,','.join(post_comms)))\n",
    "    except Forbidden:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "175000\n",
      "176000\n",
      "177000\n",
      "178000\n",
      "179000\n",
      "180000\n",
      "181000\n",
      "182000\n",
      "183000\n",
      "184000\n",
      "185000\n",
      "186000\n",
      "187000\n",
      "188000\n",
      "189000\n",
      "190000\n",
      "191000\n",
      "192000\n",
      "193000\n",
      "194000\n",
      "195000\n",
      "196000\n",
      "197000\n",
      "198000\n",
      "199000\n",
      "200000\n",
      "201000\n",
      "202000\n",
      "203000\n",
      "204000\n",
      "205000\n",
      "206000\n",
      "207000\n",
      "208000\n",
      "209000\n",
      "210000\n",
      "211000\n",
      "212000\n",
      "213000\n",
      "214000\n",
      "215000\n",
      "216000\n",
      "217000\n",
      "218000\n",
      "219000\n",
      "220000\n",
      "221000\n",
      "222000\n",
      "223000\n",
      "224000\n",
      "225000\n",
      "226000\n",
      "227000\n",
      "228000\n",
      "229000\n",
      "230000\n",
      "231000\n",
      "232000\n",
      "233000\n",
      "234000\n",
      "235000\n",
      "236000\n",
      "237000\n",
      "238000\n",
      "239000\n",
      "240000\n",
      "241000\n",
      "242000\n",
      "243000\n",
      "244000\n",
      "245000\n",
      "246000\n",
      "247000\n",
      "248000\n",
      "249000\n",
      "250000\n",
      "251000\n",
      "252000\n",
      "253000\n",
      "254000\n",
      "255000\n",
      "256000\n",
      "257000\n",
      "258000\n",
      "259000\n",
      "260000\n",
      "261000\n",
      "262000\n",
      "263000\n",
      "264000\n",
      "265000\n",
      "266000\n",
      "267000\n",
      "268000\n",
      "269000\n",
      "270000\n",
      "271000\n",
      "272000\n",
      "273000\n",
      "274000\n",
      "275000\n",
      "276000\n",
      "277000\n",
      "278000\n",
      "279000\n",
      "280000\n",
      "281000\n",
      "282000\n",
      "283000\n",
      "284000\n",
      "285000\n",
      "286000\n",
      "287000\n",
      "288000\n",
      "289000\n",
      "290000\n",
      "291000\n",
      "292000\n",
      "293000\n",
      "294000\n",
      "295000\n",
      "296000\n",
      "297000\n",
      "298000\n",
      "299000\n",
      "300000\n",
      "301000\n",
      "302000\n",
      "303000\n",
      "304000\n",
      "305000\n",
      "306000\n",
      "307000\n",
      "308000\n",
      "309000\n",
      "310000\n",
      "311000\n",
      "312000\n",
      "313000\n",
      "314000\n",
      "315000\n",
      "316000\n",
      "317000\n",
      "318000\n",
      "319000\n",
      "320000\n",
      "321000\n",
      "322000\n",
      "323000\n",
      "324000\n",
      "325000\n",
      "326000\n",
      "327000\n",
      "328000\n",
      "329000\n",
      "330000\n",
      "331000\n",
      "332000\n",
      "333000\n",
      "334000\n",
      "335000\n",
      "336000\n",
      "337000\n",
      "338000\n",
      "339000\n",
      "340000\n",
      "341000\n",
      "342000\n",
      "343000\n",
      "344000\n",
      "345000\n",
      "346000\n",
      "347000\n",
      "348000\n",
      "349000\n",
      "350000\n",
      "351000\n",
      "352000\n",
      "353000\n",
      "354000\n",
      "355000\n",
      "356000\n",
      "357000\n",
      "358000\n",
      "359000\n",
      "360000\n",
      "361000\n",
      "362000\n",
      "363000\n",
      "364000\n",
      "365000\n",
      "366000\n",
      "367000\n",
      "368000\n",
      "369000\n",
      "370000\n",
      "371000\n",
      "372000\n",
      "373000\n",
      "374000\n",
      "375000\n",
      "376000\n",
      "377000\n",
      "378000\n",
      "379000\n",
      "380000\n",
      "381000\n",
      "382000\n",
      "383000\n",
      "384000\n",
      "385000\n",
      "386000\n",
      "387000\n",
      "388000\n",
      "389000\n",
      "390000\n",
      "391000\n",
      "392000\n",
      "393000\n",
      "394000\n",
      "395000\n",
      "396000\n",
      "397000\n",
      "398000\n",
      "399000\n",
      "400000\n",
      "401000\n",
      "402000\n",
      "403000\n",
      "404000\n",
      "405000\n",
      "406000\n",
      "407000\n",
      "408000\n",
      "409000\n",
      "410000\n",
      "411000\n",
      "412000\n",
      "413000\n",
      "414000\n",
      "415000\n",
      "416000\n",
      "417000\n",
      "418000\n",
      "419000\n"
     ]
    }
   ],
   "source": [
    "for ix_sub_id in range(174623,len(sub_ids_to_fetch)):\n",
    "    sub_id = sub_ids_to_fetch[ix_sub_id]\n",
    "    praw_get_comments(sub_id)\n",
    "    \n",
    "    if ix_sub_id % 1000 == 0:\n",
    "        print(ix_sub_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(174623, '92efdf')"
      ]
     },
     "execution_count": 460,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ix_sub_id,sub_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11720"
      ]
     },
     "execution_count": 447,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(comments_per_post)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>post_id</th>\n",
       "      <th>comment_ids</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>anbg0</td>\n",
       "      <td>t1_c0ignhg,t1_c0ignz0,t1_c0igq3n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>apn8b</td>\n",
       "      <td>t1_c0irhej,t1_c0is8sw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>aprgq</td>\n",
       "      <td>t1_c0isfnb,t1_c0is90g,t1_c0iscd2,t1_c0isbol,t1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>aprgv</td>\n",
       "      <td>t1_c0is3y4,t1_c0ism1k,t1_c0isyft,t1_c0isvkx,t1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>aprie</td>\n",
       "      <td>t1_c0is4by,t1_c0islml,t1_c0ismfv,t1_c0isk7q,t1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>403412</th>\n",
       "      <td>3yuldj</td>\n",
       "      <td>t1_cygrqj0,t1_cygrhs5,t1_cygtcrx,t1_cygtrvf,t1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>403413</th>\n",
       "      <td>3yulsl</td>\n",
       "      <td>t1_cylxj3h,t1_cygqso3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>403414</th>\n",
       "      <td>3yup0j</td>\n",
       "      <td>t1_cyhjhnw,t1_cylxiyc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>403415</th>\n",
       "      <td>3yuytd</td>\n",
       "      <td>t1_cygu9uo,t1_cygx6g5,t1_cyhpqys,t1_cygxgig,t1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>403416</th>\n",
       "      <td>3yve0p</td>\n",
       "      <td>t1_cyh9a2i</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>403417 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       post_id                                        comment_ids\n",
       "0        anbg0                   t1_c0ignhg,t1_c0ignz0,t1_c0igq3n\n",
       "1        apn8b                              t1_c0irhej,t1_c0is8sw\n",
       "2        aprgq  t1_c0isfnb,t1_c0is90g,t1_c0iscd2,t1_c0isbol,t1...\n",
       "3        aprgv  t1_c0is3y4,t1_c0ism1k,t1_c0isyft,t1_c0isvkx,t1...\n",
       "4        aprie  t1_c0is4by,t1_c0islml,t1_c0ismfv,t1_c0isk7q,t1...\n",
       "...        ...                                                ...\n",
       "403412  3yuldj  t1_cygrqj0,t1_cygrhs5,t1_cygtcrx,t1_cygtrvf,t1...\n",
       "403413  3yulsl                              t1_cylxj3h,t1_cygqso3\n",
       "403414  3yup0j                              t1_cyhjhnw,t1_cylxiyc\n",
       "403415  3yuytd  t1_cygu9uo,t1_cygx6g5,t1_cyhpqys,t1_cygxgig,t1...\n",
       "403416  3yve0p                                         t1_cyh9a2i\n",
       "\n",
       "[403417 rows x 2 columns]"
      ]
     },
     "execution_count": 462,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv('output/comment_ids_per_post.tsv',sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Personally I am more concerned about the fact that methane is an extremely effective greenhouse gas.  '"
      ]
     },
     "execution_count": 405,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comment = reddit.comment(\"c0st842\")\n",
    "comment.body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comment_ids_per_post = pd.read_csv('output/comment_ids_per_post.tsv',sep='\\t')\n",
    "comment_ids = comment_ids_per_post['comment_ids']\n",
    "all_comment_ids = [x.split(',') for x in comment_ids]\n",
    "all_comment_ids = [item for sublist in all_comment_ids for item in sublist]\n",
    "unique_comment_ids = set(all_comment_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(all_comment_ids),len(unique_comment_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataframe of \n",
    "# comment_id | text\n",
    "\n",
    "with open('output/text_per_comment.tsv','w') as f:\n",
    "    f.write('{}\\t{}\\n'.format('comment_id','text'))\n",
    "    \n",
    "unique_comment_ids = list(unique_comment_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c_id_index in range(len(unique_comment_ids)):\n",
    "    c_id = unique_comment_ids[c_id_index]\n",
    "    comm = reddit.comment(c_id)\n",
    "    comment_body = strip_whitespace(comm.body)\n",
    "    \n",
    "    with open('output/text_per_comment.tsv','a') as f:\n",
    "        f.write('{}\\t{}\\n'.format(c_id,comment_body))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Pushshift -- doesn't seem to work >:("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def getComments(sub_id):\n",
    "    url = 'https://api.pushshift.io/reddit/submission/comment_ids/{}'.format(sub_id)\n",
    "    r = requests.get(url)\n",
    "    sub_comm_data = json.loads(r.text)\n",
    "    return sub_comm_data['data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def pushshift_comment_wrapper(link_id, after_str, before_str, datatype):\n",
    "    failed_requests = []\n",
    "    \n",
    "    subCount = 0\n",
    "    subStats = {}\n",
    "    before_date = datetime.datetime.strptime(before_str, \"%m-%d-%Y\")\n",
    "    after_date = datetime.datetime.strptime(after_str, \"%m-%d-%Y\")\n",
    "    before_timestamp = int(datetime.datetime.timestamp(before_date))\n",
    "    after_timestamp = int(datetime.datetime.timestamp(after_date))\n",
    "    print(\"Getting all comments with link_id '{}' from {} to {}\".format(link_id,after_str,before_str))\n",
    "    try:\n",
    "        data = getComments(link_id, after_timestamp, before_timestamp, datatype)\n",
    "        # Will run until all posts have been gathered \n",
    "        # from the 'after' date up until before date\n",
    "        while len(data) > 0:\n",
    "            for submission in data:\n",
    "                if datatype == \"submission\":\n",
    "                    collectSubData(submission,subStats)\n",
    "                else:\n",
    "                    collectCommData(submission,subStats)\n",
    "                subCount+=1\n",
    "            # Calls getPushshiftData() with the created date of the last submission\n",
    "            #print(len(data))\n",
    "            #print(str(datetime.datetime.fromtimestamp(data[-1]['created_utc'])))\n",
    "            after_timestamp = data[-1]['created_utc']\n",
    "            #print(after_timestamp)\n",
    "            try:\n",
    "                data = getComments(link_id, after_timestamp, before_timestamp, datatype)\n",
    "            except JSONDecodeError:\n",
    "                failed_requests.append((link_id,after_timestamp,before_timestamp,datatype))\n",
    "\n",
    "        print('Num submissions:',subCount,len(subStats))\n",
    "\n",
    "        interim_df = pd.DataFrame(list(subStats.values()))\n",
    "        #print(interim_df)\n",
    "\n",
    "        datatype_prefix = 'posts' if datatype == 'submission' else 'linked_post_comments'\n",
    "        out_dir = os.path.join('output','pushshift_output',datatype_prefix,'{}_to_{}'.format(after_str,before_str))\n",
    "        failed_reqs_out_dir = os.path.join('pushshift_output','failed_requests',\n",
    "                                                      '{}_{}'.format(after_str,before_str))\n",
    "        if not os.path.exists(out_dir):\n",
    "            os.mkdir(out_dir)\n",
    "        if not os.path.exists(failed_reqs_out_dir):\n",
    "            os.mkdir(failed_reqs_out_dir)\n",
    "            \n",
    "        interim_df.to_pickle(os.path.join(out_dir,'{}.pkl'.format(link_id)))\n",
    "        print('Saved query submissions to {}!'.format(os.path.join(out_dir,'{}.pkl'.format(link_id))))\n",
    "        pickle.dump(failed_requests,open(os.path.join(failed_reqs_out_dir,'{}.pkl'.format(link_id)),'wb'))\n",
    "            \n",
    "    except JSONDecodeError:\n",
    "        failed_requests.append((query,after_timestamp,before_timestamp,datatype))\n",
    "        print(\"First request failed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 390,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getComments(sub_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "url = 'https://api.pushshift.io/reddit/submission/search/?ids={}&limit=1000'.format(sub_id)\n",
    "r = requests.get(url)\n",
    "sub_data = json.loads(r.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 392,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sub_data['data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#sub_data['data'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "url = 'https://api.pushshift.io/reddit/comment/search/?id=43u5cw'\n",
    "r = requests.get(url)\n",
    "comm_data = json.loads(r.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#comm_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "url = 'https://api.pushshift.io/reddit/submission/comment_ids/{}'.format(sub_id)\n",
    "r = requests.get(url)\n",
    "sub_comm_data = json.loads(r.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 396,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub_comm_data['data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "url = 'https://api.pushshift.io/reddit/comment/search/?link_id={}'.format(sub_id)\n",
    "r = requests.get(url)\n",
    "comm_from_sub_data = json.loads(r.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 398,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comm_from_sub_data['data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 301,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comm_ids = [x['id'] for x in comm_from_sub_data['data']]\n",
    "comm_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 286,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(comm_ids).issubset(set(sub_comm_data['data']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25, 71)"
      ]
     },
     "execution_count": 287,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(comm_ids),len(sub_comm_data['data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
