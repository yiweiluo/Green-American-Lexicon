{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn import svm\n",
    "from numpy.random import RandomState\n",
    "from scipy.stats import ttest_ind\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn import metrics\n",
    "from datetime import timedelta  \n",
    "from sklearn.utils import shuffle\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "code_folding": [
     1,
     4
    ]
   },
   "outputs": [],
   "source": [
    "# Functions for importing & cleaning relevant tweets\n",
    "def lower(s):\n",
    "    return s.lower()\n",
    "\n",
    "def tweet_imports(filename):\n",
    "    imp = pd.read_pickle(filename)\n",
    "    imp = imp.drop_duplicates()\n",
    "    imp['tweet_clean'] = imp['tweet'].str.replace('http\\S+|www.\\S+|pic.twitter.com\\S+', '', case=False)\n",
    "    imp['tweet_clean'] =imp['tweet_clean'].replace('[^A-Za-z0-9 ]+','',regex=True)\n",
    "    imp['tweet_clean'] = imp['tweet_clean'].apply(lower)#map(lambda x: x.lower(), imp['tweet_clean'])\n",
    "    imp['date'] = pd.to_datetime(imp['date'])\n",
    "    return imp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(230802, 12)\n",
      "(290084, 12)\n",
      "(230802, 12)\n",
      "(290084, 12)\n"
     ]
    }
   ],
   "source": [
    "# Collect all tweets from every user into 2 groups of affirming and denying tweets, add label (1 vs. -1)\n",
    "all_affirm_tweets = []\n",
    "all_deny_tweets = []\n",
    "\n",
    "for filename in os.listdir(os.getcwd()+'/affirm_tweets/'):\n",
    "    tweets_per_user = pd.read_pickle(os.getcwd()+'/affirm_tweets/'+filename)\n",
    "    all_affirm_tweets.append(tweets_per_user)\n",
    "for filename in os.listdir(os.getcwd()+'/deny_tweets/'):\n",
    "    tweets_per_user = pd.read_pickle(os.getcwd()+'/deny_tweets/'+filename)\n",
    "    all_deny_tweets.append(tweets_per_user)\n",
    "    \n",
    "affirm_tweets = pd.concat(all_affirm_tweets)\n",
    "affirm_tweets['label'] = [1]*affirm_tweets.shape[0]\n",
    "deny_tweets = pd.concat(all_deny_tweets)\n",
    "deny_tweets['label'] = [-1]*deny_tweets.shape[0]\n",
    "print(affirm_tweets.shape)\n",
    "print(deny_tweets.shape)\n",
    "\n",
    "affirm_tweets.drop_duplicates(subset =\"id\", keep = 'first', inplace = True)\n",
    "deny_tweets.drop_duplicates(subset =\"id\", keep = 'first', inplace = True)\n",
    "print(affirm_tweets.shape)\n",
    "print(deny_tweets.shape)\n",
    "\n",
    "affirm_tweets.to_pickle('all_affirm_tweets.pkl')\n",
    "deny_tweets.to_pickle('all_deny_tweets.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_affirm_tweets = tweet_imports('all_affirm_tweets.pkl')\n",
    "cleaned_deny_tweets = tweet_imports('all_deny_tweets.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>time</th>\n",
       "      <th>tweet</th>\n",
       "      <th>mentions</th>\n",
       "      <th>replies_count</th>\n",
       "      <th>retweets_count</th>\n",
       "      <th>likes_count</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>username</th>\n",
       "      <th>search_term</th>\n",
       "      <th>label</th>\n",
       "      <th>tweet_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1177060829901885441</td>\n",
       "      <td>2019-09-25</td>\n",
       "      <td>20:22:25</td>\n",
       "      <td>#IPCC just released the #SROCC - a new report ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>1</td>\n",
       "      <td>40</td>\n",
       "      <td>75</td>\n",
       "      <td>['#ipcc', '#srocc']</td>\n",
       "      <td>350</td>\n",
       "      <td>ice</td>\n",
       "      <td>1</td>\n",
       "      <td>ipcc just released the srocc  a new report on ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1176786922687148032</td>\n",
       "      <td>2019-09-25</td>\n",
       "      <td>02:14:00</td>\n",
       "      <td>The #IPCC special report on ocean and ice is o...</td>\n",
       "      <td>[]</td>\n",
       "      <td>5</td>\n",
       "      <td>83</td>\n",
       "      <td>112</td>\n",
       "      <td>['#ipcc', '#srocc']</td>\n",
       "      <td>350</td>\n",
       "      <td>ice</td>\n",
       "      <td>1</td>\n",
       "      <td>the ipcc special report on ocean and ice is ou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1164031877910618114</td>\n",
       "      <td>2019-08-20</td>\n",
       "      <td>21:30:01</td>\n",
       "      <td>Unusually warm water surrounding one of the la...</td>\n",
       "      <td>[]</td>\n",
       "      <td>1</td>\n",
       "      <td>62</td>\n",
       "      <td>93</td>\n",
       "      <td>[]</td>\n",
       "      <td>350</td>\n",
       "      <td>ice</td>\n",
       "      <td>1</td>\n",
       "      <td>unusually warm water surrounding one of the la...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1152577659710427136</td>\n",
       "      <td>2019-07-20</td>\n",
       "      <td>06:55:02</td>\n",
       "      <td>This is one of the hottest summers on record. ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>26</td>\n",
       "      <td>['#abolishice']</td>\n",
       "      <td>350</td>\n",
       "      <td>ice</td>\n",
       "      <td>1</td>\n",
       "      <td>this is one of the hottest summers on record t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1146328119336460288</td>\n",
       "      <td>2019-07-03</td>\n",
       "      <td>01:01:36</td>\n",
       "      <td>Antarctic ice has taken a nosedive.\\n\\nThe amo...</td>\n",
       "      <td>[]</td>\n",
       "      <td>6</td>\n",
       "      <td>106</td>\n",
       "      <td>115</td>\n",
       "      <td>[]</td>\n",
       "      <td>350</td>\n",
       "      <td>ice</td>\n",
       "      <td>1</td>\n",
       "      <td>antarctic ice has taken a nosedivethe amount o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1175370661985828865</td>\n",
       "      <td>2019-09-21</td>\n",
       "      <td>04:26:17</td>\n",
       "      <td>As #GretaThunberg led the Climate Strike in Ne...</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>['#gretathunberg', '#nantichaocharoenchai', '#...</td>\n",
       "      <td>yv4ca</td>\n",
       "      <td>climate</td>\n",
       "      <td>1</td>\n",
       "      <td>as gretathunberg led the climate strike in new...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1169913415294259200</td>\n",
       "      <td>2019-09-06</td>\n",
       "      <td>03:01:08</td>\n",
       "      <td>The world is running out of time ðŸ¤œa race we ca...</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>['#apclimateweek', '#apyouth4climate']</td>\n",
       "      <td>yv4ca</td>\n",
       "      <td>climate</td>\n",
       "      <td>1</td>\n",
       "      <td>the world is running out of time a race we can...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1169535040792850437</td>\n",
       "      <td>2019-09-05</td>\n",
       "      <td>01:57:37</td>\n",
       "      <td>Will you Fight ðŸ¤œfor the future! \\n\\nJoin us to...</td>\n",
       "      <td>[]</td>\n",
       "      <td>14</td>\n",
       "      <td>149</td>\n",
       "      <td>277</td>\n",
       "      <td>['#climateemergency', '#apclimateweek', '#apyo...</td>\n",
       "      <td>yv4ca</td>\n",
       "      <td>climate</td>\n",
       "      <td>1</td>\n",
       "      <td>will you fight for the future join us to empow...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1169518364760276997</td>\n",
       "      <td>2019-09-05</td>\n",
       "      <td>00:51:21</td>\n",
       "      <td>\"Everyone has a responsible role towards tackl...</td>\n",
       "      <td>['earthdaynetwork']</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>['#apclimateweek', '#apyouth4climate']</td>\n",
       "      <td>yv4ca</td>\n",
       "      <td>climate</td>\n",
       "      <td>1</td>\n",
       "      <td>everyone has a responsible role towards tackli...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1167707623803826176</td>\n",
       "      <td>2019-08-31</td>\n",
       "      <td>00:56:07</td>\n",
       "      <td>Join the Youth Voice for Climate Action campai...</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>['#influencing', '#enhancing', '#mobilizing', ...</td>\n",
       "      <td>yv4ca</td>\n",
       "      <td>climate</td>\n",
       "      <td>1</td>\n",
       "      <td>join the youth voice for climate action campai...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>230802 rows Ã— 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     id       date      time  \\\n",
       "0   1177060829901885441 2019-09-25  20:22:25   \n",
       "1   1176786922687148032 2019-09-25  02:14:00   \n",
       "2   1164031877910618114 2019-08-20  21:30:01   \n",
       "3   1152577659710427136 2019-07-20  06:55:02   \n",
       "4   1146328119336460288 2019-07-03  01:01:36   \n",
       "..                  ...        ...       ...   \n",
       "1   1175370661985828865 2019-09-21  04:26:17   \n",
       "2   1169913415294259200 2019-09-06  03:01:08   \n",
       "3   1169535040792850437 2019-09-05  01:57:37   \n",
       "4   1169518364760276997 2019-09-05  00:51:21   \n",
       "5   1167707623803826176 2019-08-31  00:56:07   \n",
       "\n",
       "                                                tweet             mentions  \\\n",
       "0   #IPCC just released the #SROCC - a new report ...                   []   \n",
       "1   The #IPCC special report on ocean and ice is o...                   []   \n",
       "2   Unusually warm water surrounding one of the la...                   []   \n",
       "3   This is one of the hottest summers on record. ...                   []   \n",
       "4   Antarctic ice has taken a nosedive.\\n\\nThe amo...                   []   \n",
       "..                                                ...                  ...   \n",
       "1   As #GretaThunberg led the Climate Strike in Ne...                   []   \n",
       "2   The world is running out of time ðŸ¤œa race we ca...                   []   \n",
       "3   Will you Fight ðŸ¤œfor the future! \\n\\nJoin us to...                   []   \n",
       "4   \"Everyone has a responsible role towards tackl...  ['earthdaynetwork']   \n",
       "5   Join the Youth Voice for Climate Action campai...                   []   \n",
       "\n",
       "    replies_count  retweets_count  likes_count  \\\n",
       "0               1              40           75   \n",
       "1               5              83          112   \n",
       "2               1              62           93   \n",
       "3               0              17           26   \n",
       "4               6             106          115   \n",
       "..            ...             ...          ...   \n",
       "1               0               0            2   \n",
       "2               0               1            5   \n",
       "3              14             149          277   \n",
       "4               0               3            6   \n",
       "5               0               3            6   \n",
       "\n",
       "                                             hashtags username search_term  \\\n",
       "0                                 ['#ipcc', '#srocc']      350         ice   \n",
       "1                                 ['#ipcc', '#srocc']      350         ice   \n",
       "2                                                  []      350         ice   \n",
       "3                                     ['#abolishice']      350         ice   \n",
       "4                                                  []      350         ice   \n",
       "..                                                ...      ...         ...   \n",
       "1   ['#gretathunberg', '#nantichaocharoenchai', '#...    yv4ca     climate   \n",
       "2              ['#apclimateweek', '#apyouth4climate']    yv4ca     climate   \n",
       "3   ['#climateemergency', '#apclimateweek', '#apyo...    yv4ca     climate   \n",
       "4              ['#apclimateweek', '#apyouth4climate']    yv4ca     climate   \n",
       "5   ['#influencing', '#enhancing', '#mobilizing', ...    yv4ca     climate   \n",
       "\n",
       "    label                                        tweet_clean  \n",
       "0       1  ipcc just released the srocc  a new report on ...  \n",
       "1       1  the ipcc special report on ocean and ice is ou...  \n",
       "2       1  unusually warm water surrounding one of the la...  \n",
       "3       1  this is one of the hottest summers on record t...  \n",
       "4       1  antarctic ice has taken a nosedivethe amount o...  \n",
       "..    ...                                                ...  \n",
       "1       1  as gretathunberg led the climate strike in new...  \n",
       "2       1  the world is running out of time a race we can...  \n",
       "3       1  will you fight for the future join us to empow...  \n",
       "4       1  everyone has a responsible role towards tackli...  \n",
       "5       1  join the youth voice for climate action campai...  \n",
       "\n",
       "[230802 rows x 13 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_affirm_tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import manually labelled & influential tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500 238 262\n",
      "261\n"
     ]
    }
   ],
   "source": [
    "# TODO: replace w/ own manually labeled tweets\n",
    "test_tweets = pd.concat([cleaned_affirm_tweets,cleaned_deny_tweets])\n",
    "test_tweets = test_tweets.sample(n = 500) \n",
    "print(len(test_tweets), len(test_tweets[test_tweets['label']==1]), len(test_tweets[test_tweets['label']==-1]))\n",
    "x = test_tweets[test_tweets['label']==-1]\n",
    "print(len(x.drop_duplicates('tweet_clean')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "520886 230802 290084\n"
     ]
    }
   ],
   "source": [
    "# train1 = tweet_imports(\"Datasets/Training Data/influential_tweets_filter_1.csv\")\n",
    "# train2 = tweet_imports(\"Datasets/Training Data/influential_tweets_filter_2.csv\")\n",
    "# train3 = tweet_imports(\"Datasets/Training Data/influential_tweets_filter_3.csv\")\n",
    "# all_train = pd.concat([train1, train2, train3])\n",
    "all_train = pd.concat([cleaned_affirm_tweets, cleaned_deny_tweets])\n",
    "\n",
    "# Allison et al. had: (403432, 220063, 183369)\n",
    "print(len(all_train), len(all_train[all_train['label']==1]), len(all_train[all_train['label']==-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "299709\n"
     ]
    }
   ],
   "source": [
    "# # Select tweets that contain disasters from the training data \n",
    "# disaster_train = all_train[all_train['tweet_clean'].str.contains(\"michael|florence|wildfire|blizzard|fire|hurricane|bomb|cyclone|storm|snow|blaze\")==True]\n",
    "# disaster_train = disaster_train[disaster_train['tweet_clean'].str.contains(\"climate|change|global|warming\")==True]\n",
    "sub_train = all_train[all_train['tweet_clean'].str.contains(\"climate|change|global|warming\")==True]\n",
    "\n",
    "# Remove from training data the tweets that are already in the test data\n",
    "#dis_set = disaster_train[['tweet_clean', 'label']].copy()\n",
    "sub_set = sub_train[['tweet_clean', 'label']].copy()\n",
    "test_tweets = test_tweets[['tweet_clean', 'label']].copy()\n",
    "#dis_set['identifier'] = 0\n",
    "sub_set['identifier'] = 0\n",
    "test_tweets['identifier'] = 1\n",
    "#dis_set = pd.concat([dis_set, test_tweets])\n",
    "sub_set = pd.concat([sub_set,test_tweets])\n",
    "#dis_set.drop_duplicates(keep=False)\n",
    "sub_set.drop_duplicates(keep='first')\n",
    "#disaster_train = dis_set[dis_set['identifier']==0]\n",
    "sub_train = sub_set[sub_set['identifier']==0]\n",
    "#print(len(disaster_train))\n",
    "print(len(sub_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import & clean downloaded Twitter data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_tweets_restrict(filename, start_date, end_date):\n",
    "    \n",
    "    # Unlabeled tweets\n",
    "    tweets = tweet_imports(\"Datasets/Twint Output/\" + filename)\n",
    "     \n",
    "    # Constrain to relevant dates\n",
    "    print(min(tweets['date']), max(tweets['date']))\n",
    "    \n",
    "    begin_tweets = pd.to_datetime(start_date) - timedelta(weeks = 2)\n",
    "    end_tweets = pd.to_datetime(end_date) + timedelta(weeks = 2)\n",
    "    print(\"Two weeks before:\", begin_tweets, \"Two weeks after:\", end_tweets)\n",
    "    \n",
    "    tweets = tweets[tweets['date'] >= begin_tweets]\n",
    "    tweets = tweets[tweets['date'] <= end_tweets]\n",
    "\n",
    "    # Remove tweets to label that were already seen in train/valid/test for forming predictions\n",
    "    tweet_dis_overlap = tweets.merge(disaster_train, on=['tweet_clean'])\n",
    "    tweets = tweets[(~tweets.tweet_clean.isin(tweet_dis_overlap.tweet_clean))]\n",
    "    tweet_test_overlap = tweets.merge(test_tweets, on=['tweet_clean'])\n",
    "    tweets = tweets[(~tweets.tweet_clean.isin(tweet_test_overlap.tweet_clean))]\n",
    "\n",
    "    # Combine pre-labeled tweets\n",
    "    pre_labelled_tweets = pd.concat((tweet_dis_overlap, tweet_test_overlap), axis=0)\n",
    "    print(len(tweets), len(pre_labelled_tweets))\n",
    "    \n",
    "    return((tweets, pre_labelled_tweets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_tweets_restrict(filename, start_date, end_date, outfile):\n",
    "    \n",
    "    tweets, pre_labelled_tweets = clean_tweets_restrict(filename, start_date, end_date)\n",
    "    \n",
    "    # Split to pre- and post\n",
    "    pre = tweets[tweets['date'] <= start_date]\n",
    "    post = tweets[tweets['date'] > start_date]\n",
    "    pre.reset_index(inplace=True)\n",
    "    post.reset_index(inplace=True)\n",
    "    print(\"Total tweets to label\", len(tweets), \"Prior tweets to label\", len(pre), \"Post tweets to label\", len(post))\n",
    "    \n",
    "    # Merge pre- and post- tweets by same user to see if user sentiments change\n",
    "    pre_users = pd.DataFrame(pre['user_id'].unique())\n",
    "    post_users = pd.DataFrame(post['user_id'].unique())\n",
    "    merge = pd.merge(pre_users, post_users, how='inner')\n",
    "    print(\"Number of users tweeting before and after\", len(merge))\n",
    "\n",
    "    pre_tweets = pre.loc[pre['user_id'].isin(merge.iloc[:,0])]\n",
    "    post_tweets = post.loc[post['user_id'].isin(merge.iloc[:,0])]\n",
    "    print(\"Num tweets before\", len(pre_tweets), \"Num tweets after\", len(post_tweets))\n",
    "    \n",
    "    tweets.to_csv('Datasets/Event Tweets/' + outfile + '.csv', sep=',')\n",
    "    pre_labelled_tweets.to_csv('Datasets/Event Tweets/Prelabelled_' + outfile + '.csv', sep=',')\n",
    "    \n",
    "    return((tweets, pre, post, merge))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_tweets_restrict_combine(filename1, filename2, start_date, end_date, outfile):\n",
    "    \n",
    "    # Unlabeled tweets\n",
    "    tweets1, pre_labelled_tweets1 = clean_tweets_restrict(filename1, start_date, end_date)\n",
    "    tweets2, pre_labelled_tweets2 = clean_tweets_restrict(filename2, start_date, end_date)\n",
    "    print(len(tweets1))\n",
    "    print(len(tweets2))\n",
    "    tweets = pd.merge(tweets1, tweets2, how='outer')\n",
    "    pre_labelled_tweets = pd.merge(pre_labelled_tweets1, pre_labelled_tweets2, how='outer')\n",
    "    \n",
    "    # Split to pre- and post\n",
    "    pre = tweets[tweets['date'] <= start_date]\n",
    "    post = tweets[tweets['date'] > start_date]\n",
    "    pre.reset_index(inplace=True)\n",
    "    post.reset_index(inplace=True)\n",
    "    print(\"Total tweets\", len(tweets), \"Prior tweets\", len(pre), \"Post tweets\", len(post))\n",
    "    \n",
    "    # Merge pre- and post- tweets by same user to see if user sentiments change\n",
    "    pre_users = pd.DataFrame(pre['user_id'].unique())\n",
    "    post_users = pd.DataFrame(post['user_id'].unique())\n",
    "    merge = pd.merge(pre_users, post_users, how='inner')\n",
    "    print(\"Number of users tweeting before and after\", len(merge))\n",
    "\n",
    "    pre_tweets = pre.loc[pre['user_id'].isin(merge.iloc[:,0])]\n",
    "    post_tweets = post.loc[post['user_id'].isin(merge.iloc[:,0])]\n",
    "    print(\"Num tweets before\", len(pre_tweets), \"Num tweets after\", len(post_tweets))\n",
    "    \n",
    "    tweets.to_csv('Datasets/Event Tweets/' + outfile + '.csv', sep=',')\n",
    "    pre_labelled_tweets.to_csv('Datasets/Event Tweets/Prelabelled_' + outfile + '.csv', sep=',')\n",
    "    \n",
    "    return((tweets, pre, post, merge))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Timestamp('2017-11-21 00:00:00'), Timestamp('2018-01-19 00:00:00'))\n",
      "('Two weeks before:', Timestamp('2017-12-19 00:00:00'), 'Two weeks after:', Timestamp('2018-01-20 00:00:00'))\n",
      "(14957, 1610)\n",
      "('Total tweets to label', 14957, 'Prior tweets to label', 2614, 'Post tweets to label', 12343)\n",
      "('Number of users tweeting before and after', 330)\n",
      "('Num tweets before', 634, 'Num tweets after', 1479)\n"
     ]
    }
   ],
   "source": [
    "# January 2018 bomb cyclone (Jan 2 - Jan 6): https://en.wikipedia.org/wiki/January_2018_North_American_blizzard\n",
    "blizzard_tweets, blizzard_pre, blizzard_post, blizzard_merge = count_tweets_restrict('blizzard_geo_tweets_v2.csv', '1/2/18', '1/6/18', 'blizzard')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Timestamp('2018-05-31 00:00:00'), Timestamp('2018-09-26 00:00:00'))\n",
      "('Two weeks before:', Timestamp('2018-07-13 00:00:00'), 'Two weeks after:', Timestamp('2018-10-02 00:00:00'))\n",
      "(2710, 929)\n",
      "(Timestamp('2018-06-04 00:00:00'), Timestamp('2018-10-01 00:00:00'))\n",
      "('Two weeks before:', Timestamp('2018-07-13 00:00:00'), 'Two weeks after:', Timestamp('2018-10-02 00:00:00'))\n",
      "(2808, 956)\n",
      "2710\n",
      "2808\n",
      "('Total tweets', 3035, 'Prior tweets', 173, 'Post tweets', 2862)\n",
      "('Number of users tweeting before and after', 36)\n",
      "('Num tweets before', 49, 'Num tweets after', 95)\n"
     ]
    }
   ],
   "source": [
    "# California Mendocino Wildfires (July 27 - Sep 18): https://en.wikipedia.org/wiki/Mendocino_Complex_Fire\n",
    "summerfire_tweets, summerfire_pre, summerfire_post, summerfire_merge = count_tweets_restrict_combine('summerfire_geo_tweets.csv', \n",
    "                                                                                             'summerfire_geo_tweets_v2.csv', \n",
    "                                                                                             '7/27/18', '9/18/18',\n",
    "                                                                                                    'summerfire')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Timestamp('2018-07-03 00:00:00'), Timestamp('2018-09-30 00:00:00'))\n",
      "('Two weeks before:', Timestamp('2018-08-17 00:00:00'), 'Two weeks after:', Timestamp('2018-10-03 00:00:00'))\n",
      "(6413, 1032)\n",
      "('Total tweets to label', 6413, 'Prior tweets to label', 778, 'Post tweets to label', 5635)\n",
      "('Number of users tweeting before and after', 122)\n",
      "('Num tweets before', 193, 'Num tweets after', 497)\n"
     ]
    }
   ],
   "source": [
    "# Hurricane Florence (Aug 31 - Sep 19): https://en.wikipedia.org/wiki/Hurricane_Florence\n",
    "florence_tweets, florence_pre, florence_post, florence_merge = count_tweets_restrict('florence_geo_tweets.csv', '8/31/18', '9/19/18',\n",
    "                                                                                    'florence')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Timestamp('2018-09-05 00:00:00'), Timestamp('2018-10-28 00:00:00'))\n",
      "('Two weeks before:', Timestamp('2018-09-23 00:00:00'), 'Two weeks after:', Timestamp('2018-10-30 00:00:00'))\n",
      "(13035, 3126)\n",
      "('Total tweets to label', 13035, 'Prior tweets to label', 1912, 'Post tweets to label', 11123)\n",
      "('Number of users tweeting before and after', 281)\n",
      "('Num tweets before', 880, 'Num tweets after', 1351)\n"
     ]
    }
   ],
   "source": [
    "# Hurricane Michael (Oct 7 - Oct 16): https://en.wikipedia.org/wiki/Hurricane_Michael\n",
    "michael_tweets, michael_pre, michael_post, michael_merge = count_tweets_restrict('michael_geo_tweets.csv', '10/07/18', '10/16/18',\n",
    "                                                                                'michael')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Timestamp('2018-09-10 00:00:00'), Timestamp('2018-12-08 00:00:00'))\n",
      "('Two weeks before:', Timestamp('2018-10-25 00:00:00'), 'Two weeks after:', Timestamp('2018-12-09 00:00:00'))\n",
      "(5375, 151)\n",
      "(Timestamp('2018-09-10 00:00:00'), Timestamp('2018-12-08 00:00:00'))\n",
      "('Two weeks before:', Timestamp('2018-10-25 00:00:00'), 'Two weeks after:', Timestamp('2018-12-09 00:00:00'))\n",
      "(6081, 143)\n",
      "5375\n",
      "6081\n",
      "('Total tweets', 6654, 'Prior tweets', 55, 'Post tweets', 6599)\n",
      "('Number of users tweeting before and after', 14)\n",
      "('Num tweets before', 19, 'Num tweets after', 43)\n"
     ]
    }
   ],
   "source": [
    "# California Camp wildfires (Nov 8 - 25): https://en.wikipedia.org/wiki/Camp_Fire_(2018)\n",
    "winterfire_tweets, winterfire_pre, winterfire_post, winterfire_merge = count_tweets_restrict_combine(\n",
    "    'winterfire_geo_tweets.csv', \n",
    "    'winterfire_geo_tweets_v2.csv', \n",
    "    '11/08/18', '11/25/18', 'winterfire')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create train / validation / test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "276273\n",
      "125790 150483\n",
      "238 262\n"
     ]
    }
   ],
   "source": [
    "#randomly select tweets that go to validation set or training set\n",
    "sub_train = shuffle(sub_train,random_state=123)\n",
    "sub_train = sub_train.drop_duplicates('tweet_clean')\n",
    "num_tweets = len(sub_train)\n",
    "\n",
    "print(num_tweets)\n",
    "infl_val_pos = sub_train[sub_train['label']==1]\n",
    "infl_val_neg = sub_train[sub_train['label']==-1]\n",
    "print(len(infl_val_pos), len(infl_val_neg))\n",
    "\n",
    "labeled_tweets = test_tweets[test_tweets['label']!=0]\n",
    "test_tweets_shuffle = shuffle(labeled_tweets,random_state=456)\n",
    "manual_pos = test_tweets_shuffle[test_tweets_shuffle['label']==1]\n",
    "manual_neg = test_tweets_shuffle[test_tweets_shuffle['label']==-1]\n",
    "print(len(manual_pos), len(manual_neg))\n",
    "\n",
    "train_pct = .9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "499\n",
      "238\n",
      "261\n",
      "Num total tweets 276273 \n",
      " Num train tweets 248645 \n",
      " Num validation tweets 27628 \n",
      " Num test tweets 488\n"
     ]
    }
   ],
   "source": [
    "training_data = sub_train[:int(num_tweets*train_pct)]\n",
    "\n",
    "val_tweets = sub_train[int(num_tweets*train_pct):]\n",
    "\n",
    "# concatenate test tweets\n",
    "half_test_num = 250\n",
    "\n",
    "test_tweets_shuffle = test_tweets_shuffle.drop_duplicates('tweet_clean')\n",
    "print(len(test_tweets_shuffle))\n",
    "test_tweets_pos = test_tweets_shuffle[test_tweets_shuffle['label']==1]\n",
    "print(len(test_tweets_pos))\n",
    "test_tweets_neg = test_tweets_shuffle[test_tweets_shuffle['label']==-1]\n",
    "print(len(test_tweets_neg))\n",
    "\n",
    "test_tweets = shuffle(pd.concat([test_tweets_neg[:half_test_num],test_tweets_pos[:half_test_num]]),random_state=0)\n",
    "\n",
    "print(\"Num total tweets\", num_tweets,\n",
    "      \"\\n Num train tweets\", len(training_data), \n",
    "      \"\\n Num validation tweets\", len(val_tweets), \n",
    "      \"\\n Num test tweets\", len(test_tweets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num train tweets 248645 Num positive tweets 113434 Num negative tweets 135211\n",
      "Num val tweets 27628 Num positive tweets 12356 Num negative tweets 15272\n",
      "Num test tweets 488 Num positive tweets 238 Num negative tweets 250\n"
     ]
    }
   ],
   "source": [
    "# Basic stats on training data from celebrities\n",
    "train_pos = training_data[training_data['label']==1]\n",
    "train_neg = training_data[training_data['label']==-1]\n",
    "print(\"Num train tweets\", len(training_data), \"Num positive tweets\", len(train_pos), \n",
    "      \"Num negative tweets\", len(train_neg))\n",
    "\n",
    "# Check number of manual labelled tweets and tweets to label\n",
    "val_tweets_pos = val_tweets[val_tweets['label']==1]\n",
    "val_tweets_neg = val_tweets[val_tweets['label']==-1]\n",
    "print(\"Num val tweets\", len(val_tweets), \"Num positive tweets\", len(val_tweets_pos), \n",
    "      \"Num negative tweets\", len(val_tweets_neg))\n",
    "\n",
    "# Check number of manual labelled tweets and tweets to label\n",
    "test_tweets_pos = test_tweets[test_tweets['label']==1]\n",
    "test_tweets_neg = test_tweets[test_tweets['label']==-1]\n",
    "print(\"Num test tweets\", len(test_tweets), \"Num positive tweets\", len(test_tweets_pos), \n",
    "      \"Num negative tweets\", len(test_tweets_neg))\n",
    "\n",
    "test_tweets = pd.concat([test_tweets_pos, test_tweets_neg])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove test tweets from training & validation sets\n",
    "test_train_overlap = training_data.merge(test_tweets, on=['tweet_clean'])\n",
    "#print(len(test_train_overlap))\n",
    "training_data = training_data[(~training_data.tweet_clean.isin(test_train_overlap.tweet_clean))]\n",
    "#print(len(training_data))\n",
    "\n",
    "test_val_overlap = val_tweets.merge(test_tweets, on=['tweet_clean'])\n",
    "#print(len(test_val_overlap))\n",
    "val_tweets = val_tweets[(~val_tweets.tweet_clean.isin(test_val_overlap.tweet_clean))]\n",
    "#print(len(val_tweets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num train tweets 248378 Num positive tweets 113298 Num negative tweets 135080\n",
      "Num val tweets 27607 Num positive tweets 12347 Num negative tweets 15260\n",
      "Num test tweets 488 Num positive tweets 238 Num negative tweets 250\n"
     ]
    }
   ],
   "source": [
    "# Basic stats on training data from celebrities\n",
    "train_pos = training_data[training_data['label']==1]\n",
    "train_neg = training_data[training_data['label']==-1]\n",
    "print(\"Num train tweets\", len(training_data), \"Num positive tweets\", len(train_pos), \n",
    "      \"Num negative tweets\", len(train_neg))\n",
    "\n",
    "# Check number of manual labelled tweets and tweets to label\n",
    "val_tweets_pos = val_tweets[val_tweets['label']==1]\n",
    "val_tweets_neg = val_tweets[val_tweets['label']==-1]\n",
    "print(\"Num val tweets\", len(val_tweets), \"Num positive tweets\", len(val_tweets_pos), \n",
    "      \"Num negative tweets\", len(val_tweets_neg))\n",
    "\n",
    "# Check number of manual labelled tweets and tweets to label\n",
    "test_tweets_pos = test_tweets[test_tweets['label']==1]\n",
    "test_tweets_neg = test_tweets[test_tweets['label']==-1]\n",
    "print(\"Num test tweets\", len(test_tweets), \"Num positive tweets\", len(test_tweets_pos), \n",
    "      \"Num negative tweets\", len(test_tweets_neg))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export relevant data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data.to_csv('dedup_training_data.csv', sep=',')\n",
    "val_tweets.to_csv('dedup_val_data.csv', sep=',')\n",
    "test_tweets.to_csv('dedup_test_data.csv', sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
