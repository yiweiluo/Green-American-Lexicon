{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn import svm\n",
    "from numpy.random import RandomState\n",
    "from scipy.stats import ttest_ind\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn import metrics\n",
    "from datetime import timedelta  \n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions for importing & cleaning relevant tweets\n",
    "\n",
    "def tweet_imports(url):\n",
    "    imp = pd.read_csv(url)\n",
    "    imp = imp.drop_duplicates()\n",
    "    imp['tweet_clean'] = imp['tweet'].str.replace('http\\S+|www.\\S+|pic.twitter.com\\S+', '', case=False)\n",
    "    imp['tweet_clean'] =imp['tweet_clean'].replace('[^A-Za-z0-9 ]+','',regex=True)\n",
    "    imp['tweet_clean'] = map(lambda x: x.lower(), imp['tweet_clean'])\n",
    "    imp['date'] = pd.to_datetime(imp['date'])\n",
    "    return imp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import manually labelled & influential tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1507, 990, 277)\n",
      "253\n"
     ]
    }
   ],
   "source": [
    "test_tweets = tweet_imports(\"Datasets/Correct Manual Labels/Manual_Labelled_Tweets.csv\")\n",
    "print(len(test_tweets), len(test_tweets[test_tweets['label']==1]), len(test_tweets[test_tweets['label']==-1]))\n",
    "x = test_tweets[test_tweets['label']==-1]\n",
    "print(len(x.drop_duplicates('tweet_clean')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(403432, 220063, 183369)\n"
     ]
    }
   ],
   "source": [
    "train1 = tweet_imports(\"Datasets/Training Data/influential_tweets_filter_1.csv\")\n",
    "train2 = tweet_imports(\"Datasets/Training Data/influential_tweets_filter_2.csv\")\n",
    "train3 = tweet_imports(\"Datasets/Training Data/influential_tweets_filter_3.csv\")\n",
    "all_train = pd.concat([train1, train2, train3])\n",
    "\n",
    "print(len(all_train), len(all_train[all_train['label']==1]), len(all_train[all_train['label']==-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19066\n"
     ]
    }
   ],
   "source": [
    "# Select tweets that contain disasters from the training data \n",
    "disaster_train = all_train[all_train['tweet_clean'].str.contains(\"michael|florence|wildfire|blizzard|fire|hurricane|bomb|cyclone|storm|snow|blaze\")==True]\n",
    "disaster_train = disaster_train[disaster_train['tweet_clean'].str.contains(\"climate|change|global|warming\")==True]\n",
    "\n",
    "# Remove from training data the tweets that are already in the test data\n",
    "dis_set = disaster_train[['tweet_clean', 'label']].copy()\n",
    "test_tweets = test_tweets[['tweet_clean', 'label']].copy()\n",
    "dis_set['identifier'] = 0\n",
    "test_tweets['identifier'] = 1\n",
    "dis_set = pd.concat([dis_set, test_tweets])\n",
    "dis_set.drop_duplicates(keep=False)\n",
    "disaster_train = dis_set[dis_set['identifier']==0]\n",
    "print(len(disaster_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import & clean downloaded Twitter data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_tweets_restrict(filename, start_date, end_date):\n",
    "    \n",
    "    # Unlabeled tweets\n",
    "    tweets = tweet_imports(\"Datasets/Twint Output/\" + filename)\n",
    "     \n",
    "    # Constrain to relevant dates\n",
    "    print(min(tweets['date']), max(tweets['date']))\n",
    "    \n",
    "    begin_tweets = pd.to_datetime(start_date) - timedelta(weeks = 2)\n",
    "    end_tweets = pd.to_datetime(end_date) + timedelta(weeks = 2)\n",
    "    print(\"Two weeks before:\", begin_tweets, \"Two weeks after:\", end_tweets)\n",
    "    \n",
    "    tweets = tweets[tweets['date'] >= begin_tweets]\n",
    "    tweets = tweets[tweets['date'] <= end_tweets]\n",
    "\n",
    "    # Remove tweets to label that were already seen in train/valid/test for forming predictions\n",
    "    tweet_dis_overlap = tweets.merge(disaster_train, on=['tweet_clean'])\n",
    "    tweets = tweets[(~tweets.tweet_clean.isin(tweet_dis_overlap.tweet_clean))]\n",
    "    tweet_test_overlap = tweets.merge(test_tweets, on=['tweet_clean'])\n",
    "    tweets = tweets[(~tweets.tweet_clean.isin(tweet_test_overlap.tweet_clean))]\n",
    "\n",
    "    # Combine pre-labeled tweets\n",
    "    pre_labelled_tweets = pd.concat((tweet_dis_overlap, tweet_test_overlap), axis=0)\n",
    "    print(len(tweets), len(pre_labelled_tweets))\n",
    "    \n",
    "    return((tweets, pre_labelled_tweets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_tweets_restrict(filename, start_date, end_date, outfile):\n",
    "    \n",
    "    tweets, pre_labelled_tweets = clean_tweets_restrict(filename, start_date, end_date)\n",
    "    \n",
    "    # Split to pre- and post\n",
    "    pre = tweets[tweets['date'] <= start_date]\n",
    "    post = tweets[tweets['date'] > start_date]\n",
    "    pre.reset_index(inplace=True)\n",
    "    post.reset_index(inplace=True)\n",
    "    print(\"Total tweets to label\", len(tweets), \"Prior tweets to label\", len(pre), \"Post tweets to label\", len(post))\n",
    "    \n",
    "    # Merge pre- and post- tweets by same user to see if user sentiments change\n",
    "    pre_users = pd.DataFrame(pre['user_id'].unique())\n",
    "    post_users = pd.DataFrame(post['user_id'].unique())\n",
    "    merge = pd.merge(pre_users, post_users, how='inner')\n",
    "    print(\"Number of users tweeting before and after\", len(merge))\n",
    "\n",
    "    pre_tweets = pre.loc[pre['user_id'].isin(merge.iloc[:,0])]\n",
    "    post_tweets = post.loc[post['user_id'].isin(merge.iloc[:,0])]\n",
    "    print(\"Num tweets before\", len(pre_tweets), \"Num tweets after\", len(post_tweets))\n",
    "    \n",
    "    tweets.to_csv('Datasets/Event Tweets/' + outfile + '.csv', sep=',')\n",
    "    pre_labelled_tweets.to_csv('Datasets/Event Tweets/Prelabelled_' + outfile + '.csv', sep=',')\n",
    "    \n",
    "    return((tweets, pre, post, merge))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_tweets_restrict_combine(filename1, filename2, start_date, end_date, outfile):\n",
    "    \n",
    "    # Unlabeled tweets\n",
    "    tweets1, pre_labelled_tweets1 = clean_tweets_restrict(filename1, start_date, end_date)\n",
    "    tweets2, pre_labelled_tweets2 = clean_tweets_restrict(filename2, start_date, end_date)\n",
    "    print(len(tweets1))\n",
    "    print(len(tweets2))\n",
    "    tweets = pd.merge(tweets1, tweets2, how='outer')\n",
    "    pre_labelled_tweets = pd.merge(pre_labelled_tweets1, pre_labelled_tweets2, how='outer')\n",
    "    \n",
    "    # Split to pre- and post\n",
    "    pre = tweets[tweets['date'] <= start_date]\n",
    "    post = tweets[tweets['date'] > start_date]\n",
    "    pre.reset_index(inplace=True)\n",
    "    post.reset_index(inplace=True)\n",
    "    print(\"Total tweets\", len(tweets), \"Prior tweets\", len(pre), \"Post tweets\", len(post))\n",
    "    \n",
    "    # Merge pre- and post- tweets by same user to see if user sentiments change\n",
    "    pre_users = pd.DataFrame(pre['user_id'].unique())\n",
    "    post_users = pd.DataFrame(post['user_id'].unique())\n",
    "    merge = pd.merge(pre_users, post_users, how='inner')\n",
    "    print(\"Number of users tweeting before and after\", len(merge))\n",
    "\n",
    "    pre_tweets = pre.loc[pre['user_id'].isin(merge.iloc[:,0])]\n",
    "    post_tweets = post.loc[post['user_id'].isin(merge.iloc[:,0])]\n",
    "    print(\"Num tweets before\", len(pre_tweets), \"Num tweets after\", len(post_tweets))\n",
    "    \n",
    "    tweets.to_csv('Datasets/Event Tweets/' + outfile + '.csv', sep=',')\n",
    "    pre_labelled_tweets.to_csv('Datasets/Event Tweets/Prelabelled_' + outfile + '.csv', sep=',')\n",
    "    \n",
    "    return((tweets, pre, post, merge))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Timestamp('2017-11-21 00:00:00'), Timestamp('2018-01-19 00:00:00'))\n",
      "('Two weeks before:', Timestamp('2017-12-19 00:00:00'), 'Two weeks after:', Timestamp('2018-01-20 00:00:00'))\n",
      "(14957, 1610)\n",
      "('Total tweets to label', 14957, 'Prior tweets to label', 2614, 'Post tweets to label', 12343)\n",
      "('Number of users tweeting before and after', 330)\n",
      "('Num tweets before', 634, 'Num tweets after', 1479)\n"
     ]
    }
   ],
   "source": [
    "# January 2018 bomb cyclone (Jan 2 - Jan 6): https://en.wikipedia.org/wiki/January_2018_North_American_blizzard\n",
    "blizzard_tweets, blizzard_pre, blizzard_post, blizzard_merge = count_tweets_restrict('blizzard_geo_tweets_v2.csv', '1/2/18', '1/6/18', 'blizzard')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Timestamp('2018-05-31 00:00:00'), Timestamp('2018-09-26 00:00:00'))\n",
      "('Two weeks before:', Timestamp('2018-07-13 00:00:00'), 'Two weeks after:', Timestamp('2018-10-02 00:00:00'))\n",
      "(2710, 929)\n",
      "(Timestamp('2018-06-04 00:00:00'), Timestamp('2018-10-01 00:00:00'))\n",
      "('Two weeks before:', Timestamp('2018-07-13 00:00:00'), 'Two weeks after:', Timestamp('2018-10-02 00:00:00'))\n",
      "(2808, 956)\n",
      "2710\n",
      "2808\n",
      "('Total tweets', 3035, 'Prior tweets', 173, 'Post tweets', 2862)\n",
      "('Number of users tweeting before and after', 36)\n",
      "('Num tweets before', 49, 'Num tweets after', 95)\n"
     ]
    }
   ],
   "source": [
    "# California Mendocino Wildfires (July 27 - Sep 18): https://en.wikipedia.org/wiki/Mendocino_Complex_Fire\n",
    "summerfire_tweets, summerfire_pre, summerfire_post, summerfire_merge = count_tweets_restrict_combine('summerfire_geo_tweets.csv', \n",
    "                                                                                             'summerfire_geo_tweets_v2.csv', \n",
    "                                                                                             '7/27/18', '9/18/18',\n",
    "                                                                                                    'summerfire')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Timestamp('2018-07-03 00:00:00'), Timestamp('2018-09-30 00:00:00'))\n",
      "('Two weeks before:', Timestamp('2018-08-17 00:00:00'), 'Two weeks after:', Timestamp('2018-10-03 00:00:00'))\n",
      "(6413, 1032)\n",
      "('Total tweets to label', 6413, 'Prior tweets to label', 778, 'Post tweets to label', 5635)\n",
      "('Number of users tweeting before and after', 122)\n",
      "('Num tweets before', 193, 'Num tweets after', 497)\n"
     ]
    }
   ],
   "source": [
    "# Hurricane Florence (Aug 31 - Sep 19): https://en.wikipedia.org/wiki/Hurricane_Florence\n",
    "florence_tweets, florence_pre, florence_post, florence_merge = count_tweets_restrict('florence_geo_tweets.csv', '8/31/18', '9/19/18',\n",
    "                                                                                    'florence')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Timestamp('2018-09-05 00:00:00'), Timestamp('2018-10-28 00:00:00'))\n",
      "('Two weeks before:', Timestamp('2018-09-23 00:00:00'), 'Two weeks after:', Timestamp('2018-10-30 00:00:00'))\n",
      "(13035, 3126)\n",
      "('Total tweets to label', 13035, 'Prior tweets to label', 1912, 'Post tweets to label', 11123)\n",
      "('Number of users tweeting before and after', 281)\n",
      "('Num tweets before', 880, 'Num tweets after', 1351)\n"
     ]
    }
   ],
   "source": [
    "# Hurricane Michael (Oct 7 - Oct 16): https://en.wikipedia.org/wiki/Hurricane_Michael\n",
    "michael_tweets, michael_pre, michael_post, michael_merge = count_tweets_restrict('michael_geo_tweets.csv', '10/07/18', '10/16/18',\n",
    "                                                                                'michael')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Timestamp('2018-09-10 00:00:00'), Timestamp('2018-12-08 00:00:00'))\n",
      "('Two weeks before:', Timestamp('2018-10-25 00:00:00'), 'Two weeks after:', Timestamp('2018-12-09 00:00:00'))\n",
      "(5375, 151)\n",
      "(Timestamp('2018-09-10 00:00:00'), Timestamp('2018-12-08 00:00:00'))\n",
      "('Two weeks before:', Timestamp('2018-10-25 00:00:00'), 'Two weeks after:', Timestamp('2018-12-09 00:00:00'))\n",
      "(6081, 143)\n",
      "5375\n",
      "6081\n",
      "('Total tweets', 6654, 'Prior tweets', 55, 'Post tweets', 6599)\n",
      "('Number of users tweeting before and after', 14)\n",
      "('Num tweets before', 19, 'Num tweets after', 43)\n"
     ]
    }
   ],
   "source": [
    "# California Camp wildfires (Nov 8 - 25): https://en.wikipedia.org/wiki/Camp_Fire_(2018)\n",
    "winterfire_tweets, winterfire_pre, winterfire_post, winterfire_merge = count_tweets_restrict_combine(\n",
    "    'winterfire_geo_tweets.csv', \n",
    "    'winterfire_geo_tweets_v2.csv', \n",
    "    '11/08/18', '11/25/18', 'winterfire')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create train / validation / test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18191\n",
      "(8939, 9252)\n",
      "(990, 277)\n"
     ]
    }
   ],
   "source": [
    "#randomly select tweets that go to validation set or training set\n",
    "disaster_train = shuffle(disaster_train,random_state=123)\n",
    "disaster_train = disaster_train.drop_duplicates('tweet_clean')\n",
    "num_tweets = len(disaster_train)\n",
    "\n",
    "print(num_tweets)\n",
    "infl_val_pos = disaster_train[disaster_train['label']==1]\n",
    "infl_val_neg = disaster_train[disaster_train['label']==-1]\n",
    "print(len(infl_val_pos), len(infl_val_neg))\n",
    "\n",
    "labeled_tweets = test_tweets[test_tweets['label']!=0]\n",
    "test_tweets_shuffle = shuffle(labeled_tweets,random_state=456)\n",
    "manual_pos = test_tweets_shuffle[test_tweets_shuffle['label']==1]\n",
    "manual_neg = test_tweets_shuffle[test_tweets_shuffle['label']==-1]\n",
    "print(len(manual_pos), len(manual_neg))\n",
    "\n",
    "train_pct = .9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1111\n",
      "859\n",
      "252\n",
      "('Num total tweets', 18191, '\\n Num train tweets', 16371, '\\n Num validation tweets', 1820, '\\n Num test tweets', 500)\n"
     ]
    }
   ],
   "source": [
    "training_data = disaster_train[:int(num_tweets*train_pct)]\n",
    "\n",
    "val_tweets = disaster_train[int(num_tweets*train_pct):]\n",
    "\n",
    "# concatenate test tweets\n",
    "half_test_num = 250\n",
    "\n",
    "test_tweets_shuffle = test_tweets_shuffle.drop_duplicates('tweet_clean')\n",
    "print(len(test_tweets_shuffle))\n",
    "test_tweets_pos = test_tweets_shuffle[test_tweets_shuffle['label']==1]\n",
    "print(len(test_tweets_pos))\n",
    "test_tweets_neg = test_tweets_shuffle[test_tweets_shuffle['label']==-1]\n",
    "print(len(test_tweets_neg))\n",
    "\n",
    "test_tweets = shuffle(pd.concat([test_tweets_neg[:half_test_num],test_tweets_pos[:half_test_num]]),random_state=0)\n",
    "\n",
    "print(\"Num total tweets\", num_tweets,\n",
    "      \"\\n Num train tweets\", len(training_data), \n",
    "      \"\\n Num validation tweets\", len(val_tweets), \n",
    "      \"\\n Num test tweets\", len(test_tweets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Num train tweets', 16371, 'Num positive tweets', 8048, 'Num negative tweets', 8323)\n",
      "('Num val tweets', 1820, 'Num positive tweets', 891, 'Num negative tweets', 929)\n",
      "('Num test tweets', 500, 'Num positive tweets', 250, 'Num negative tweets', 250)\n"
     ]
    }
   ],
   "source": [
    "# Basic stats on training data from celebrities\n",
    "train_pos = training_data[training_data['label']==1]\n",
    "train_neg = training_data[training_data['label']==-1]\n",
    "print(\"Num train tweets\", len(training_data), \"Num positive tweets\", len(train_pos), \n",
    "      \"Num negative tweets\", len(train_neg))\n",
    "\n",
    "# Check number of manual labelled tweets and tweets to label\n",
    "val_tweets_pos = val_tweets[val_tweets['label']==1]\n",
    "val_tweets_neg = val_tweets[val_tweets['label']==-1]\n",
    "print(\"Num val tweets\", len(val_tweets), \"Num positive tweets\", len(val_tweets_pos), \n",
    "      \"Num negative tweets\", len(val_tweets_neg))\n",
    "\n",
    "# Check number of manual labelled tweets and tweets to label\n",
    "test_tweets_pos = test_tweets[test_tweets['label']==1]\n",
    "test_tweets_neg = test_tweets[test_tweets['label']==-1]\n",
    "print(\"Num test tweets\", len(test_tweets), \"Num positive tweets\", len(test_tweets_pos), \n",
    "      \"Num negative tweets\", len(test_tweets_neg))\n",
    "\n",
    "test_tweets = pd.concat([test_tweets_pos, test_tweets_neg])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove test tweets from training & validation sets\n",
    "test_train_overlap = training_data.merge(test_tweets, on=['tweet_clean'])\n",
    "#print(len(test_train_overlap))\n",
    "training_data = training_data[(~training_data.tweet_clean.isin(test_train_overlap.tweet_clean))]\n",
    "#print(len(training_data))\n",
    "\n",
    "test_val_overlap = val_tweets.merge(test_tweets, on=['tweet_clean'])\n",
    "#print(len(test_val_overlap))\n",
    "val_tweets = val_tweets[(~val_tweets.tweet_clean.isin(test_val_overlap.tweet_clean))]\n",
    "#print(len(val_tweets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Num train tweets', 16360, 'Num positive tweets', 8044, 'Num negative tweets', 8316)\n",
      "('Num val tweets', 1819, 'Num positive tweets', 891, 'Num negative tweets', 928)\n",
      "('Num test tweets', 500, 'Num positive tweets', 250, 'Num negative tweets', 250)\n"
     ]
    }
   ],
   "source": [
    "# Basic stats on training data from celebrities\n",
    "train_pos = training_data[training_data['label']==1]\n",
    "train_neg = training_data[training_data['label']==-1]\n",
    "print(\"Num train tweets\", len(training_data), \"Num positive tweets\", len(train_pos), \n",
    "      \"Num negative tweets\", len(train_neg))\n",
    "\n",
    "# Check number of manual labelled tweets and tweets to label\n",
    "val_tweets_pos = val_tweets[val_tweets['label']==1]\n",
    "val_tweets_neg = val_tweets[val_tweets['label']==-1]\n",
    "print(\"Num val tweets\", len(val_tweets), \"Num positive tweets\", len(val_tweets_pos), \n",
    "      \"Num negative tweets\", len(val_tweets_neg))\n",
    "\n",
    "# Check number of manual labelled tweets and tweets to label\n",
    "test_tweets_pos = test_tweets[test_tweets['label']==1]\n",
    "test_tweets_neg = test_tweets[test_tweets['label']==-1]\n",
    "print(\"Num test tweets\", len(test_tweets), \"Num positive tweets\", len(test_tweets_pos), \n",
    "      \"Num negative tweets\", len(test_tweets_neg))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export relevant data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data.to_csv('Datasets/SentimentTests/dedup_training_data.csv', sep=',')\n",
    "val_tweets.to_csv('Datasets/SentimentTests/dedup_val_data.csv', sep=',')\n",
    "test_tweets.to_csv('Datasets/SentimentTests/dedup_test_data.csv', sep=',')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
